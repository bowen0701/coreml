{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Modeling\n",
    "\n",
    "- Bowen Li\n",
    "- 2018/01/17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Bayesian modeling is one of the most important machine learning techniques. This notebook is to summarize its methodology and implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem\n",
    "\n",
    "Thomas Bayes and Pierre-Simon Laplace were aware of what is now known as **Bayes theorem,** or the so-called **inverse probablity,** which is a simple and beautiful relation between two conditional probabilities; one is an inverse of the other. I will borrow [Michael Hochster (Director of Data Science at Stitch Fix)'s Quora answer](https://www.quora.com/What-is-an-intuitive-explanation-of-Bayes-Rule/answer/Michael-Hochster) and provide another example to illustrate the concept of Bayes theorem.\n",
    "\n",
    "**Example (Rich and Happy).** Your friend is trying to convince you that money cannot buy happiness, citing from a Harvard study that shows only 10% of happy people are rich, thus we have $P(\\text{rich}\\ |\\ \\text{happy})$. Nevertheless, what we really want to know is how is the proprotion of rich people is happy, that is $P(\\text{happy}\\ |\\ \\text{rich})$. How could we obtain the result?\n",
    "\n",
    "Let's do some probability calculation:\n",
    "\n",
    "$$\n",
    "P(\\text{happy}\\ |\\ \\text{rich})\n",
    "= \\frac{P(\\text{happy}, \\text{rich})}{P(\\text{rich})}\n",
    "= \\frac{P(\\text{rich}\\ |\\ \\text{happy}) \\times P(\\text{happy})}{P(\\text{rich})}\n",
    "$$\n",
    "\n",
    "From the above equation we can observe that we have to know how is the proportion of happy people in the whole population, and how is the proportion of rich people. Suppose we know that 40% of people are happy, and 5% of people are rich, then the result follows. Specifically, summarizing the information we have:\n",
    "\n",
    "- 10% of happy people are rich: $P(\\text{rich}\\ |\\ \\text{happy}) = 10\\%$\n",
    "- 40% of people are happy: $P(\\text{happy}) = 40\\%$\n",
    "- 5% of people are rich: $P(\\text{rich}) = 5\\%$\n",
    "\n",
    "Hence, after simple calculation, we obtain that $P(\\text{happy}\\ |\\ \\text{rich}) = 80\\%$, that is, 80% of rich people are happy. So a really strong majority of rich people are happy. Let's work hard and smart to be rich. :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example (Breast Cancer).** TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Probability Models\n",
    "\n",
    "Most of Bayesian modeling is calculating the following quantities in some way or either:\n",
    "\n",
    "- **Likelihood:** $p(x | \\theta)$\n",
    "- **Prior distribution:** $p(\\theta)$\n",
    "- **Marginal likelihood:** \n",
    "  $p(x) = \\int p(x | \\theta) p(\\theta) d(\\theta)$\n",
    "- **Posterior distribution:** \n",
    "  $p(\\theta | x) = p(x | \\theta) p(\\theta) / p(x)$\n",
    "- **Predictive distribution:** \n",
    "  $p(x_{new} | x) = \\int p(x_{new} | \\theta) p(\\theta | x) d(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De Finetti's Thereom\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point Estimation\n",
    "\n",
    "### MLE vs. MAP\n",
    "\n",
    "**Maximum Likelihood Estimation (MLE).** Estimate $\\theta$ as the **mode of the likelihood function** $p(x | \\theta)$:\n",
    "\n",
    "$$\n",
    "\\theta_{MLE} = argmax_{\\theta} p(x | \\theta)\n",
    "$$\n",
    "\n",
    "**Maximum a Posteriori (MAP).** Estimate $\\theta$ as the **mode of the posterior distributin** $p(\\theta | x)$:\n",
    "\n",
    "$$\n",
    "\\theta_{MAP} = argmax_{\\theta} p(\\theta | x) = argmax_{\\theta} p(x | \\theta) p(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Estimator\n",
    "\n",
    "TODO\n",
    "\n",
    "Nevertheless, the above MLE, MAP and Bayes estimator just provide point estimators for parameter, $\\theta$, of interest. In the usual Bayesian modeling, we would like to obtain the distribution of $\\theta$ given the observed data $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate Prior\n",
    "\n",
    "\n",
    "**Conjugate Prior.** A family of prior distributions, upon being multiplied by the likelihood, yield a posterior in the same family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial-Beta Conjugacy\n",
    "\n",
    "- Likelihood: $p(x | \\theta) \\sim Binomial(n, \\theta)$\n",
    "- Prior: $p(\\theta) \\sim Beta(\\alpha, \\beta)$\n",
    "- Posterior: $ p(\\theta | x) \\sim Beta(\\alpha^*, \\beta^*)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial-Dirichlet Conjugacy\n",
    "\n",
    "- Likelihood: $p(x | \\theta) \\sim Multinomial(n, \\theta_1,...,\\theta_k)$\n",
    "- Prior: $p(\\theta) \\sim Dirichlet(\\alpha_1,...,\\alpha_k)$\n",
    "- Posterior: $p(\\theta | x) \\sim Dirichlet(\\alpha_1^*,...,\\alpha_k^*)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson-Gamma Conjugacy\n",
    "\n",
    "- p(x | \\theta) ~ Poisson(\\theta)\n",
    "- p(\\theta) ~ Gamma(\\theta, \\theta)\n",
    "- p(\\theta | x) ~ Gamma(.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian-Gaussian Conjugacy\n",
    "\n",
    "- p(x | \\mu, \\sigma^2) ~ Gaussian(\\mu, \\sigma^2)\n",
    "- If \\sigma^2 fixed,\n",
    "  * p(\\mu) ~ Gaussian(\\mu_0, \\sigma_0^2)\n",
    "  * p(\\mu | x) ~ Gaussian(.)\n",
    "  * Notes: Var(X) = E[Var(X | \\mu)] + Var[E(X | \\mu)]\n",
    "- If \\mu^2 fixed,\n",
    "  * p(\\sigma^2) ~ InverseGamma(\\theta_1, \\theta_2)\n",
    "  * p(\\sigma^2 | x) ~ InverseGamma(.)\n",
    "  * Note: X ~ InverseGamma() => X^(-1) ~ Gamma(.)\n",
    "- If both (\\mu, \\sigma^2) are unknown, fit by stagewise modeling:\n",
    "  * p(\\mu | \\sigma^2) ~ N(\\mu_0, n0 \\tau)\n",
    "  * p(\\tau) ~ Gamma(\\alpha, \\beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Exponential Family Conjugacy\n",
    "\n",
    "- Exponential Family: \n",
    "  p(x | \\eta) = h(x) exp{\\eta^T(x) - A(\\eta)}\n",
    "- Conjugate prior:\n",
    "  p(\\eta) = H(\\tau, n0) exp{\\tau^\\eta - n0 A(\\eta)}\n",
    "- Posterior probability:\n",
    "  p(\\eta | x) ~ (\\tau, n0) -> (\\tau + \\sum T(x_j), n + n0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutivariate Gaussian Conjugacy\n",
    "\n",
    "- p(x | \\mu, \\Sigma) ~ MutivariateGaussian(\\mu, \\Sigma)\n",
    "  * Similar with Gaussian(.)\n",
    "- If \\Sigma fixed,\n",
    "  * p(\\mu) ~ MutivariateGaussian(\\mu_0, \\Sigma_0)\n",
    "  * p(\\mu | x) ~ MutivariateGaussian(.)\n",
    "- If \\mu fixed,\n",
    "  * p(\\Sigma) ~ InverseWishart(\\theta_1, \\theta_2)\n",
    "  * InverseWishart = InverseGamma(.)\n",
    "  * p(Precision Matrix) = p(\\Sigma^{-1}) \n",
    "    ~ Wishart(.) = GeneralizedGamma(.)\n",
    "  * p(\\Sigma | x) ~ InverseWishart(.)\n",
    "- If both (\\mu, \\Sigma) are unknown:\n",
    "  * p(\\mu | \\Sigma) ~ N(\\mu_0, n0 \\tau)\n",
    "  * p(\\tau) ~ InverseWishart(\\alpha, \\beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jeffreys Priors\n",
    "\n",
    "- Invariance Principle: Jeffreys Priors is invariant to change of variable \\theta.\n",
    "- Jeffreys Priors: Square root of Fisher Information.\n",
    "  \\pi_J = I(\\theta)^{1/2}.\n",
    "- Fisher Information:\n",
    "  I(\\theta) = E [- d^2 log p(x | \\theta) / d\\theta^2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References\n",
    "\n",
    "- Jordan (2010)'s lecture notes on Bayesian Modeling and Inference.\n",
    "- Hochster's Quora answer on [What is an intuitive explanation of Bayes' Rule?](https://www.quora.com/What-is-an-intuitive-explanation-of-Bayes-Rule/answer/Michael-Hochster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
