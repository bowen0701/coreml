{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic Regression Implementation from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Logistic regression is one of the most fundamental machine learning models for binary classification. I will summarize its methodology and implement it from scratch using NumPy.\n",
    "\n",
    "### Binary classification\n",
    "\n",
    "For example, the doctor would like to base on patients's features, including mean radius, mean texture, etc, to classify  breat cancer into one of the following two case: \n",
    "- \"malignant\": $y = 1$\n",
    "- \"benign\": $y = 0$\n",
    "\n",
    "which correspond to serious and gentle case respectively. \n",
    "\n",
    "We can load the breast cancer data from scikit-learn as a toy dataset, and split the data into the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "bc_data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 71\n",
    "TRAIN_PERCENT = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_names: \n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "data: \n",
      "[[  1.79900000e+01   1.03800000e+01   1.22800000e+02 ...,   5.37200000e-01\n",
      "    2.38800000e-01   2.76800000e-01]\n",
      " [  7.61500000e-02   1.35400000e+01   1.43600000e+01 ...,   4.61900000e-02\n",
      "    4.83300000e-02   5.01300000e-02]\n",
      " [  1.98700000e-01   6.16900000e-02   1.49900000e+01 ...,   1.62400000e-01\n",
      "    3.51100000e-01   3.87900000e-01]\n",
      " ..., \n",
      " [  5.56700000e+02   1.10600000e-01   1.46900000e-01 ...,   7.75700000e-02\n",
      "    1.16700000e+01   2.00200000e+01]\n",
      " [  7.52100000e+01   4.16200000e+02   1.01600000e-01 ...,   3.05900000e-01\n",
      "    7.62600000e-02   1.08600000e+01]\n",
      " [  2.14800000e+01   6.85100000e+01   3.60500000e+02 ...,   0.00000000e+00\n",
      "    2.87100000e-01   7.03900000e-02]]\n",
      "X.shape: (30, 569)\n"
     ]
    }
   ],
   "source": [
    "features = bc_data.get('feature_names')\n",
    "X = bc_data.get('data')\n",
    "X = X.reshape((X.shape[1], X.shape[0]))\n",
    "\n",
    "print('feature_names: \\n{}'.format(features))\n",
    "print('data: \\n{}'.format(X))\n",
    "\n",
    "print('X.shape: {}'.format(X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_names: ['malignant' 'benign']\n",
      "target: \n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      "  1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      "  1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      "  1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      "  1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      "  1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      "  1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      "  0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      "  1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      "  0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      "  1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      "  1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 0 0 0 0 0 0 1]]\n",
      "Y: (1, 569)\n"
     ]
    }
   ],
   "source": [
    "target = bc_data.get('target_names')\n",
    "Y = bc_data.get('target')\n",
    "Y = Y.reshape((1, Y.shape[0]))\n",
    "\n",
    "print('target_names: {}'.format(target))\n",
    "print('target: \\n{}'.format(Y))\n",
    "\n",
    "print('Y: {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of malignant: 212\n",
      "Number of benign: 357\n"
     ]
    }
   ],
   "source": [
    "print('Number of malignant: {}'.format((Y == 0).sum()))\n",
    "print('Number of benign: {}'.format((Y == 1).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (30, 455)\n",
      "Y_train.shape: (1, 455)\n",
      "X_test.shape: (30, 114)\n",
      "Y_test.shape: (1, 114)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "train_flag = np.random.rand(X.shape[1]) < TRAIN_PERCENT\n",
    "\n",
    "X_train = X[:, train_flag]\n",
    "Y_train = Y[:, train_flag]\n",
    "X_test = X[:, ~train_flag]\n",
    "Y_test = Y[:, ~train_flag]\n",
    "\n",
    "print('X_train.shape: {}'.format(X_train.shape))\n",
    "print('Y_train.shape: {}'.format(Y_train.shape))\n",
    "print('X_test.shape: {}'.format(X_test.shape))\n",
    "print('Y_test.shape: {}'.format(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sigmoid function.\n",
    "def sigmoid(x):\n",
    "    \"\"\"Compute the sigmoid of x.\n",
    "\n",
    "    Args:\n",
    "      x: A scalar or numpy array of any size.\n",
    "\n",
    "    Returns:\n",
    "      s: sigmoid(x).\n",
    "    \"\"\"\n",
    "    s = 1 / (1 + np.exp(-x))    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(s):\n",
    "    \"\"\"Compute the gradient of the sigmoid function.\n",
    "    \n",
    "    Args:\n",
    "      s: A scalar or numpy array. Sigmoid function.\n",
    "\n",
    "    Returns:\n",
    "      ds: Computed gradient.\n",
    "    \"\"\"\n",
    "    ds = s * (1 - s)    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_weights(dim):\n",
    "    \"\"\"Initialize weights.\n",
    "\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and b to 0.\n",
    "    \n",
    "    Args:\n",
    "      dim: A integer. Size of the w vector (or number of parameters.)\n",
    "    \n",
    "    Returns:\n",
    "      w: A Numpy array. Initialized vector of shape (dim, 1)\n",
    "      b: A integer. Initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    w = np.zeros(dim).reshape(dim, 1)\n",
    "    b = 0\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"Forward & backward propagation.\n",
    "\n",
    "    Implement the cost function and its gradient for the propagation.\n",
    "\n",
    "    Args:\n",
    "      w: A Numpy array. Weights of size (num_px * num_px * 3, 1)\n",
    "      b: A float. Bias.\n",
    "      X: A Numpy array. Data of size (num_px * num_px * 3, number of examples).\n",
    "      Y: A Numpy array. True \"label\" vector (containing 0 or 1) \n",
    "         of size (1, number of examples).\n",
    "\n",
    "    Returns:\n",
    "      cost: A float. Negative log-likelihood cost for logistic regression.\n",
    "      dw: A Numpy array. Gradient of the loss w.r.t. w, thus same shape as w.\n",
    "      db: A float. Gradient of the loss w.r.t b, thus same shape as b.\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Forward propagation from X to cost.\n",
    "    # Compute activation.\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    # Compute cost.\n",
    "    cost = - 1 / m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n",
    "    \n",
    "    # Backward propagation to find gradient.\n",
    "    dw = 1 / m * np.dot(X, (A - Y).T)\n",
    "    db = 1 / m * np.sum(A - Y)\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db} \n",
    "\n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
    "    \"\"\"Optimization function.\n",
    "\n",
    "    This function optimizes w and b by running a gradient descent algorithm.\n",
    "    That is, write down two steps and iterate through them:\n",
    "      1. Calculate the cost and the gradient for the current parameters. \n",
    "        Use propagate().\n",
    "      2. Update the parameters using gradient descent rule for w and b.\n",
    "    \n",
    "    Args:\n",
    "      w: A Numpy array. Weights of size (num_px * num_px * 3, 1).\n",
    "      b: A scalar. Bias.\n",
    "      X: A Numpy array. Data of shape (num_px * num_px * 3, number of examples).\n",
    "      Y: A Numpy array. True \"label\" vector (containing 0 if non-cat, 1 if cat), \n",
    "        of shape (1, number of examples)\n",
    "      num_iterations: A integer. Number of iterations of the optimization loop.\n",
    "      learning_rate: A scalr. Learning rate of the gradient descent update rule.\n",
    "      print_cost: A Boolean. Print the loss every 100 steps. Default: False.\n",
    "    \n",
    "    Returns:\n",
    "      params: A dictionary containing the weights w and bias b.\n",
    "      grads: A dictionary containing the gradients of the weights and bias \n",
    "        with respect to the cost function\n",
    "      costs: A list of all the costs computed during the optimization, \n",
    "        this will be used to plot the learning curve.\n",
    "    \"\"\"   \n",
    "    costs = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads.get('dw')\n",
    "        db = grads.get('db')\n",
    "        \n",
    "        # Update rule.\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    \"\"\"Prediction.\n",
    "\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression \n",
    "    parameters (w, b)\n",
    "    \n",
    "    Args:\n",
    "      w: A Numpy array. Learned weights of size (num_px * num_px * 3, 1).\n",
    "      b: A scalar. Learned bias.\n",
    "      X: A Numpy array. New data of size (num_px * num_px * 3, number of examples).\n",
    "    \n",
    "    Returns:\n",
    "      Y_prediction: A Numpy array containing all predictions (0/1) \n",
    "        for the examples in X.\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a label 1 \n",
    "    # being present in the picture.\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        # Convert probabilities a[0,i] to actual predictions p[0,i]\n",
    "        if A[0, i] > 0.5:\n",
    "            Y_prediction[0, i] = 1\n",
    "        else:\n",
    "            Y_prediction[0, i] = 0\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(Y_prediction, Y):\n",
    "    acc = 1 - np.mean(np.abs(Y_prediction_train - Y_train))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(X_train, Y_train, X_test, Y_test, \n",
    "                        num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "    \"\"\"Wrap-up function for logistic regression.\n",
    "\n",
    "    Builds the logistic regression model by calling the function \n",
    "    you've implemented previously.\n",
    "    \n",
    "    Args:\n",
    "      X_train: A Numpy. Training set of shape (num_px * num_px * 3, m_train).\n",
    "      Y_train: A Numpy array. Training labels of shape (1, m_train).\n",
    "      X_test: A Numpy array. Test set of shape (num_px * num_px * 3, m_test).\n",
    "      Y_test: A Numpy array. Test labels of shape (1, m_test).\n",
    "      num_iterations: An integer. Hyperparameter for the number of iterations \n",
    "        to optimize the parameters\n",
    "      learning_rate: A scalar. Hyperparameter for the learning rate used \n",
    "        in the update rule of optimize()\n",
    "      print_cost: A Boolean. Print the cost every 100 iterations. Default: True.\n",
    "    \n",
    "    Returns:\n",
    "      d: A dictionary containing information about the model.\n",
    "    \"\"\"    \n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "\n",
    "    # Gradient descent.\n",
    "    parameters, grads, costs = optimize(\n",
    "        w, b, X_train, Y_train, \n",
    "        num_iterations=num_iterations, learning_rate=learning_rate, \n",
    "        print_cost=print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters.get('w')\n",
    "    b = parameters.get('b')\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"Train accuracy: {} %\"\n",
    "          .format(accuracy(Y_prediction_train, Y_train) * 100))\n",
    "    print(\"Test accuracy: {} %\"\n",
    "          .format(accuracy(Y_prediction_test, Y_test) * 100))\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
