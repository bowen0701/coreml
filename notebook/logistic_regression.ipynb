{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Logistic regression is one of the most fundamental machine learning models for binary classification. I will summarize its methodology and implement it from scratch using NumPy.\n",
    "\n",
    "The problem we solve is **binary classification,** for example, the doctor would like to base on patients's features, including mean radius, mean texture, etc, to classify breat cancer into one of the following two case:\n",
    "\n",
    "- \"malignant\":  ùë¶=1 \n",
    "- \"benign\":  ùë¶=0 \n",
    "\n",
    "which correspond to serious and gentle case respectively.\n",
    "\n",
    "We will load the breast cancer data from scikit-learn as a toy dataset, and split the data into the training and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model\n",
    "\n",
    "[To be continued.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy Implementation of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch imports.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# TensorFlow import.\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    \"\"\"Numpy implementation of Logistic Regression.\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size=64, lr=0.01, n_epochs=1000):\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "    def get_data(self, X_train, y_train, shuffle=True):\n",
    "        \"\"\"Get dataset and information.\"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        # Get the numbers of examples and inputs.\n",
    "        self.n_examples, self.n_inputs = self.X_train.shape\n",
    "\n",
    "        if shuffle:\n",
    "            idx = list(range(self.n_examples))\n",
    "            random.shuffle(idx)\n",
    "            self.X_train = self.X_train[idx]\n",
    "            self.y_train = self.y_train[idx]\n",
    "\n",
    "    def _create_weights(self):\n",
    "        \"\"\"Create model weights and bias.\"\"\"\n",
    "        self.w = np.zeros(self.n_inputs).reshape(self.n_inputs, 1)\n",
    "        self.b = np.zeros(1).reshape(1, 1)\n",
    "\n",
    "    def _logit(self, X):\n",
    "        \"\"\"Logit: unnormalized log probability.\"\"\"\n",
    "        return np.matmul(X, self.w) + self.b\n",
    "\n",
    "    def _sigmoid(self, logit):\n",
    "        \"\"\"Sigmoid function by stabilization trick.\n",
    "\n",
    "        sigmoid(z) = 1 / (1 + exp(-z)) \n",
    "                   = exp(z) / (1 + exp(z)) * exp(z_max) / exp(z_max)\n",
    "                   = exp(z - z_max) / (exp(-z_max) + exp(z - z_max)),\n",
    "        where z is the logit, and z_max = z - max(0, z).\n",
    "        \"\"\"\n",
    "        logit_max = np.maximum(0, logit)\n",
    "        logit_stable = logit - logit_max\n",
    "        return np.exp(logit_stable) / (np.exp(-logit_max) + np.exp(logit_stable))\n",
    "    \n",
    "    def _model(self, X):\n",
    "        \"\"\"Logistic regression model.\"\"\"\n",
    "        logit = self._logit(X)\n",
    "        return self._sigmoid(logit)\n",
    "\n",
    "    def _loss(self, y, logit):\n",
    "        \"\"\"Cross entropy loss by stabilizaiton trick.\n",
    "\n",
    "        cross_entropy_loss(y, z) \n",
    "          = - 1/n * \\sum_{i=1}^n y_i * log p(y_i = 1|x_i) + (1 - y_i) * log p(y_i = 0|x_i)\n",
    "          = - 1/n * \\sum_{i=1}^n y_i * (z_i - log(1 + exp(z_i))) + (1 - y_i) * (-log(1 + exp(z_i))),\n",
    "        where z is the logit, z_max = z - max(0, z),\n",
    "          log p(y = 1|x)\n",
    "            = log (1 / (1 + exp(-z))) \n",
    "            = log (exp(z) / (1 + exp(z)))\n",
    "            = z - log(1 + exp(z))\n",
    "        and \n",
    "          log(1 + exp(z)) := logsumexp(z)\n",
    "            = log(exp(0) + exp(z))\n",
    "            = log(exp(0) + exp(z) * exp(z_max) / exp(z_max))\n",
    "            = z_max + log(exp(-z_max) + exp(z - z_max)).\n",
    "        \"\"\"\n",
    "        logit_max = np.maximum(0, logit)\n",
    "        logit_stable = logit - logit_max\n",
    "        logsumexp_stable = logit_max + np.log(np.exp(-logit_max) + np.exp(logit_stable))\n",
    "        self.cross_entropy = -(y * (logit - logsumexp_stable) + (1 - y) * (-logsumexp_stable))\n",
    "        return np.mean(self.cross_entropy)\n",
    "\n",
    "    def _optimize(self, X, y):\n",
    "        \"\"\"Optimize by stochastic gradient descent.\"\"\"\n",
    "        m = X.shape[0]\n",
    "\n",
    "        y_ = self._model(X) \n",
    "        dw = 1 / m * np.matmul(X.T, y_ - y)\n",
    "        db = np.mean(y_ - y)\n",
    "\n",
    "        for (param, grad) in zip([self.w, self.b], [dw, db]):\n",
    "            param[:] = param - self.lr * grad\n",
    "\n",
    "    def _fetch_batch(self):\n",
    "        \"\"\"Fetch batch dataset.\"\"\"\n",
    "        idx = list(range(self.n_examples))\n",
    "        for i in range(0, self.n_examples, self.batch_size):\n",
    "            idx_batch = idx[i:min(i + self.batch_size, self.n_examples)]\n",
    "            yield (self.X_train.take(idx_batch, axis=0), self.y_train.take(idx_batch, axis=0))\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"Fit model.\"\"\"\n",
    "        self._create_weights()\n",
    "\n",
    "        for epoch in range(1, self.n_epochs + 1):\n",
    "            total_loss = 0\n",
    "            for X_train_b, y_train_b in self._fetch_batch():\n",
    "                y_train_b = y_train_b.reshape((y_train_b.shape[0], -1))\n",
    "                self._optimize(X_train_b, y_train_b)\n",
    "                train_loss = self._loss(y_train_b, self._logit(X_train_b))\n",
    "                total_loss += train_loss * X_train_b.shape[0]\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print('epoch {0}: training loss {1}'.format(epoch, total_loss / self.n_examples))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_coeff(self):\n",
    "        return self.b, self.w.reshape((-1,))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self._model(X).reshape((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression as LogisticRegressionSklearn\n",
    "\n",
    "# https://github.com/bowen0701/machine-learning/blob/master/metrics.py\n",
    "from metrics import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read breast cancer data.\n",
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.799e+01, 1.038e+01, 1.228e+02, 1.001e+03, 1.184e-01, 2.776e-01,\n",
       "        3.001e-01, 1.471e-01, 2.419e-01, 7.871e-02, 1.095e+00, 9.053e-01,\n",
       "        8.589e+00, 1.534e+02, 6.399e-03, 4.904e-02, 5.373e-02, 1.587e-02,\n",
       "        3.003e-02, 6.193e-03, 2.538e+01, 1.733e+01, 1.846e+02, 2.019e+03,\n",
       "        1.622e-01, 6.656e-01, 7.119e-01, 2.654e-01, 4.601e-01, 1.189e-01],\n",
       "       [2.057e+01, 1.777e+01, 1.329e+02, 1.326e+03, 8.474e-02, 7.864e-02,\n",
       "        8.690e-02, 7.017e-02, 1.812e-01, 5.667e-02, 5.435e-01, 7.339e-01,\n",
       "        3.398e+00, 7.408e+01, 5.225e-03, 1.308e-02, 1.860e-02, 1.340e-02,\n",
       "        1.389e-02, 3.532e-03, 2.499e+01, 2.341e+01, 1.588e+02, 1.956e+03,\n",
       "        1.238e-01, 1.866e-01, 2.416e-01, 1.860e-01, 2.750e-01, 8.902e-02],\n",
       "       [1.969e+01, 2.125e+01, 1.300e+02, 1.203e+03, 1.096e-01, 1.599e-01,\n",
       "        1.974e-01, 1.279e-01, 2.069e-01, 5.999e-02, 7.456e-01, 7.869e-01,\n",
       "        4.585e+00, 9.403e+01, 6.150e-03, 4.006e-02, 3.832e-02, 2.058e-02,\n",
       "        2.250e-02, 4.571e-03, 2.357e+01, 2.553e+01, 1.525e+02, 1.709e+03,\n",
       "        1.444e-01, 4.245e-01, 4.504e-01, 2.430e-01, 3.613e-01, 8.758e-02]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test datasets.\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=71, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30) (426,)\n",
      "(143, 30) (143,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_raw.shape, y_train.shape)\n",
    "print(X_test_raw.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering for standardizing features by min-max scaler.\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "X_train = min_max_scaler.fit_transform(X_train_raw)\n",
    "X_test = min_max_scaler.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert arrays to float32.\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    np.float32(X_train), np.float32(X_test), np.float32(y_train), np.float32(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float32'), dtype('float32'))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype, y_train.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Logistic Regression in NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our Logistic Regression.\n",
    "logreg = LogisticRegression(batch_size=64, lr=1, n_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get datasets and build graph.\n",
    "logreg.get_data(X_train, y_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100: training loss 0.10471912386231665\n",
      "epoch 200: training loss 0.08454669375629945\n",
      "epoch 300: training loss 0.07554912526419114\n",
      "epoch 400: training loss 0.07015805509606268\n",
      "epoch 500: training loss 0.06646531267573902\n",
      "epoch 600: training loss 0.06373787850011073\n",
      "epoch 700: training loss 0.06162297443169833\n",
      "epoch 800: training loss 0.05992532367850267\n",
      "epoch 900: training loss 0.05852633216647383\n",
      "epoch 1000: training loss 0.057349146010503435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.LogisticRegression at 0x7f844e186f60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[16.52129462]]),\n",
       " array([-1.40672925, -3.62553084, -1.42597638, -2.83856207, -1.53105707,\n",
       "         1.69560885, -4.60904947, -7.00227206, -1.55716025,  2.80913001,\n",
       "        -8.67203252, -0.33897257, -6.70854376, -5.02197434,  1.41026049,\n",
       "         4.40956172,  1.93940382,  0.83835797,  3.44227294,  3.46665622,\n",
       "        -5.4506232 , -4.78411994, -4.81910325, -5.12896557, -3.27458594,\n",
       "        -0.90580351, -4.48998728, -4.91530489, -3.62362622, -1.15632904]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get coefficient.\n",
    "logreg.get_coeff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.94886718e-01, 6.66396278e-13, 7.77193847e-05, 9.98286083e-01,\n",
       "       9.99107123e-01, 9.99467033e-01, 3.61089969e-03, 1.01959963e-03,\n",
       "       9.99965184e-01, 1.18665034e-02])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted probabilities for training data.\n",
    "p_train_ = logreg.predict(X_train)\n",
    "p_train_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted labels for training data.\n",
    "y_train_ = (p_train_ > 0.5) * 1\n",
    "y_train_[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9882629107981221"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction accuracy for training data.\n",
    "accuracy(y_train, y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.52413246e-04 9.99934493e-01 2.51019009e-03 9.97170130e-01\n",
      " 4.25204590e-05 9.99858929e-01 2.60675238e-06 9.99999604e-01\n",
      " 4.53260387e-01 4.70845180e-09]\n"
     ]
    }
   ],
   "source": [
    "# Predicted label correctness for test data.\n",
    "p_test_ = logreg.predict(X_test)\n",
    "print(p_test_[:10])\n",
    "y_test_ = (p_test_ > 0.5) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.972027972027972"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction accuracy for test data.\n",
    "accuracy(y_test, y_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Implementation of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionTorch(nn.Module):\n",
    "    \"\"\"PyTorch implementation of Logistic Regression.\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size=64, lr=0.01, n_epochs=1000):\n",
    "        super(LogisticRegressionTorch, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "    def get_data(self, X_train, y_train, shuffle=True):\n",
    "        \"\"\"Get dataset and information.\"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        # Get the numbers of examples and inputs.\n",
    "        self.n_examples, self.n_inputs = self.X_train.shape\n",
    "\n",
    "        if shuffle:\n",
    "            idx = list(range(self.n_examples))\n",
    "            random.shuffle(idx)\n",
    "            self.X_train = self.X_train[idx]\n",
    "            self.y_train = self.y_train[idx]\n",
    "\n",
    "    def _create_model(self):\n",
    "        \"\"\"Create logistic regression model.\"\"\"\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.n_inputs, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.model(x)\n",
    "        return y\n",
    "\n",
    "    def _create_loss(self):\n",
    "        \"\"\"Create (binary) cross entropy loss.\"\"\"\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def _create_optimizer(self):\n",
    "        \"\"\"Create optimizer by stochastic gradient descent.\"\"\"\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def build_graph(self):\n",
    "        \"\"\"Build computational graph.\"\"\"\n",
    "        self._create_model()\n",
    "        self._create_loss()\n",
    "        self._create_optimizer()\n",
    "\n",
    "    def _fetch_batch(self):\n",
    "        \"\"\"Fetch batch dataset.\"\"\"\n",
    "        idx = list(range(self.n_examples))\n",
    "        for i in range(0, self.n_examples, self.batch_size):\n",
    "            idx_batch = idx[i:min(i + self.batch_size, self.n_examples)]\n",
    "            yield (self.X_train.take(idx_batch, axis=0), \n",
    "                   self.y_train.take(idx_batch, axis=0))\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"Fit model.\"\"\"\n",
    "        for epoch in range(1, self.n_epochs + 1):\n",
    "            total_loss = 0\n",
    "            for X_train_b, y_train_b in self._fetch_batch():\n",
    "                # Convert to Tensor from NumPy array and reshape ys.\n",
    "                X_train_b, y_train_b = (\n",
    "                    torch.from_numpy(X_train_b), \n",
    "                    torch.from_numpy(y_train_b).view(-1, 1))\n",
    "\n",
    "                y_pred_b = self.model(X_train_b)\n",
    "                batch_loss = self.criterion(y_pred_b, y_train_b)\n",
    "                total_loss += batch_loss * X_train_b.shape[0]\n",
    "\n",
    "                # Zero grads, performs backward pass, and update weights.\n",
    "                self.optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print('Epoch {0}: training loss: {1}'\n",
    "                      .format(epoch, total_loss / self.n_examples))\n",
    "\n",
    "    def get_coeff(self):\n",
    "        \"\"\"Get model coefficients.\"\"\"\n",
    "        # Detach var which require grad.\n",
    "        return (self.model[0].bias.detach().numpy(),\n",
    "                self.model[0].weight.detach().numpy())\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict for new data.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            X_ = torch.from_numpy(X)\n",
    "            return self.model(X_).numpy().reshape((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Logistic Regression in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PyTorch Logistic Regression.\n",
    "logreg_torch = LogisticRegressionTorch(batch_size=64, lr=0.5, n_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_torch.get_data(X_train, y_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_torch.build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=30, out_features=1, bias=True)\n",
       "  (1): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_torch.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: training loss: 0.1344248503446579\n",
      "Epoch 200: training loss: 0.10645044595003128\n",
      "Epoch 300: training loss: 0.09378331899642944\n",
      "Epoch 400: training loss: 0.08620325475931168\n",
      "Epoch 500: training loss: 0.08102358877658844\n",
      "Epoch 600: training loss: 0.07719012349843979\n",
      "Epoch 700: training loss: 0.07419849187135696\n",
      "Epoch 800: training loss: 0.07177480310201645\n",
      "Epoch 900: training loss: 0.06975651532411575\n",
      "Epoch 1000: training loss: 0.06804037094116211\n"
     ]
    }
   ],
   "source": [
    "logreg_torch.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([13.545173], dtype=float32),\n",
       " array([[-1.3439738 , -2.3745942 , -1.2252212 , -2.306892  , -1.0910149 ,\n",
       "          0.473653  , -3.6592252 , -5.858444  , -0.9620514 ,  2.6239612 ,\n",
       "         -5.7088504 , -0.78457713, -4.560005  , -3.5590298 ,  1.3276924 ,\n",
       "          3.3612664 ,  1.8847641 ,  0.8996072 ,  2.5249946 ,  2.2905066 ,\n",
       "         -4.4273787 , -4.0534863 , -4.046817  , -4.318798  , -2.7103667 ,\n",
       "         -0.9162895 , -2.9415145 , -5.005501  , -2.149271  , -0.6811218 ]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get coefficient.\n",
    "logreg_torch.get_coeff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.8857111e-01, 7.6673147e-11, 3.9149617e-04, 9.9429590e-01,\n",
       "       9.9696511e-01, 9.9811780e-01, 8.3037931e-03, 3.4119210e-03,\n",
       "       9.9978751e-01, 2.1330371e-02], dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted probabilities for training data.\n",
    "p_train_ = logreg_torch.predict(X_train)\n",
    "p_train_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted labels for training data.\n",
    "y_train_ = (p_train_ > 0.5) * 1\n",
    "y_train_[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9835680751173709"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction accuracy for training data.\n",
    "accuracy(y_train, y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.6844194e-04 9.9963987e-01 1.4914670e-02 9.9478203e-01 3.2207184e-04\n",
      " 9.9945217e-01 3.5559508e-05 9.9999511e-01 5.2174652e-01 1.5732945e-07]\n"
     ]
    }
   ],
   "source": [
    "# Predicted label correctness for test data.\n",
    "p_test_ = logreg_torch.predict(X_test)\n",
    "print(p_test_[:10])\n",
    "y_test_ = (p_test_ > 0.5) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.972027972027972"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction accuracy for test data.\n",
    "accuracy(y_test, y_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Implementation of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_tf_graph(seed=71):\n",
    "    \"\"\"Reset default TensorFlow graph.\"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "class LogisticRegressionTF(object):\n",
    "    \"\"\"A TensorFlow implementation of Logistic Regression.\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size=64, learning_rate=0.01, n_epochs=1000):\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def get_data(self, X_train, y_train, shuffle=True):\n",
    "        \"\"\"Get dataset and information.\"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        # Get the numbers of examples and inputs.\n",
    "        self.n_examples, self.n_inputs = self.X_train.shape\n",
    "\n",
    "        idx = list(range(self.n_examples))\n",
    "        if shuffle:\n",
    "            random.shuffle(idx)\n",
    "        self.X_train = self.X_train[idx]\n",
    "        self.y_train = self.y_train[idx]\n",
    "\n",
    "    def _create_placeholders(self):\n",
    "        \"\"\"Create placeholder for features and labels.\"\"\"\n",
    "        self.X = tf.placeholder(tf.float32, shape=(None, self.n_inputs), name='X')\n",
    "        self.y = tf.placeholder(tf.float32, shape=(None, 1), name='y')\n",
    "\n",
    "    def _create_weights(self):\n",
    "        \"\"\"Create and initialize model weights and bias.\"\"\"\n",
    "        self.w = tf.get_variable(shape=[self.n_inputs, 1],\n",
    "                                 initializer=tf.random_normal_initializer(),\n",
    "                                 name='weights')\n",
    "        self.b = tf.get_variable(shape=[1],\n",
    "                                 initializer=tf.zeros_initializer(),\n",
    "                                 name='bias')\n",
    "\n",
    "    def _logit(self, X):\n",
    "        \"\"\"Logit: unnormalized log probability.\"\"\"\n",
    "        return tf.matmul(X, self.w) + self.b\n",
    "\n",
    "    def _model(self, X):\n",
    "        \"\"\"Logistic regression model.\"\"\"\n",
    "        logits = self._logit(X)\n",
    "        return tf.math.sigmoid(logits)\n",
    "\n",
    "    def _create_model(self):\n",
    "        # Create logistic regression model.\n",
    "        self.logits = self._logit(self.X)\n",
    "\n",
    "    def _create_loss(self):\n",
    "        # Create cross entropy loss.\n",
    "        self.cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=self.y,\n",
    "            logits=self.logits,\n",
    "            name='cross_entropy')\n",
    "        self.loss = tf.reduce_mean(self.cross_entropy, name='loss')\n",
    "\n",
    "    def _create_optimizer(self):\n",
    "        # Create gradient descent optimization.\n",
    "        self.optimizer = (\n",
    "            tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)\n",
    "            .minimize(self.loss))\n",
    "\n",
    "    def build_graph(self):\n",
    "        \"\"\"Build computational graph.\"\"\"\n",
    "        self._create_placeholders()\n",
    "        self._create_weights()\n",
    "        self._create_model()\n",
    "        self._create_loss()\n",
    "        self._create_optimizer()\n",
    "\n",
    "    def _fetch_batch(self): \n",
    "        \"\"\"Fetch batch dataset.\"\"\"\n",
    "        idx = list(range(self.n_examples))\n",
    "        for i in range(0, self.n_examples, self.batch_size):\n",
    "            idx_batch = idx[i:min(i + self.batch_size, self.n_examples)]\n",
    "            yield (self.X_train[idx_batch, :], self.y_train[idx_batch].reshape(-1, 1))\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"Fit model.\"\"\"\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session() as sess:            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for epoch in range(1, self.n_epochs + 1):\n",
    "                total_loss = 0\n",
    "                for X_train_b, y_train_b in self._fetch_batch():\n",
    "                    feed_dict = {self.X: X_train_b, self.y: y_train_b}\n",
    "                    _, batch_loss = sess.run([self.optimizer, self.loss],\n",
    "                                             feed_dict=feed_dict)\n",
    "                    total_loss += batch_loss * X_train_b.shape[0]\n",
    "\n",
    "                if epoch % 100 == 0:\n",
    "                    print('Epoch {0}: training loss: {1}'\n",
    "                          .format(epoch, total_loss / self.n_examples))\n",
    "\n",
    "            # Save model.\n",
    "            saver.save(sess, 'checkpoints/logreg')\n",
    "\n",
    "    def get_coeff(self):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            # Load model.\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, 'checkpoints/logreg')\n",
    "            return self.b.eval(), self.w.eval().reshape((-1,))\n",
    "\n",
    "    def predict(self, X):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            # Load model.\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, 'checkpoints/logreg')\n",
    "            return self._model(X).eval().reshape((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Logistic Regression in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf_graph()\n",
    "logreg_tf = LogisticRegressionTF(batch_size=64, learning_rate=0.5, n_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_tf.get_data(X_train, y_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "logreg_tf.build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: training loss: 0.13793940541330077\n",
      "Epoch 200: training loss: 0.10872595124121563\n",
      "Epoch 300: training loss: 0.09537097629806805\n",
      "Epoch 400: training loss: 0.08737332209455015\n",
      "Epoch 500: training loss: 0.08191518744392574\n",
      "Epoch 600: training loss: 0.07788145919920693\n",
      "Epoch 700: training loss: 0.07473775576537764\n",
      "Epoch 800: training loss: 0.07219409795714096\n",
      "Epoch 900: training loss: 0.07007863250136935\n",
      "Epoch 1000: training loss: 0.06828221622767024\n"
     ]
    }
   ],
   "source": [
    "logreg_tf.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/logreg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([13.305871], dtype=float32),\n",
       " array([-0.48871428, -3.111531  , -1.0498405 , -1.9807501 , -0.7382942 ,\n",
       "         0.17857291, -3.6536336 , -7.2938232 , -1.1253657 ,  2.6964319 ,\n",
       "        -4.308619  , -0.7267858 , -5.074624  , -3.93839   ,  0.9107544 ,\n",
       "         3.1807373 ,  2.4267256 ,  1.1122545 ,  2.5476625 ,  2.9644372 ,\n",
       "        -4.529931  , -3.8162556 , -4.2280235 , -4.247452  , -2.2962825 ,\n",
       "        -0.90618277, -3.2818813 , -4.6058207 , -2.6785274 , -0.06868351],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_tf.get_coeff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/logreg\n",
      "[9.8821372e-01 0.0000000e+00 4.0054321e-04 9.9521744e-01 9.9722141e-01\n",
      " 9.9857461e-01 7.6086819e-03 3.6685169e-03 9.9977803e-01 2.2061288e-02]\n",
      "[1 0 0 1 1 1 0 0 1 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9788732394366197"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted probabilities for training data.\n",
    "p_train_ = logreg_tf.predict((tf.cast(X_train, dtype=tf.float32)))\n",
    "print(p_train_[:10])\n",
    "\n",
    "# Predicted labels for training data.\n",
    "y_train_ = (p_train_ > 0.5) * 1\n",
    "print(y_train_[:10])\n",
    "\n",
    "# Prediction accuracy for training data.\n",
    "accuracy(y_train, y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/logreg\n",
      "[1.1076927e-03 9.9964797e-01 1.5742362e-02 9.9530858e-01 3.5384297e-04\n",
      " 9.9942517e-01 3.5881996e-05 9.9999422e-01 4.2618003e-01 1.7881393e-07]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.965034965034965"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted probabilities for test data.\n",
    "p_test_ = logreg_tf.predict((tf.cast(X_test, dtype=tf.float32)))\n",
    "print(p_test_[:10])\n",
    "\n",
    "# Predicted labels for training data.\n",
    "y_test_ = (p_test_ > 0.5) * 1\n",
    "y_test_[:3]\n",
    "\n",
    "# Prediction accuracy for training data.\n",
    "accuracy(y_test, y_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark with Sklearn's Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10000.0, max_iter=500)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit sklearn's Logistic Regression.\n",
    "logreg_sk = LogisticRegressionSklearn(C=1e4, solver='lbfgs', max_iter=500)\n",
    "\n",
    "logreg_sk.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([56.06250509]),\n",
       " array([[  53.5460616 ,  -27.2575739 ,   48.30697654,   10.5636878 ,\n",
       "          -14.75837806,   98.5009966 ,  -52.51936527,  -52.16906591,\n",
       "           -5.08742246,  -53.96348797,  -33.97198842,   -5.48905184,\n",
       "          -19.38885928,  -43.89981909,   38.75665922,  -51.43678914,\n",
       "           83.21007672,  -21.89925037,   14.96797392,   79.99757062,\n",
       "          -59.04206865,   -3.91791317,  -63.58395555, -103.96747709,\n",
       "           -7.9699581 ,   20.04904076,  -21.96650031,  -21.30939901,\n",
       "          -21.55187209,  -11.69936363]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get coefficients.\n",
    "logreg_sk.intercept_, logreg_sk.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted labels for training data.\n",
    "p_train_ = logreg_sk.predict(X_train)\n",
    "p_train_[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ = (p_train_ > 0.5) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted label correctness for training data.\n",
    "# y_pred_train == y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction accuracy for training data.\n",
    "accuracy(y_train, y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted label correctness for test data.\n",
    "p_test_ = logreg_sk.predict(X_test)\n",
    "y_test_ = (p_test_ > 0.5) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.965034965034965"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Prediction accuracy for test data.\n",
    "accuracy(y_test, y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
