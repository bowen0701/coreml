{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics (WIP)\n",
    "\n",
    "- Bowen Li\n",
    "- 2017/11/17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "How to measure our machine learning model's performance? Different problems and different purposes need different metics for us to fairly and effectively make decision. This technical report is to introduce useful metrics for machine learning evaluation.\n",
    "\n",
    "### Metrics Overview\n",
    "\n",
    "- R-Square: To measure model fitness for continuous response\n",
    "- Mean Squared Errors (MSE)\n",
    "- Mean Absolute Errors (MAE)\n",
    "- Akaike Information Criterion (AIC)\n",
    "- Bayesian Information Criterion (BIC)\n",
    "- Variance Inflation Factor (VIF): To measure multi-collineairity for features\n",
    "- Cook Distance: To detect influential instance points\n",
    "- Confusion Matrix\n",
    "- Accurary\n",
    "- Precission / Recall\n",
    "- Specificity\n",
    "- False Positive Rate (FPR)\n",
    "- Area under Curve (AUC)\n",
    "- Gain / Lift Charts\n",
    "- Kolmogorov-Smirnov Chart\n",
    "- Gini Coefficient\n",
    "- Kendall's tau\n",
    "- $F_{\\beta}$-Measures\n",
    "- Macro / Micro / Weighted Precision and Recall\n",
    "- Precision@K\n",
    "- Average Precision (AvP)\n",
    "- Mean Average Precision (MAP)\n",
    "- Discounted Cumulative Gain (DCG)\n",
    "- Normalized DCG (nDCG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-Square\n",
    "\n",
    "**R-square = Explained Variation / Total Variation.** To measure regression model fitness with continuous response $y$.\n",
    "\n",
    "$$\n",
    "R^2 = \\sum_{i=1}^n (\\hat y_i - y_i)^2 / \\sum (y_i - \\bar y)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Errors (MSE)\n",
    "\n",
    "MSE is a common metric to model fitness for regression problems. \n",
    "\n",
    "$$\n",
    "MSE = \\sum_{i=1}^n (\\hat y_i - y_i)^2\n",
    "$$\n",
    "\n",
    "where $\\hat y$ is the predicted value of $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Errors (MAE)\n",
    "\n",
    "Similarly for regressio problem, instead of using squared errors, we can use absolute errors to measure model fitness.\n",
    "\n",
    "$$\n",
    "MAE = \\sum_{i=1}^n |\\hat y_i - y_i|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Akaike Information Criterion (AIC)\n",
    "\n",
    "For model selection with likelihood inference, we would like to penalize higher likelihood by using more features.\n",
    "\n",
    "$$\n",
    "AIC = -2 \\times \\text{log-likelihood} + 2k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Information Criterion\n",
    "\n",
    "For model selection with likelihood inference in Bayesian framework.\n",
    "\n",
    "$$\n",
    "BIC = -2 \\times \\text{log-likelihood} + \\log(n)k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shift geer a little to talk about useful metrics for measuring features's multi-collinearity and for detecting influential instance.\n",
    "\n",
    "## Variance Inflation Factor (VIF)\n",
    "\n",
    "To measure multi-collineairity for multiple features.\n",
    "\n",
    "- $R^2_j$: R-square with fitting $X_j$ by $X_k$, for all $k != j$\n",
    "\n",
    "$$\n",
    "VIF_j = 1 / (1 - R^2_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cook Distance\n",
    "\n",
    "To detect influential instance:\n",
    "\n",
    "- $\\hat \\beta$: regression weights\n",
    "- $\\hat \\beta_{(-i)}$: regression weights without instance $i$\n",
    "\n",
    "$$\n",
    "[(\\hat \\beta - \\hat \\beta_{(-i)})^T X^T X (\\hat \\beta - \\hat \\beta_{(-i)}) / [(p + 1) \\sigma^2]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start to introduce lots of metrics basically focusing on classification problems with categorical response; for example binary response $y = 0, 1$.\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "|   True   | Predicted Positive (P)  | Predicted Negative (N)  |\n",
    "|------------------------------------|-------------------------|\n",
    "|Postitive |           TP            |           FN            |\n",
    "|Negative  |           FP            |           TN            |\n",
    "\n",
    "where $n = Total = TP + FP + FN + TN$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accurary\n",
    "\n",
    "The proportion of true positive and negative cases are correctly identified.\n",
    "\n",
    "$$\n",
    "Accurary = (TP + TN) / n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision\n",
    "\n",
    "- The proportion of predicted positive cases are true positive.\n",
    "- Also called **Positive Predictive Value (PPV).**\n",
    "- **Precision-Recall Curve's y-axis** (for PR Curve see later)\n",
    "\n",
    "$$\n",
    "Precision = TP / (\\text{Predicted Positive}) = TP / (TP + FP)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False Discovery Rate (FDR)\n",
    "\n",
    "$$\n",
    "FDR =  1 - Precision = FP / (\\text{Predicted Positive}) = FP / (TP + FP)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall\n",
    "\n",
    "- The proportion of true positive cases are identified correctly as predictive positive\n",
    "- Also called **Sensitivity,** or **True Positive Rate (TPR)**\n",
    "- **ROC's y-axis** (for ROC Curve see later)\n",
    "- **Preciasion-Recall Curve's x-axis**\n",
    "\n",
    "$$\n",
    "Recall = Sensitivity = TP / Positive = TP / P = TP / (TP + FN)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specificity\n",
    "\n",
    "- The proportion of predicted negative cases are true negative\n",
    "- Also called **True Negative Rate (TNR)**\n",
    "- 1 - Specificity: **ROC Curve's x-axis**\n",
    "\n",
    "$$\n",
    "Specificity = TN / N = TN / (FP + TN)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False Positive Rate (FPR)\n",
    "\n",
    "- The proportion of predicted positive cases are true negative\n",
    "- **ROC Curve's x-axis**\n",
    "\n",
    "$$\n",
    "FPR = 1 - Specificity = FP / N = FP / (FP + TN)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area under Curve (AUC)\n",
    "\n",
    "- The following two AUCs are independent of the change in proportion of \n",
    "responders.\n",
    "- To get a single number which was AUC-ROC or PR-AUC for model judgment.\n",
    "- ROC or PR curve is almost independent of the response rate, compared with lift chart.\n",
    "\n",
    "### AUC under Receiver Operating Characteristic (ROC) Curve\n",
    "\n",
    "AUC-ROC: \n",
    "\n",
    "- y-axis: Recall = Sensitivity = TP / P = TP / (TP + FN)\n",
    "- x-axis: FPR = 1 - Specificity = FP / N = FP / (FP + TN)\n",
    "\n",
    "**Performance guideline:**\n",
    "\n",
    "- 0.90-1.00: excellent\n",
    "- 0.80-0.90: good\n",
    "- 0.70-0.80: fair\n",
    "- 0.60-0.70: poor\n",
    "- 0.50-0.60: fail\n",
    "\n",
    "### AUC under Precision-Recall (PR) Curve\n",
    "\n",
    "AUC-PR:\n",
    "\n",
    "- y-axis: Precision = TP / (Predicted Positive) = TP / (TP + FP)\n",
    "- x-axis: Recall = Sensitivity = TP / (True Positive) = TP / P = TP / (TP + FN) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gain / Lift Charts\n",
    "\n",
    "To check the rank ordering of the probabilities:\n",
    "\n",
    "- Step 1 : Calculate probability for each observation\n",
    "- Step 2 : Rank these probabilities in decreasing order.\n",
    "- Step 3 : Build deciles with each group having almost 10% of the observations; group i is located in the (i - 1)th and ith deciles.\n",
    "- Step 4 : Calculate the response rate at each deciles for Good (positive), Bad (negative) and total.\n",
    "\n",
    "Table:\n",
    "\n",
    "- Decile ID: 1,...,10\n",
    "- Count for true negative cases\n",
    "- Count for true positive cases\n",
    "- Grand total count for one decile\n",
    "- %Right\n",
    "- %Wrong\n",
    "- %Population\n",
    "- Cum %Right\n",
    "- Cum %Population\n",
    "- Lift @decile\n",
    "- Total lift\n",
    "\n",
    "### Cumulative Gain Chart\n",
    "\n",
    "- y-axis: Cum %Right\n",
    "- x-axis: Cum %Population\n",
    "\n",
    "**Lift:** Cum %Right / Cum %Population.\n",
    "\n",
    "For example, the first decile with 10% of the population has 14% of positive cases. This means we have a 14% / 10% = 140% lift at first decile.\n",
    "\n",
    "How is this result? We can evaluate the result compared with the **maximum lift at first decile.** \n",
    "\n",
    "- Total number of positive cases are 3850. \n",
    "- Also the first decile contains 543 observations. \n",
    "- So the maximum lift at first decile could have been 543/3850 ~ 14.1%. \n",
    "- Hence, we are quite close to perfection with this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lift Charts\n",
    "\n",
    "Total lift chart:\n",
    "\n",
    "- y-axis: Total lift\n",
    "- x-axis: Cum %Population (decile) \n",
    "\n",
    "Decile-wise lift chart:\n",
    "\n",
    "- y-axis: Lift @decile\n",
    "- x-axis: Cum %Population (decile) \n",
    "\n",
    "The purpose for the decile-wise lift chart? For example,\n",
    "\n",
    "- Our model does well till the 7th decile. Post which every decile will be skewed towards negative cases. \n",
    "- Any model with lift @ decile above 100% till minimum 3rd decile and maximum 7th decile is a good model.\n",
    "\n",
    "\n",
    "### Lift based on conditional probability\n",
    "\n",
    "$$\n",
    "Lift = p(action | feature) / p(action) \\\\\n",
    "     = p(action, feature) / [p(action) * p(feature)]\n",
    "$$\n",
    "\n",
    "**Issue for lift:** Lift is dependent on total response rate of the population. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kolmogorov-Smirnov Chart\n",
    "\n",
    "Table:\n",
    "\n",
    "- Decile ID: 1,...,10\n",
    "- Count for true negative cases\n",
    "- Count for true positive cases\n",
    "- Grand total count for one decile\n",
    "- %Right\n",
    "- %Wrong\n",
    "- %Population\n",
    "- Cum %Right\n",
    "- Cum %Wrong\n",
    "- Cum %Population\n",
    "- K-S: Cum %Right - Cum %Wrong\n",
    "\n",
    "### Kolmogorov-Smirnov Chart\n",
    "\n",
    "Two curves for Cum %Right & Cum %Wrong\n",
    "\n",
    "- y-axis: percent\n",
    "- x-axis: decile\n",
    "\n",
    "**K-S Statistics:** Maximum separation between Cum %Right and Cum %Wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini Coefficient\n",
    "\n",
    "$$\n",
    "\\text{Gini Coefficient} = (\\text{Area between the ROC curve and the diagnol line}) / (\\text{Area of the above triangle}) \\\\\n",
    "= (\\text{Area between the ROC curve and the diagnol line}) / 0.5 \\\\\n",
    "= 2 * (\\text{Area between the ROC curve and the diagnol line}) \\\\\n",
    "= 1 - 2 * (\\text{Area between the ROC curve and the y-axis})\n",
    "$$\n",
    "\n",
    "Gini above 60% is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kendall's tau\n",
    "\n",
    "Kendall's tau: Probability of concordant pairs - Probability of discordant pairs.\n",
    "\n",
    "$$\n",
    "\\tau\n",
    "= {\\frac {({\\text{number of concordant pairs}}) - ({\\text{number of discordant pairs}})}{n(n-1)/2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are variations of Precision or Recall or their combinations.\n",
    "\n",
    "## $F_{\\beta}$ Measures\n",
    "\n",
    "$F_{\\beta}$ Measures is a **harmonic average of Precision & Recall:**\n",
    "\n",
    "$$\n",
    "F_{\\beta} \n",
    "= 1 / {[\\beta^2/(\\beta^2 + 1) \\times (1/Precision)] + 1/(\\beta^2 + 1) * (1/Recall)} \\\\\n",
    "= (\\beta^2 + 1) (Precision \\times Recall) / (\\beta^2 \\times Precision + Recall)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macro / Micro / Weighted Precision and Recall\n",
    "\n",
    "**Macro:** Mean of the binary metrics, giving equal weight to each class. \n",
    "\n",
    "- In problems where infrequent classes are nonetheless important, macro-averaging may be a means of highlighting their performance. \n",
    "-On the other hand, the assumption that all classes are equally important is often untrue, such that macro-averaging will over-emphasize the typically low performance on an infrequent class.\n",
    "\n",
    "**Micro:** Give each sample-class pair an equal contribution to the overall metric (except as a result of sample-weight). \n",
    "\n",
    "- Rather than summing the metric per class, this sums the dividends and divisors that make up the per-class metrics to calculate an overall quotient.\n",
    "- Micro-averaging may be preferred in multilabel settings, including multiclass classification where a majority class is to be ignored.\n",
    "- In multi-class problem, micro Precision & Recall will be same value\n",
    "\n",
    "**Weighted:** accounts for class imbalance by computing the average of binary metrics in which each class’s score is weighted by its presence in the true data sample.\n",
    "\n",
    "### Example: Micro / Macro-Precision / Recall\n",
    "\n",
    "For a set of data:\n",
    "\n",
    "- True positive (TP1) = 12\n",
    "- False positive (FP1) = 9\n",
    "- False negative (FN1) = 3\n",
    "- Precision (P1) = TP1 / (TP1 + FP1) = 57.14% \n",
    "- Recall (R1) = TP1 / (TP1 + FN1) = 80%\n",
    "\n",
    "For a different set of data:\n",
    "\n",
    "- True positive (TP2) = 50\n",
    "- False positive (FP2) = 23\n",
    "- False negative (FN2) = 9\n",
    "- Precision (P2) = TP2 / (TP2 + FP2) = 68.49\n",
    "- Recall (R2) = TP2 / (TP2 + FN2) = 84.75\n",
    "\n",
    "Thus,\n",
    "\n",
    "- Micro-Average Precision = (TP1+TP2) / (TP1+TP2+FP1+FP2) = (12+50) / (12+50+9+23) = 65.96.\n",
    "- Micro-Average Recall = (TP1+TP2) / (TP1+TP2+FN1+FN2) = (12+50) / (12+50+3+9) = 83.78.\n",
    "- Macro-Average Precision = (P1+P2) / 2 = (57.14+68.49) / 2 = 62.82.\n",
    "- Macro-average Recall = (R1+R2) / 2 = (80+84.75) / 2 = 82.25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision@K\n",
    "\n",
    "**Precision@K:** The number of relevant results on the first k results.\n",
    "\n",
    "- Nevertheless, it fails to take into account the positions of the relevant documents among the top k.\n",
    "- More useful than Recall@k since few users will be interested in reading all of their interested articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Precision\n",
    "\n",
    "**Average Precision (AvP):** Averaging the precision over a set of evenly spaced recall levels {0, 0.1, 0.2,..., 1.0}:\n",
    "\n",
    "$$\n",
    "AvP\n",
    "={\\frac {1}{11}}\\sum_{recall \\in \\{0, 0.1, \\ldots, 1.0\\}} p_{\\text{inter_precision}}(recall)\n",
    "$$\n",
    "\n",
    "where $p_{\\text{inter_precision}}(recall)$ is an interpolated precision that takes the maximum precision over all recalls greater than $recall$:\n",
    "\n",
    "$$\n",
    "p_{\\text{inter_precision}}(r)= \\max_{\\tilde r: \\tilde {r} \\geq r} p(\\tilde r)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Average Precision\n",
    "\n",
    "**Mean Average Precision** for a set of queries: Mean of the average precision scores for each query.\n",
    "\n",
    "$$\n",
    "\\mbox{MAP}(Q) = \\frac{1}{\\vert Q\\vert} \\sum_{j=1}^{\\vert Q\\vert} \\frac{1}{m_j}\n",
    "\\sum_{k=1}^{m_j} \\mbox{precision}(recall_{jk})\n",
    "$$\n",
    "\n",
    "where $Q$ is the number of queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discounted Cumulative Gain (DCG)\n",
    "\n",
    "**DCG:** Allow us to consider relative degrees of relevance, not just binary results.\n",
    "\n",
    "$$\n",
    "DCG_{k} = \\sum_{r=2}^{k} \\frac{rel_{r}}{\\log_{2}(r + 1)}. \n",
    "$$\n",
    "\n",
    "where $r$ is the rank, $k$ is the number of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized DCG (nDCG)\n",
    "\n",
    "Since DCGs may vary significantly for different queries or systems, to fairly compare performances the normalised version of DCG uses an ideal DCG, which sorts documents by their relevance, generating an ideal DCG ($IDCG_{r}$) at rank $k$:\n",
    "\n",
    "$$\n",
    "nDCG_{k} = \\frac{DCG_{k}}{IDCG_{k}}. \n",
    "$$\n",
    "\n",
    "Note that in a perfect ranking system, the $DCG_{k}$ will be the same as the $DCG_{k}$, thus this results in $nDCG = 1.0$. All $nDCG$ from different queries or systems are then relative values ranging from 0.0 to 1.0 and comparable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
