{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Metrics\n",
    "\n",
    "- Bowen Li\n",
    "- 2017/11/17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "How to measure our machine learning model's performance? Different problems with different purposes need different metics for us to fairly and effectively make decision. This notebook is to introduce useful metrics for machine learning evaluation.\n",
    "\n",
    "### Metrics Outline\n",
    "\n",
    "- R-Square: To measure model fitness for continuous response\n",
    "- Mean Squared Errors (MSE)\n",
    "- Mean Absolute Errors (MAE)\n",
    "- Akaike Information Criterion (AIC)\n",
    "- Bayesian Information Criterion (BIC)\n",
    "- Variance Inflation Factor (VIF): To measure multi-collineairity for features\n",
    "- Cook Distance: To detect influential instance points\n",
    "- Confusion Matrix\n",
    "- Accurary\n",
    "- Precission / Recall\n",
    "- Specificity\n",
    "- False Positive Rate (FPR)\n",
    "- Area under Curve (AUC)\n",
    "- Gain / Lift Charts\n",
    "- Kolmogorov-Smirnov Chart\n",
    "- Gini Coefficient\n",
    "- Kendall's tau\n",
    "- $F_{\\beta}$-Measures\n",
    "- Macro / Micro / Weighted Precision and Recall\n",
    "- Precision@K\n",
    "- Average Precision (AvP)\n",
    "- Mean Average Precision (MAP)\n",
    "- Discounted Cumulative Gain (DCG)\n",
    "- Normalized DCG (nDCG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-Square\n",
    "\n",
    "For regression problems with continuous response $y$, R-square is to measure goodness of fit for regression model, defined as a ratio: **Explained Variation / Total Variation.**\n",
    "\n",
    "$$\n",
    "R^2 = \\sum_{i=1}^n (\\hat y_i - y_i)^2 / \\sum (y_i - \\bar y)^2\n",
    "$$\n",
    "\n",
    "where $y_i$ is the continuous response for instance $i$, with sample mean $\\bar y$, and $\\hat y_i$ is the predicted value of $y_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Errors (MSE)\n",
    "\n",
    "For regression problems, MSE is a common metric for model evaluation and use **squared errors** to measure the discrepancy between $\\hat y_i$ and $y_i$.\n",
    "\n",
    "$$\n",
    "MSE = \\sum_{i=1}^n (\\hat y_i - y_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Errors (MAE)\n",
    "\n",
    "For regression problems, MAE, instead of using squared errors, uses **absolute errors** to measure prediction discrepancy.\n",
    "\n",
    "$$\n",
    "MAE = \\sum_{i=1}^n |\\hat y_i - y_i|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Akaike Information Criterion (AIC)\n",
    "\n",
    "For model selection based on likelihood inference, we would like to penalize higher likelihood (which is good) by using more features (which is bad). A classic metric is the AIC:\n",
    "\n",
    "$$\n",
    "AIC = -2 \\cdot \\text{log-likelihood} + 2k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Information Criterion\n",
    "\n",
    "For model selection in Bayesian framework, we can use the BIC:\n",
    "\n",
    "$$\n",
    "BIC = -2 \\cdot \\text{log-likelihood} + \\log(n)k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shift geer a little to talk about useful metrics to measure features's multi-collinearity and to detect influential instance.\n",
    "\n",
    "## Variance Inflation Factor (VIF)\n",
    "\n",
    "To measure multi-collineairity for multiple features, the VIF is a classic metric. \n",
    "\n",
    "- First, we calculate $R^2_j$, which is the R-square by fitting $X_j$ by $X_k$, for all $k != j$, \n",
    "- Then, the VIF is to measure other features's interpretation ability to one specific feature, say $X_j$,\n",
    "\n",
    "$$\n",
    "VIF_j = 1 / (1 - R^2_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cook Distance\n",
    "\n",
    "To detect influential instance:\n",
    "\n",
    "- $\\hat \\beta$: usual regression weights\n",
    "- $\\hat \\beta_{(-i)}$: regression weights obtained by excluding instance $i$\n",
    "- The Cook distance is defined by\n",
    "\n",
    "$$\n",
    "(\\hat \\beta - \\hat \\beta_{(-i)})^T X^T X (\\hat \\beta - \\hat \\beta_{(-i)}) \\big / [(p + 1) \\sigma^2]\n",
    "$$\n",
    "\n",
    "The higher Cook distance for instance $i$, the higher likely instance $i$ is a influential point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start to introduce lots of metrics basically focusing on classification problems with categorical response; for example binary response $y = 0, 1$.\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "|   True   | Predicted Positive (P)  | Predicted Negative (N)  |\n",
    "|------------------------------------|-------------------------|\n",
    "|Postitive |           TP            |           FN            |\n",
    "|Negative  |           FP            |           TN            |\n",
    "\n",
    "where $n = Total = TP + FP + FN + TN$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accurary\n",
    "\n",
    "Accuracy is the proportion of true positive and negative cases are correctly identified.\n",
    "\n",
    "$$\n",
    "Accurary = (TP + TN) / n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision\n",
    "\n",
    "Precision is the proportion of predicted positive cases are true positive.\n",
    "\n",
    "- Also called **Positive Predictive Value (PPV)**\n",
    "- **Precision-Recall Curve's y-axis** (for PR Curve see later)\n",
    "\n",
    "$$\n",
    "Precision = TP / (\\textit{Predicted Positive}) = TP / (TP + FP)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False Discovery Rate (FDR)\n",
    "\n",
    "$$\n",
    "FDR =  1 - Precision = FP / (\\textit{Predicted Positive}) = FP / (TP + FP)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall\n",
    "\n",
    "Recall is rhe proportion of true positive cases are identified correctly as predictive positive.\n",
    "\n",
    "- Also called **Sensitivity,** or **True Positive Rate (TPR)**\n",
    "- **ROC Curve's y-axis** (for ROC Curve see later)\n",
    "- **Preciasion-Recall Curve's x-axis**\n",
    "\n",
    "$$\n",
    "Recall = Sensitivity = TP / Positive = TP / P = TP / (TP + FN)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specificity\n",
    "\n",
    "Specificity is the proportion of predicted negative cases among true negative.\n",
    "\n",
    "- Also called **True Negative Rate (TNR)**\n",
    "- $1 - Specificity$: **ROC Curve's x-axis**\n",
    "\n",
    "$$\n",
    "Specificity = TN / N = TN / (FP + TN)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False Positive Rate (FPR)\n",
    "\n",
    "FPR is the proportion of predicted positive cases among true negative.\n",
    "\n",
    "- **ROC Curve's x-axis**\n",
    "\n",
    "$$\n",
    "FPR = 1 - Specificity = FP / N = FP / (FP + TN)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area under Curve (AUC)\n",
    "\n",
    "To get a single number: AUC-ROC or PR-AUC for model selection. Note that ROC or PR Curve is almost independent of the response rate, compared with lift chart (see later).\n",
    "\n",
    "### AUC under Receiver Operating Characteristic (ROC) Curve\n",
    "\n",
    "**ROC Curve:** \n",
    "\n",
    "- y-axis: $Recall = Sensitivity = TP / P = TP / (TP + FN)$\n",
    "- x-axis: $FPR = 1 - Specificity = FP / N = FP / (FP + TN)$\n",
    "\n",
    "**AUC-ROC:** area under the ROC Curve.\n",
    "\n",
    "Performance guideline:\n",
    "\n",
    "- 0.90-1.00: excellent\n",
    "- 0.80-0.90: good\n",
    "- 0.70-0.80: fair\n",
    "- 0.60-0.70: poor\n",
    "- 0.50-0.60: fail\n",
    "\n",
    "### AUC under Precision-Recall (PR) Curve\n",
    "\n",
    "**PR Curve:**\n",
    "\n",
    "- y-axis: $Precision = TP / (\\textit{Predicted Positive}) = TP / (TP + FP)$\n",
    "- x-axis: $Recall = Sensitivity = TP / (\\textit{True Positive}) = TP / P = TP / (TP + FN)$\n",
    "\n",
    "**AUC-PR:** area under the PR Curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gain / Lift Charts\n",
    "\n",
    "To check the **rank ordering of the probabilities,** we can use the Gain Chart or Lift Chart.\n",
    "\n",
    "- Step 1 : Calculate probability for each observation\n",
    "- Step 2 : Rank these probabilities in decreasing order.\n",
    "- Step 3 : Build deciles with each group having almost 10% of the observations; group i is located in the (i - 1)th and ith deciles.\n",
    "- Step 4 : Calculate the response rate at each deciles for Good (positive), Bad (negative) and total.\n",
    "\n",
    "Then we calculate the following information in a table:\n",
    "\n",
    "- Decile ID: 1,...,10\n",
    "- Count for true negative cases\n",
    "- Count for true positive cases\n",
    "- Grand total count for one decile\n",
    "- %Right\n",
    "- %Wrong\n",
    "- %Population\n",
    "- Cum %Right\n",
    "- Cum %Population\n",
    "- Lift @decile\n",
    "- Total lift\n",
    "\n",
    "[Insert Table here??]\n",
    "\n",
    "### Cumulative Gain Chart\n",
    "\n",
    "- y-axis: Cum %Right\n",
    "- x-axis: Cum %Population\n",
    "\n",
    "**Lift:** Cum %Right / Cum %Population.\n",
    "\n",
    "For example, the first decile with 10% of the population has 14% of positive cases. This means we have a 14% / 10% = 140% lift at first decile.\n",
    "\n",
    "[Insert Figure here??]\n",
    "\n",
    "How is this result? We can evaluate the result compared with the **maximum lift at first decile.** For example,\n",
    "\n",
    "- Total number of positive cases are 3850. \n",
    "- Also the first decile contains 543 observations. \n",
    "- So the maximum lift at first decile could have been 543/3850 ~ 14.1%. \n",
    "- Hence, we are quite close to perfection with this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lift Charts\n",
    "\n",
    "**Total lift chart:**\n",
    "\n",
    "- y-axis: Total lift\n",
    "- x-axis: Cum %Population (decile) \n",
    "\n",
    "[Insert Figure here??]\n",
    "\n",
    "**Decile-wise lift chart:**\n",
    "\n",
    "- y-axis: Lift @decile\n",
    "- x-axis: Cum %Population (decile)\n",
    "\n",
    "[Insert Figure here??]\n",
    "\n",
    "The purpose for the decile-wise lift chart? For example,\n",
    "\n",
    "- Our model does well till the 7th decile. Post which every decile will be skewed towards negative cases. \n",
    "- Any model with lift @ decile above 100% till minimum 3rd decile and maximum 7th decile is a good model.\n",
    "\n",
    "\n",
    "### Lift based on conditional probability\n",
    "\n",
    "$$\n",
    "Lift = p(action | feature) / p(action)\n",
    "     = p(action, feature) / [p(action) * p(feature)]\n",
    "$$\n",
    "\n",
    "**Issue for lift:** Lift is dependent on total response rate of the population. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kolmogorov-Smirnov Chart\n",
    "\n",
    "We first calculate the following information in a table:\n",
    "\n",
    "- Decile ID: 1,...,10\n",
    "- Count for true negative cases\n",
    "- Count for true positive cases\n",
    "- Grand total count for one decile\n",
    "- %Right\n",
    "- %Wrong\n",
    "- %Population\n",
    "- Cum %Right\n",
    "- Cum %Wrong\n",
    "- Cum %Population\n",
    "- K-S: Cum %Right - Cum %Wrong\n",
    "\n",
    "[Insert Table here??]\n",
    "\n",
    "### Kolmogorov-Smirnov Chart\n",
    "\n",
    "Two curves for Cum %Right & Cum %Wrong\n",
    "\n",
    "- y-axis: percent\n",
    "- x-axis: decile\n",
    "\n",
    "[Insert Figure here??]\n",
    "\n",
    "**K-S Statistics:** *Maximum separation* between Cum %Right and Cum %Wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini Coefficient\n",
    "\n",
    "Suppose:\n",
    "\n",
    "- $A$: Area between the ROC Curve and the diagnol line\n",
    "- $B$: Area between the ROC Curve and the y-axis\n",
    "- $A+B$: Area between the diagonal line and the y-axis, which is 0.5.\n",
    "\n",
    "$$\n",
    "\\textit{Gini} = \\frac{ A }{ A + B } = \\frac{ A }{ 0.5 } = 2 A = 1 - 2 B\n",
    "$$\n",
    "\n",
    "Gini above 60% is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kendall's tau\n",
    "\n",
    "For two variable vectors of $\\{(X_i, Y_i), i = 1,...,n\\}$, Kendall's tau is defined by **probability of concordant pairs - probability of discordant pairs.** For two vectors $(X_1, Y_1), (X_2, Y_2)$, \n",
    "\n",
    "- **Concordant pairs:** $(X_2 - X_1)(Y_2 - Y_1) > 0$, e.g., $X_2 > X_1$ and $Y_2 > Y_1$\n",
    "- **Discordant pairs:** $(X_2 - X_1)(Y_2 - Y_1) < 0$, e.g., $X_2 > X_1$ and $Y_2 < Y_1$\n",
    "\n",
    "$$\n",
    "\\tau\n",
    "= \\frac{\\sum_{i < j} I[(X_j - X_i)(Y_j - Y_i) > 0] - \\sum_{i < j} I[(X_j - X_i)(Y_j - Y_i) < 0]}{n(n-1)/2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are variations of Precision or Recall or their combinations.\n",
    "\n",
    "## $F_{\\beta}$-Measures\n",
    "\n",
    "$F_{\\beta}$-Measures is a **harmonic average of Precision & Recall:** for $\\beta > 0$,\n",
    "\n",
    "$$\n",
    "F_{\\beta} \n",
    "= \\frac{1}{\\frac{\\beta^2}{\\beta^2 + 1} \\cdot \\frac{1}{Precision} + \\frac{1}{\\beta^2 + 1} \\cdot \\frac{1}{Recall}}\n",
    "= \\frac{ (\\beta^2 + 1) Precision \\cdot Recall }{ \\beta^2 \\cdot Precision + Recall}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macro / Micro / Weighted Precision and Recall\n",
    "\n",
    "**Macro:** Mean of the binary metrics, giving equal weight to each class. \n",
    "\n",
    "- In problems where infrequent classes are nonetheless important, macro-averaging may be a means of highlighting their performance. \n",
    "- On the other hand, the assumption that all classes are equally important is often untrue, such that macro-averaging will over-emphasize the typically low performance on an infrequent class.\n",
    "\n",
    "**Micro:** Give each sample-class pair an equal contribution to the overall metric (except as a result of sample-weight). \n",
    "\n",
    "- Rather than summing the metric per class, this sums the dividends and divisors that make up the per-class metrics to calculate an overall quotient.\n",
    "- Micro-averaging may be preferred in multilabel settings, including multiclass classification where a majority class is to be ignored.\n",
    "- In multi-class problem, micro Precision & Recall will be same value\n",
    "\n",
    "**Weighted:** accounts for class imbalance by computing the average of binary metrics in which each class’s score is weighted by its presence in the true data sample.\n",
    "\n",
    "### Example: Micro / Macro-Precision / Recall\n",
    "\n",
    "For a set of data:\n",
    "\n",
    "- True positive (TP1) = 12\n",
    "- False positive (FP1) = 9\n",
    "- False negative (FN1) = 3\n",
    "- Precision (P1) = TP1 / (TP1 + FP1) = 57.14% \n",
    "- Recall (R1) = TP1 / (TP1 + FN1) = 80%\n",
    "\n",
    "For a different set of data:\n",
    "\n",
    "- True positive (TP2) = 50\n",
    "- False positive (FP2) = 23\n",
    "- False negative (FN2) = 9\n",
    "- Precision (P2) = TP2 / (TP2 + FP2) = 68.49\n",
    "- Recall (R2) = TP2 / (TP2 + FN2) = 84.75\n",
    "\n",
    "Thus,\n",
    "\n",
    "- Micro-Average Precision = (TP1+TP2) / (TP1+TP2+FP1+FP2) = (12+50) / (12+50+9+23) = 65.96.\n",
    "- Micro-Average Recall = (TP1+TP2) / (TP1+TP2+FN1+FN2) = (12+50) / (12+50+3+9) = 83.78.\n",
    "- Macro-Average Precision = (P1+P2) / 2 = (57.14+68.49) / 2 = 62.82.\n",
    "- Macro-average Recall = (R1+R2) / 2 = (80+84.75) / 2 = 82.25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision@K\n",
    "\n",
    "Precision@K is the **number of relevant results on the first k results.**\n",
    "\n",
    "- Nevertheless, it fails to take into account the positions of the relevant documents among the top k.\n",
    "- More useful than Recall@k since few users will be interested in reading all of their interested articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Precision\n",
    "\n",
    "Average Precision (AvP) is defined by **averaging the precision over a set of evenly spaced recall levels {0, 0.1, 0.2,..., 1.0}:**\n",
    "\n",
    "$$\n",
    "AvP\n",
    "={\\frac {1}{11}}\\sum_{recall \\in \\{0, 0.1, \\ldots, 1.0\\}} precision_{\\text{inter}}(recall)\n",
    "$$\n",
    "\n",
    "where $precision_{\\text{inter}}(recall)$ is an interpolated precision that takes the **maximum precision over all recalls greater than $recall$:**\n",
    "\n",
    "$$\n",
    "precision_{\\text{inter}}(r)= \\max_{\\tilde r: \\tilde {r} \\geq r} precision(\\tilde r)\n",
    "$$\n",
    "\n",
    "[Insert Figure here??]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Average Precision\n",
    "\n",
    "Mean Average Precision is for a set of queries, defined by the **mean of the average precision scores for each query.**\n",
    "\n",
    "$$\n",
    "MAP(Q) = \\frac{1}{\\vert Q\\vert} \\sum_{j=1}^{\\vert Q\\vert} \\frac{1}{m_j}\n",
    "\\sum_{k=1}^{m_j} precision(recall_{jk})\n",
    "$$\n",
    "\n",
    "where $Q$ is the number of queries.\n",
    "\n",
    "[Insert Figure here??]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discounted Cumulative Gain (DCG)\n",
    "\n",
    "DCG allows us to consider, instead of binary results for document relevance, **relative degrees of relevance.**\n",
    "\n",
    "$$\n",
    "DCG_{k} = \\sum_{r=2}^{k} \\frac{rel_{r}}{\\log_{2}(r + 1)}\n",
    "$$\n",
    "\n",
    "where $r$ is the rank, $rel_r$ is the relative degree of relevance for rank $r$, and $k$ is the number of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized DCG (nDCG)\n",
    "\n",
    "Since DCG may vary significantly for different queries or systems, to fairly compare performances the **normalised version of DCG uses an ideal DCG,** which sorts documents by their relevance, generating an ideal DCG, $IDCG_{r}$, at rank $r$:\n",
    "\n",
    "$$\n",
    "nDCG_{k} = \\frac{DCG_{k}}{IDCG_{k}}\n",
    "$$\n",
    "\n",
    "Note that in a perfect ranking system, the $DCG_{k}$ will be the same as the $DCG_{k}$, thus this results in $nDCG_k = 1.0$. All $nDCG$ from different queries or systems are then relative values ranging from 0.0 to 1.0 and comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Add references."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
