{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA and SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bowen Li\n",
    "- 2016/10/05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "- [What is Principal Component Analysis (PCA)?](#What-is-PCA?)\n",
    "  * [Assumptions for PCA](#Assumptions-for-PCA)\n",
    "  * [PCA Computation](#PCA-Computation)\n",
    "  * [Eigenvector-Decomposition](#Eigenvector-Decomposition)\n",
    "- [What is Singular Value Decomposition (SVD)?](#What-is-SVD?)\n",
    "- [PCA and SVD](#PCA-and-SVD)\n",
    "- [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PCA?\n",
    "\n",
    "The motivation of PCA is to identify the most meaningful basis to re-express data which can maximize the [signal-to-noise ratio (SNR):](https://en.wikipedia.org/wiki/Signal-to-noise_ratio) $\\sigma^2_{signal} / \\sigma^2_{noise}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/snr.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Maximize the signal, measured by the magnitude of variance; large variance corresponds to interesting structure.\n",
    "- Minimize redundancy (i.e. noise), measured by the magnitude of covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the observed data be\n",
    "\n",
    "$$\n",
    "\\underset{(n \\times p)}X = \\{x_{ij}\\} =\n",
    "\\begin{bmatrix}\n",
    "x_{11} & \\cdots & x_{1p} \\\\\n",
    "\\vdots &        & \\vdots \\\\ \n",
    "x_{n1} & \\cdots & x_{np}\n",
    "\\end{bmatrix}\n",
    "= [x_1, \\cdots, x_p]\n",
    "$$\n",
    "\n",
    "where each row represents an example measured on $p$-dimensional features, and each column, $x_j$, represents normalized feature with zero means, $E(x_j) = 0$, and variance one, $Var(x_j) = 1$.\n",
    "\n",
    "$$\n",
    "x_j = \n",
    "\\begin{bmatrix}\n",
    "x_{1j} \\\\\n",
    "\\vdots \\\\ \n",
    "x_{nj}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The covariance matrix of $X$ is\n",
    "\n",
    "$$\n",
    "\\underset{(p \\times p)}{\\Sigma_X} = \\frac{1}{n}X^{T}X \n",
    "= \\frac{1}{n}\n",
    "\\begin{bmatrix}\n",
    "x^T_{1} \\\\\n",
    "\\vdots \\\\ \n",
    "x^T_{p}\n",
    "\\end{bmatrix}\n",
    "[x_1,...,x_p]\n",
    "= \\frac{1}{n}\n",
    "\\begin{bmatrix}\n",
    "x^T_{1}x_1 & \\cdots & x^T_{1}x_p \\\\\n",
    "\\vdots     &        & \\vdots \\\\ \n",
    "x^T_{p}x_1 & \\cdots & x^T_{p}x_p\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Our goal is to find some transformation, $Y = f(X)$, of X such that covariance of $Y$ is a *diagonal* matrix,\n",
    "\n",
    "$$\n",
    "\\underset{(p \\times p)}{\\Sigma_Y} = \\frac{1}{n}Y^{T}Y\n",
    "$$\n",
    "\n",
    "There are many transformations for diagonalizing the convariance matrix $\\Sigma_X$, PCA arguably selects the easiest one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions for PCA\n",
    "- Linear transformation: Let $\\underset{p \\times p}E = [e_1,...,e_p]$,\n",
    "\n",
    "$$\n",
    "Y = XE\n",
    "$$\n",
    "\n",
    "- Large variance have important structure.\n",
    "- The principal components are orthogonal. \n",
    "\n",
    "The last assumption provides an intuitive simplification that makes PCA soluble with linear algebra decomposition techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Computation\n",
    "\n",
    "Based on the above assumptions, define\n",
    "- The 1st principal component = linear combination $Xe_1$ that maximizes $Var(Xe_1)$, subject to $e^T_1e_1 = 1$.\n",
    "- The 2nd principal component = linear combination $Xe_2$ that maximizes $Var(Xe_2)$, subject to $e^T_2e_2 = 1$ and $Cov(Xe_1, Xe_2) = 0$.\n",
    "- The $j$th principal component = linear combination $Xe_j$ that maximizes $Var(Xe_j)$, subject to $e^T_j e_j = 1$ and $Cov(Xe_k, Xe_j) = 0$ for $k < j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvector Decomposition\n",
    "\n",
    "The [eigenvector decomposition](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) is also called the [spectral secomposition](https://en.wikipedia.org/wiki/Spectral_theorem). Suppose $Y = XE$, its covariance matrix\n",
    "\n",
    "$$\n",
    "\\Sigma_Y = \\frac{1}{n}Y^TY = \\frac{1}{n}(XE)^T(XE) = E^T \\left(\\frac{1}{n}X^TX \\right)E\n",
    "$$\n",
    "\n",
    "Since $\\frac{1}{n}X^TX$ is a *symmetric* matrix, it can be diagonalized by a matrix of orthonormal eigenvectors using the spectral decomposistion,\n",
    "\n",
    "$$\n",
    "\\frac{1}{n}X^TX = VDV^T\n",
    "$$\n",
    "\n",
    "where $D = diag(d_1,...,d_p)$ is a diagonal matrix with rank-ordered set of eigenvalues, $d_1 \\ge d_2 \\ge ... \\ge d_p$, and $V = [v_1,...,v_p]$ is the matrix of the corresponding eigenvectors with $V^TV=VV^T = I$.\n",
    "\n",
    "*Sketch of proof:* From the spectral decomposition, for all $j$, we have \n",
    "\n",
    "$$\n",
    "\\frac{1}{n}X^T X v_j = d_j v_j\n",
    "$$\n",
    "\n",
    "Then we have $\\frac{1}{n}X^T X V = V D$ which implies $\\frac{1}{n}X^T X = V D V^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Alternative proof:* We can interpret PCA as finding a unit vector $u$, with $|u| = 1$, which maximizes variance of the projection of all example $x_i$'s onto $u$,\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^n (x_i^T u)^2 \n",
    "= \\frac{1}{n} \\sum_{i=1}^n (x_i^T u)^T (x_i^T u)\n",
    "= u^T \\big(\\frac{1}{n} \\sum_{i=1}^n x_i x_i^T \\big) u\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's digress a little to ask why the variance of the projection of all examples $x_i$'s onto $u$ has form like the above? First, suppose the angle between $x_i$ and $u$ is $\\theta$, then the projection of each example $x_i$ onto $u$ is\n",
    "\n",
    "$$\n",
    "x_i' = x_i cos\\theta\n",
    "= x_i \\frac{x_i^T u}{|x_i||u|}\n",
    "= x_i \\frac{x_i^T u}{|x_i|}\n",
    "$$\n",
    "\n",
    "since $u$ is a unit vector. Then the length of the projection of one example $x_i$'s onto $u$ is\n",
    "\n",
    "$$\n",
    "\\left[ x_i'^T x_i' \\right]^{1/2}\n",
    "= \\left[ \\left( x_i \\frac{x_i^T u}{|x_i|} \\right)^T \\left( x_i \\frac{x_i^T u}{|x_i|} \\right) \\right]^{1/2}\n",
    "= \\left[ \\left( \\frac{x_i^T u}{|x_i|} \\right) x_i^T x_i \\left( \\frac{x_i^T u}{|x_i|} \\right) \\right]^{1/2}\n",
    "= \\left[ \\left(x_i^T u \\right)^2 \\right]^{1/2}\n",
    "= x_i^T u\n",
    "$$\n",
    "\n",
    "since $x_i^T x_i = |x_i|^2$. Finally, the projection of one example $x_i$'s onto $u$ is the distance $x_i^T u$ from the origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go back to our optimization problem, we would like to solve\n",
    "\n",
    "$$\n",
    "max_u u^T \\big(\\frac{1}{n} \\sum_{i=1}^n x_i x_i^T \\big) u\n",
    "$$\n",
    "\n",
    "such that $u^T u = 1$. Using the **Lagrange Multiplier Method,** the objective becomes\n",
    "\n",
    "$$\n",
    "max_u L(u, \\lambda) = u^T \\big(\\frac{1}{n} \\sum_{i=1}^n x_i x_i^T \\big) u - \\lambda (u^T u - 1)\n",
    "$$\n",
    "\n",
    "Take partial derivative with respect to $u$ we have\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial u} L(u, \\lambda) = \\frac{1}{n} X^T X u - \\lambda u := 0\n",
    "$$\n",
    "\n",
    "Hence, we are solving $\\frac{1}{n} X^T X u = \\lambda u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trick:** Select $E = V$. Then since $V^TV=VV^T = I$, $C_Y$ now is a diagonal matrix\n",
    "\n",
    "$$\n",
    "C_Y = E^T \\left( \\frac{1}{n}X^T X \\right) E\n",
    "    = V^T \\left( V D V^T \\right) V = D\n",
    "$$\n",
    "\n",
    "From the above results we can observe that the **eigenvalue $d_j$ is the variance of transformed feature $y_j$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of PCA:**\n",
    "- Substract off the mean of each feature: $x_j \\leftarrow x_j - \\overline x_j$, and divide the standard deviation: $x_j \\leftarrow x_j/\\sigma_j$\n",
    "- Compute the eigenvectors of covariance matrix, $\\frac{1}{n}X^TX$\n",
    "- Principal components of $X$: eigenvectors, $v_i$, of covariance matrix\n",
    "- The ith diagonal value of $C_Y$: variance of $X$ along $v_i$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is SVD?\n",
    "First recall the eigenvector decomposition: $\\frac{1}{n}X^T X v_j = d_j v_j$. Define\n",
    "- **Singular values:** $\\lambda_j = \\sqrt{d_j}$, **square root of eigenvalue (variance)** for $j = 1,...,k$.\n",
    "- Matrix $U = [ u_1,...,u_2 ]$, with $u_j$ defined by\n",
    "\n",
    "$$\n",
    "u_j = \\frac{1}{\\lambda_j}\\left( \\frac{1}{\\sqrt{n}}X \\right) v_j\n",
    "$$\n",
    "\n",
    "where $v_j$ is the eigenvector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then (1) $U$ is *orthonormal* matrix:\n",
    "$u_j^T u_k = \n",
    "\\begin{cases} \n",
    "1, \\text{if } j = k \\\\\n",
    "0, \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "and (2) $\\lVert X v_j\\rVert = \\lambda_j$\n",
    "\n",
    "- *Sketch of proof:* $u_j^T u_k = \\frac{1}{n} \\left( \\frac{1}{\\lambda_j} X v_j \\right)^T \\left( \\frac{1}{\\lambda_k} X v_k \\right) = \\frac{1}{n} \\frac{1}{\\lambda_j \\lambda_k} v_j^T X^T X v_k = \\frac{1}{\\lambda_j \\lambda_k} v_j^T d_j v_k = \\frac{\\lambda_j}{\\lambda_k} v_j^T v_k$.\n",
    "\n",
    "The first result follows. Further, we can obtain the second result similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By rewriting we have\n",
    "\n",
    "$$\n",
    "\\left( \\frac{1}{\\sqrt{n}}X \\right) v_j = \\lambda_j u_j\n",
    "$$\n",
    "\n",
    "That is, **normalized $X$ multiplied by an eigenvector of covariance of $X$, $\\frac{1}{n} X^T X$, is equal to a scalar times another vector.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By constructing a new *diagonal* matrix of singular values, $\\Sigma = diag(\\lambda_1,...,\\lambda_p)$, where $\\lambda_1 \\ge \\lambda_2 \\ge ... \\ge \\lambda_p$.\n",
    "- Define $V = [v_1,...,v_p]$ and $U = [u_1,...,u_p]$.\n",
    "\n",
    "**Matrix Version of SVD:**\n",
    "\n",
    "$$\n",
    "\\left( \\frac{1}{\\sqrt{n}}X \\right) V = U \\Sigma,\n",
    "$$\n",
    "$$\n",
    "\\frac{1}{\\sqrt{n}}X = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "\n",
    "Hence, **any arbitrary matrix $\\frac{1}{\\sqrt{n}}X$ can be converted to**\n",
    "- **an orthogonal matrix (rotation): $U$**\n",
    "- **a diagonal matrix (stretch): $\\Sigma$**\n",
    "- **another orthogonal matrix (second rotation): $V$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA and SVD\n",
    "From SVD, we have\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} X^T X = \\left( \\frac{1}{\\sqrt{n}}X \\right)^T \\left( \\frac{1}{\\sqrt{n}}X \\right)\n",
    "                  = \\left( U \\Sigma V^T \\right)^T \\left( U \\Sigma V^T \\right) \\\\\n",
    "                  = \\left( V \\Sigma U^T U \\Sigma V^T \\right)\n",
    "                  = V \\Sigma^2 V^T \\equiv V D V^T\n",
    "$$\n",
    "\n",
    "That is, **squared singular value is equal to variance of $X$ along $v_j$, $\\lambda_j^2 = d_j$.**\n",
    "\n",
    "**How many pricipal components we shall use?**\n",
    "- **Total variances:** $\\textstyle \\sum_{j=1}^p \\lambda_j^2$.\n",
    "- **Scree plot:** Choose the number of principal component by **Elbow Method.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/scree_plot_pca.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More on SVD:**\n",
    "- Recall from SVD, $\\frac{1}{\\sqrt{n}}X = U \\Sigma V^T$\n",
    "- Let $s < p = rank(X)$. Then the **reduced rank-$s$ least squares approximation to $\\frac{1}{\\sqrt{n}} X$ is**\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sqrt{n}}\\widehat{X} = \\sum_{j=1}^s \\lambda_j u_j v_j^T\n",
    "$$\n",
    "\n",
    "which **minimizes**\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^p \\left( x_{ij} - \\widehat{x}_{ij} \\right)^2\n",
    "= tr \\left[ \\left(\\frac{1}{\\sqrt{n}} (X - \\widehat{X}) \\right) \\left(\\frac{1}{\\sqrt{n}} (X - \\widehat{X}) \\right)^T \\right]\n",
    "$$\n",
    "\n",
    "over all matrices $\\widehat{X}$ having rank no greater than $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Johnson & Wichern (2002). Applied Multivariate Statistical Analysis.\n",
    "- Shlens (arXiv, 2014). A Tutorial on Principal Component Analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
