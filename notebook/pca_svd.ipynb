{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis and Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bowen Li\n",
    "- 2016/10/05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "- What is Principal Component Analysis (PCA)?\n",
    "- What is Singular Value Decomposition (SVD)?\n",
    "- PCA and SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is PCA?\n",
    "\n",
    "**Motivation:** Identify the most meaningful basis to re-express data which can maximize the **signal-to-noise ratio (SNR):** $\\sigma^2_{signal} / \\sigma^2_{noise}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/snr.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Minimize redundancy (noise), measured by the magnitude of the covariance.\n",
    "- Maximize the signal, measured by the variance; large variance corresponds to interesting structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the observed data be\n",
    "\n",
    "$$\n",
    "\\underset{(n \\times p)}X = \\{x_{ij}\\} =\n",
    "\\begin{bmatrix}\n",
    "x_{11} & \\cdots & x_{1p} \\\\\n",
    "\\vdots &        & \\vdots \\\\ \n",
    "x_{n1} & \\cdots & x_{np}\n",
    "\\end{bmatrix}\n",
    "= [x_1, \\cdots, x_p]\n",
    "$$\n",
    "\n",
    "- each row represents an *example* measured on $p$-dimensional features,\n",
    "- each column, $x_j$, represents *feature* with *zero means*, $E(x_j) = 0$, or *centered variables,* $x_j \\leftarrow x_j - \\overline x_j$,\n",
    "\n",
    "$$\n",
    "x_j = \n",
    "\\begin{bmatrix}\n",
    "x_{1j} \\\\\n",
    "\\vdots \\\\ \n",
    "x_{nj}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Covariance matrix:**\n",
    "\n",
    "$$\n",
    "\\underset{(p \\times p)}{C_X} = \\frac{1}{n}X^{T}X \n",
    "= \\frac{1}{n}\n",
    "\\begin{bmatrix}\n",
    "x^T_{1} \\\\\n",
    "\\vdots \\\\ \n",
    "x^T_{p}\n",
    "\\end{bmatrix}\n",
    "[x_1,...,x_p]\n",
    "= \\frac{1}{n}\n",
    "\\begin{bmatrix}\n",
    "x^T_{1}x_1 & \\cdots & x^T_{1}x_p \\\\\n",
    "\\vdots     &        & \\vdots \\\\ \n",
    "x^T_{p}x_1 & \\cdots & x^T_{p}x_p\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Goal:** Find some transformation, $Y = f(X)$, of X such that covariance of $Y$ is a *diagonal* matrix,\n",
    "\n",
    "$$\n",
    "\\underset{(p \\times p)}{C_Y} = \\frac{1}{n}Y^{T}Y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways for diagonalizing $C_X$, PCA arguably select the easiest method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assumptions for PCA:**\n",
    "- Linear transformation: Let $\\underset{p \\times p}E = [e_1,...,e_p]$,\n",
    "\n",
    "$$\n",
    "Y = XE\n",
    "$$\n",
    "\n",
    "- Large variance have important structure.\n",
    "- The principal components are orthogonal. \n",
    "\n",
    "The last assumption provides an intuitive simplification that makes PCA soluble with linear algebra decomposition techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computation for PCA:** Based on the above assumptions, define\n",
    "- First principal component = linear combination $e^T_1X$ that maximizes $Var(e^T_1X)$, subject to $e^T_1e_1 = 1$.\n",
    "- Second principal component = linear combination $e^T_2X$ that maximizes $Var(e^T_2X)$, subject to $e^T_2e_2 = 1$ and $Cov(e^T_1X, e^T_2X) = 0$.\n",
    "- At the ith step, ith principal component = linear combination $e^T_iX$ that maximizes $Var(e^T_iX)$, subject to $e^T_ie_i = 1$ and $Cov(e^T_kX, e^T_iX) = 0$ for $k < i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eigenvector decomposition (also called spectral decomposition):**\n",
    "- Suppose $Y = XE$,\n",
    "\n",
    "$$\n",
    "C_Y = \\frac{1}{n}Y^TY = \\frac{1}{n}(XE)^T(XE) = E^T \\left(\\frac{1}{n}X^TX \\right)E\n",
    "$$\n",
    "\n",
    "- Since $\\frac{1}{n}X^TX$ is a *symmetric* matrix, it can be **diagonalized by a matrix of orthonormal eigenvectors** using **spectral decomposistion.** That is,\n",
    "\n",
    "$$\n",
    "\\frac{1}{n}X^TX = VDV^T\n",
    "$$\n",
    "\n",
    "where $D = diag(d_1,...,d_p)$ is a *diagonal* matrix with rank-ordered set of **eigenvalues**, $d_1 \\ge d_2 \\ge ... \\ge d_p$, and $V = [v_1,...,v_p]$ is the matrix of the corresponding **eigenvectors** with $V^TV=VV^T=1$.\n",
    "\n",
    "- *Sketch of proof:* From the spectral decomposition, for all $i$, we have \n",
    "\n",
    "$$\n",
    "\\frac{1}{n}X^T X v_i = d_i v_i\n",
    "$$\n",
    "\n",
    "Then, $\\frac{1}{n}X^T X V = V D \\Rightarrow \\frac{1}{n}X^T X = V D V^T$.\n",
    "\n",
    "- Trick: Select $E = V$. Then since $V^TV=VV^T = I$, $C_Y$ now is a diagonal matrix\n",
    "\n",
    "$$\n",
    "C_Y = E^T \\left( \\frac{1}{n}X^T X \\right) E\n",
    "    = V^T \\left( V D V^T \\right) V = D\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of PCA:**\n",
    "- Substract off the mean of each feature: $x_j \\leftarrow x_j - \\overline x_j$\n",
    "- Compute the eigenvectors of covariance matrix, $\\frac{1}{n}X^TX$\n",
    "- Principal components of $X$: eigenvectors, $v_i$, of covariance matrix\n",
    "- The ith diagonal value of $C_Y$: variance of $X$ along $v_i$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is SVD?\n",
    "First recall the eigenvector decomposition: $\\frac{1}{n}X^T X v_i = d_i v_i$. Define\n",
    "- **Singular values:** $\\lambda_i = \\sqrt{d_i}$, for $i = 1,...,k$\n",
    "- Matrix $U = [ u_1,...,u_2 ]$, with $u_i$ defined by\n",
    "\n",
    "$$\n",
    "u_i = \\frac{1}{\\lambda_i}\\left( \\frac{1}{\\sqrt{n}}X \\right) v_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then (1) $U$ is *orthonormal* matrix:\n",
    "$u_i^T u_j = \n",
    "\\begin{cases} \n",
    "1, \\text{if } i =j \\\\\n",
    "0, \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "and (2) $\\lVert X v_i\\rVert = \\lambda_i$\n",
    "\n",
    "- *Sketch of proof:* $u_i^T u_j = \\frac{1}{n} \\left( \\frac{1}{\\lambda_i} X v_i \\right)^T \\left( \\frac{1}{\\lambda_j} X v_j \\right) = \\frac{1}{n} \\frac{1}{\\lambda_i \\lambda_j} v_i^T X^T X v_j = \\frac{1}{\\lambda_i \\lambda_j} v_i^T d_i v_j = \\frac{\\lambda_i}{\\lambda_j} v_i^T v_j$. \n",
    "The first result follows. Further, we can obtain the second result similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By rewriting we have\n",
    "\n",
    "$$\n",
    "\\left( \\frac{1}{\\sqrt{n}}X \\right) v_i = \\lambda_i u_i\n",
    "$$\n",
    "\n",
    "That is, **normalized $X$ multiplied by an eigenvector of covariance of $X$, $\\frac{1}{n} X^T X$, is equal to a scalar times another vector.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By constructing a new *diagonal* matrix of singular values, $\\Sigma = diag(\\lambda_1,...,\\lambda_p)$, where $\\lambda_1 \\ge \\lambda_2 \\ge ... \\ge \\lambda_p$.\n",
    "- Define $V = [v_1,...,v_p]$ and $U = [u_1,...,u_p]$.\n",
    "\n",
    "**Matrix version of SVD:**\n",
    "\n",
    "$$\n",
    "\\left( \\frac{1}{\\sqrt{n}}X \\right) V = U \\Sigma,\n",
    "$$\n",
    "$$\n",
    "\\frac{1}{\\sqrt{n}}X = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "\n",
    "Hence, **any arbitrary matrix $\\frac{1}{\\sqrt{n}}X$ can be converted to**\n",
    "- **an orthogonal matrix (rotation): $U$**\n",
    "- **a diagonal matrix (stretch): $\\Sigma$**\n",
    "- **another orthogonal matrix (second rotation): $V$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCA and SVD\n",
    "From SVD, we have\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} X^T X = \\left( \\frac{1}{\\sqrt{n}}X \\right)^T \\left( \\frac{1}{\\sqrt{n}}X \\right)\n",
    "                  = \\left( U \\Sigma V^T \\right)^T \\left( U \\Sigma V^T \\right) \\\\\n",
    "                  = \\left( V \\Sigma U^T U \\Sigma V^T \\right)\n",
    "                  = V \\Sigma^2 V^T \\equiv V D V^T\n",
    "$$\n",
    "\n",
    "That is, **squared singular value is equal to variance of $X$ along $v_i$, $\\lambda_i^2 = d_i$.**\n",
    "\n",
    "**How many pricipal components we shall use?**\n",
    "- **Total variances:** $\\textstyle \\sum_{k=1}^p \\lambda_k^2$.\n",
    "- **Scree plot:** Choose the number of principal component by *elbow method.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/scree_plot_pca.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More on SVD:**\n",
    "- Recall from SVD, $\\frac{1}{\\sqrt{n}}X = U \\Sigma V^T$\n",
    "- Let $s < p = rank(X)$. Then the **reduced rank-$s$ least squares approximation to $\\frac{1}{\\sqrt{n}} X$ is**\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sqrt{n}}\\widehat{X} = \\sum_{k=1}^s \\lambda_i u_i v_i^T\n",
    "$$\n",
    "\n",
    "which **minimizes**\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^p \\left( x_{ij} - \\widehat{x}_{ij} \\right)^2\n",
    "= tr \\left[ \\left(\\frac{1}{\\sqrt{n}} (X - \\widehat{X}) \\right) \\left(\\frac{1}{\\sqrt{n}} (X - \\widehat{X}) \\right)^T \\right]\n",
    "$$\n",
    "\n",
    "over all matrices $\\widehat{X}$ having rank no greater than $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Johnson & Wichern (2002). Applied Multivariate Statistical Analysis.\n",
    "- Shlens (arXiv, 2014). A Tutorial on Principal Component Analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
