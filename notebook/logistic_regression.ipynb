{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Logistic regression is one of the most fundamental machine learning models for binary classification. I will summarize its methodology and implement it from scratch using NumPy.\n",
    "\n",
    "Binary classification\n",
    "For example, the doctor would like to base on patients's features, including mean radius, mean texture, etc, to classify breat cancer into one of the following two case:\n",
    "\n",
    "\"malignant\":  ùë¶=1 \n",
    "\"benign\":  ùë¶=0 \n",
    "which correspond to serious and gentle case respectively.\n",
    "\n",
    "We would like to load the breast cancer data from scikit-learn as a toy dataset, and split the data into the training and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression Model\n",
    "\n",
    "[To be continued.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression as LogisticRegressionSklearn\n",
    "\n",
    "# https://github.com/bowen0701/machine-learning/blob/master/np_logistic_regression.py\n",
    "from np_logistic_regression import LogisticRegression\n",
    "\n",
    "# https://github.com/bowen0701/machine-learning/blob/master/np_metrics.py\n",
    "from np_metrics import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read breast cancer data.\n",
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.799e+01, 1.038e+01, 1.228e+02, 1.001e+03, 1.184e-01, 2.776e-01,\n",
       "        3.001e-01, 1.471e-01, 2.419e-01, 7.871e-02, 1.095e+00, 9.053e-01,\n",
       "        8.589e+00, 1.534e+02, 6.399e-03, 4.904e-02, 5.373e-02, 1.587e-02,\n",
       "        3.003e-02, 6.193e-03, 2.538e+01, 1.733e+01, 1.846e+02, 2.019e+03,\n",
       "        1.622e-01, 6.656e-01, 7.119e-01, 2.654e-01, 4.601e-01, 1.189e-01],\n",
       "       [2.057e+01, 1.777e+01, 1.329e+02, 1.326e+03, 8.474e-02, 7.864e-02,\n",
       "        8.690e-02, 7.017e-02, 1.812e-01, 5.667e-02, 5.435e-01, 7.339e-01,\n",
       "        3.398e+00, 7.408e+01, 5.225e-03, 1.308e-02, 1.860e-02, 1.340e-02,\n",
       "        1.389e-02, 3.532e-03, 2.499e+01, 2.341e+01, 1.588e+02, 1.956e+03,\n",
       "        1.238e-01, 1.866e-01, 2.416e-01, 1.860e-01, 2.750e-01, 8.902e-02],\n",
       "       [1.969e+01, 2.125e+01, 1.300e+02, 1.203e+03, 1.096e-01, 1.599e-01,\n",
       "        1.974e-01, 1.279e-01, 2.069e-01, 5.999e-02, 7.456e-01, 7.869e-01,\n",
       "        4.585e+00, 9.403e+01, 6.150e-03, 4.006e-02, 3.832e-02, 2.058e-02,\n",
       "        2.250e-02, 4.571e-03, 2.357e+01, 2.553e+01, 1.525e+02, 1.709e+03,\n",
       "        1.444e-01, 4.245e-01, 4.504e-01, 2.430e-01, 3.613e-01, 8.758e-02]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test datasets.\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=71, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30) (426,)\n",
      "(143, 30) (143,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_raw.shape, y_train.shape)\n",
    "print(X_test_raw.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering for standardizing features by min-max scaler.\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "X_train = min_max_scaler.fit_transform(X_train_raw)\n",
    "X_test = min_max_scaler.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fitting Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss 1.7624097476465403\n",
      "epoch 11: loss 0.08531405379884446\n",
      "epoch 21: loss 0.07546780421763014\n",
      "epoch 31: loss 0.06061107482508217\n",
      "epoch 41: loss 0.07066675347132022\n",
      "epoch 51: loss 0.06624984449545909\n",
      "epoch 61: loss 0.07946156456165038\n",
      "epoch 71: loss 0.04244837018695555\n",
      "epoch 81: loss 0.04599355450292453\n",
      "epoch 91: loss 0.20816019466056923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<np_logistic_regression.LogisticRegression at 0x7f3e902949b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit our Logistic Regression.\n",
    "clf = LogisticRegression(batch_size=100, lr=5, n_epochs=100)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[12.58238886]]),\n",
       " array([-1.44828804, -2.4451783 , -1.54606817, -2.38209972, -0.53998014,\n",
       "         0.1065096 , -3.59740319, -5.51118006, -0.75081283,  2.81550995,\n",
       "        -4.66326531, -0.2845859 , -3.66005788, -2.97214053,  1.01134402,\n",
       "         2.87572042,  1.54450324,  0.94991713,  2.08574918,  2.22362632,\n",
       "        -4.17370632, -4.0605423 , -3.81021779, -3.88435588, -2.68032624,\n",
       "        -0.96638358, -2.6955358 , -4.65629093, -2.06641196, -0.60069727]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get coefficient.\n",
    "clf.get_coeff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.81739936e-01, 1.76616481e-10, 3.40336632e-04, 9.89404781e-01,\n",
       "       9.94418978e-01, 9.96694044e-01, 6.72956664e-03, 2.70643618e-03,\n",
       "       9.99589118e-01, 1.62120998e-02, 9.95502282e-01, 9.71949915e-01,\n",
       "       7.44820761e-01, 2.85686599e-05, 5.11605598e-07, 7.95056599e-03,\n",
       "       1.24678732e-02, 7.98166225e-07, 9.99651468e-01, 5.50698011e-10,\n",
       "       9.63022033e-01, 9.98426830e-01, 1.06572570e-08, 8.14663624e-01,\n",
       "       9.98933453e-01, 1.73137781e-01, 3.26778515e-01, 9.99991195e-01,\n",
       "       3.98096368e-02, 7.14831513e-01, 1.17517045e-04, 2.95048101e-02,\n",
       "       4.30810933e-06, 9.99835054e-01, 9.74804842e-02, 2.51839219e-03,\n",
       "       9.97317151e-02, 9.72539417e-01, 1.27184203e-02, 2.85987048e-02,\n",
       "       9.68963510e-01, 1.25343284e-02, 9.98863311e-01, 6.40478274e-01,\n",
       "       1.13779740e-03, 5.84987403e-01, 9.97077869e-01, 1.34710237e-02,\n",
       "       1.57891711e-05, 8.72927595e-08, 9.77424996e-01, 9.41670357e-01,\n",
       "       3.85596191e-03, 9.99063057e-01, 2.64019581e-06, 9.94782397e-01,\n",
       "       9.94259380e-01, 3.97990545e-01, 9.97725425e-01, 9.92886136e-01,\n",
       "       9.87329041e-01, 3.82948001e-06, 3.58164304e-02, 4.89022249e-01,\n",
       "       9.76613822e-01, 9.56641152e-01, 8.82149823e-01, 8.69287548e-01,\n",
       "       9.96745204e-01, 9.99690980e-01, 7.49504669e-01, 9.98739967e-01,\n",
       "       9.40453539e-01, 9.99766246e-01, 9.99971739e-01, 4.07403253e-05,\n",
       "       9.97549808e-01, 9.83049669e-01, 9.62808130e-01, 9.98024883e-01,\n",
       "       8.46504822e-01, 9.91210949e-01, 7.08109821e-01, 9.93187073e-01,\n",
       "       9.89735677e-01, 9.93413595e-01, 2.85910139e-10, 9.70157623e-01,\n",
       "       8.42584643e-01, 7.74051066e-04, 3.53169749e-04, 6.98566642e-01,\n",
       "       6.70174806e-04, 3.77825038e-02, 1.56737482e-05, 4.27293731e-01,\n",
       "       3.73498782e-02, 9.99528128e-01, 9.76364512e-01, 9.99599217e-01,\n",
       "       6.18644922e-05, 5.47704154e-04, 9.31163335e-01, 2.00181188e-04,\n",
       "       9.99583944e-01, 9.97423714e-01, 3.76773470e-03, 9.97020986e-01,\n",
       "       9.92504656e-01, 8.68008699e-01, 9.90428023e-01, 9.71446249e-01,\n",
       "       3.11320474e-03, 1.60147682e-03, 9.99231970e-01, 2.37074182e-03,\n",
       "       2.01826136e-01, 4.68554254e-06, 9.98573165e-01, 5.78414591e-05,\n",
       "       9.61265053e-01, 5.58002629e-03, 1.31472601e-07, 9.97398357e-01,\n",
       "       9.78061271e-01, 1.30073980e-02, 8.30861451e-01, 9.44480231e-01,\n",
       "       9.87516890e-01, 6.36263274e-01, 2.93497776e-02, 9.91780411e-01,\n",
       "       9.97476791e-01, 1.37777599e-08, 9.99287534e-01, 9.96638655e-01,\n",
       "       9.77931764e-01, 9.97414894e-01, 7.46941768e-03, 7.46582667e-04,\n",
       "       9.88104736e-01, 9.88944086e-01, 9.99779073e-01, 9.98776468e-01,\n",
       "       9.83711328e-01, 9.84329129e-01, 9.81991650e-01, 9.89335675e-01,\n",
       "       9.95331610e-01, 9.95386473e-01, 7.59061723e-01, 9.96082653e-01,\n",
       "       9.93460139e-01, 9.97765543e-01, 7.28936945e-01, 9.89743498e-01,\n",
       "       9.53511370e-01, 9.81129137e-01, 9.72643012e-04, 9.44531089e-01,\n",
       "       9.70044863e-01, 9.87574935e-01, 8.45122826e-04, 9.99913686e-01,\n",
       "       9.99747844e-01, 6.92367153e-01, 9.99462932e-01, 1.19048332e-03,\n",
       "       9.81902760e-01, 9.95423338e-01, 2.05739270e-03, 3.79630492e-02,\n",
       "       4.35788496e-03, 9.99708578e-01, 9.99601736e-01, 9.99996790e-01,\n",
       "       7.59292917e-05, 9.72939535e-01, 2.75336310e-05, 4.73179858e-05,\n",
       "       2.04978149e-01, 9.74823365e-01, 7.42393020e-01, 9.96195238e-01,\n",
       "       4.83370636e-01, 9.57237081e-01, 5.24230824e-02, 9.96753351e-01,\n",
       "       9.99516769e-01, 1.78678975e-03, 1.27384937e-01, 9.89569099e-01,\n",
       "       9.97738930e-01, 9.87150912e-01, 9.93575657e-01, 1.79916913e-02,\n",
       "       7.32666831e-01, 9.96673538e-01, 6.02061343e-04, 9.07284908e-01,\n",
       "       9.05355403e-01, 9.94690772e-01, 8.63412122e-01, 9.93320873e-01,\n",
       "       7.99947229e-02, 9.97198659e-01, 1.03266250e-04, 9.66596559e-01,\n",
       "       6.63791420e-01, 9.65981767e-01, 1.20644857e-02, 9.70765182e-01,\n",
       "       2.62807278e-02, 9.31548418e-01, 3.81329233e-04, 7.04247934e-01,\n",
       "       2.73142620e-14, 2.42586771e-01, 6.64852434e-05, 9.99687222e-01,\n",
       "       9.74365378e-01, 9.72454210e-01, 9.97499809e-01, 9.31372750e-01,\n",
       "       9.97219698e-01, 5.12994935e-06, 9.95685992e-01, 2.99596097e-04,\n",
       "       2.09918537e-03, 9.62025660e-01, 9.99337151e-01, 1.11259725e-09,\n",
       "       9.00303884e-01, 9.83166255e-01, 9.99023525e-01, 4.54998205e-05,\n",
       "       9.99524276e-01, 9.73006414e-01, 9.70983922e-01, 1.08490501e-04,\n",
       "       8.75104366e-06, 9.99875039e-01, 9.99986267e-01, 9.97427485e-01,\n",
       "       9.63878014e-01, 9.97686761e-01, 8.97478640e-01, 6.39180029e-01,\n",
       "       9.99096108e-01, 9.99761636e-01, 9.98818902e-01, 6.82629096e-02,\n",
       "       9.84510042e-01, 9.97379723e-01, 8.37449676e-01, 9.86580980e-01,\n",
       "       1.50015806e-05, 9.91753182e-01, 9.38390596e-01, 9.26077680e-07,\n",
       "       3.79974665e-08, 5.36624888e-02, 9.94095051e-01, 9.99395939e-01,\n",
       "       2.33450352e-04, 2.92138711e-03, 2.06410541e-07, 9.67876619e-01,\n",
       "       9.99929166e-01, 7.75213470e-02, 2.17320109e-04, 4.61166806e-04,\n",
       "       7.79804089e-01, 9.70289848e-01, 7.62625151e-05, 4.10255375e-06,\n",
       "       9.92546440e-01, 2.76383868e-01, 5.93712634e-03, 9.98874385e-01,\n",
       "       9.86840734e-10, 1.99841863e-03, 9.94566663e-01, 1.14455112e-02,\n",
       "       7.34169383e-04, 9.81232495e-01, 9.92724647e-01, 9.89619269e-01,\n",
       "       2.41189065e-02, 9.67632706e-01, 2.63708788e-05, 9.99658741e-01,\n",
       "       1.65041840e-05, 3.48676319e-01, 7.27851607e-01, 3.22940272e-01,\n",
       "       8.28980918e-01, 9.98550190e-01, 8.64243571e-07, 9.99528529e-01,\n",
       "       9.52886527e-01, 9.77253960e-03, 9.99926017e-01, 6.66900481e-03,\n",
       "       9.94538727e-01, 1.58074264e-03, 7.64150890e-01, 2.90428355e-01,\n",
       "       1.17290669e-01, 1.64564714e-01, 9.39553000e-01, 2.57165514e-03,\n",
       "       9.97623904e-01, 5.50996748e-02, 7.58557696e-09, 9.99010752e-01,\n",
       "       9.99805987e-01, 8.54510771e-01, 9.87962832e-01, 9.57290072e-01,\n",
       "       8.58266735e-01, 9.83470413e-01, 9.99969276e-01, 9.04191717e-01,\n",
       "       9.90966507e-01, 9.63048753e-01, 3.99127843e-01, 9.95048598e-01,\n",
       "       9.99007949e-01, 5.05299691e-01, 1.43151076e-05, 9.87255180e-01,\n",
       "       9.99267696e-01, 9.78467546e-01, 9.33413890e-01, 9.99331315e-01,\n",
       "       5.62781493e-01, 8.54107789e-01, 9.86225399e-01, 1.21327392e-02,\n",
       "       9.93119586e-01, 1.68518623e-06, 5.52956441e-04, 9.81143196e-01,\n",
       "       8.66766304e-01, 9.95355227e-01, 1.68956367e-03, 9.98932663e-01,\n",
       "       9.66881921e-01, 9.99679455e-01, 3.66294217e-05, 9.80092817e-01,\n",
       "       9.02794314e-01, 9.98767753e-01, 1.11845314e-01, 9.61724704e-01,\n",
       "       9.99985526e-01, 9.98723282e-01, 9.99836253e-01, 9.99574733e-01,\n",
       "       9.98719682e-01, 5.52057812e-03, 9.86023481e-01, 2.87864497e-01,\n",
       "       2.16545953e-03, 1.10908795e-03, 7.20685781e-06, 8.72542321e-01,\n",
       "       9.64008925e-01, 3.43118264e-01, 9.84076921e-01, 9.71123560e-01,\n",
       "       2.80115971e-05, 1.32337859e-03, 9.91029613e-01, 9.94864025e-01,\n",
       "       9.37066169e-01, 1.37902873e-05, 7.80885768e-01, 4.89894804e-08,\n",
       "       9.32970769e-01, 9.35884035e-01, 9.64248885e-06, 6.29797081e-02,\n",
       "       9.86127976e-01, 1.07096101e-02, 7.54076260e-01, 9.91607152e-01,\n",
       "       9.99825133e-01, 9.63952106e-01, 8.89114190e-01, 9.94976862e-01,\n",
       "       1.26325814e-02, 9.77815971e-01, 8.54474860e-01, 9.89653363e-01,\n",
       "       1.50355073e-06, 1.06103051e-04, 9.97989999e-01, 2.48881681e-07,\n",
       "       6.64334926e-02, 9.99554156e-01, 9.90492797e-01, 9.99959321e-01,\n",
       "       5.38957839e-05, 2.80182457e-02, 2.28373416e-04, 9.97825922e-01,\n",
       "       9.99361036e-01, 2.72441239e-04, 9.99783757e-01, 5.74780829e-08,\n",
       "       9.99498858e-01, 9.61688962e-01, 9.98095101e-01, 9.94486758e-01,\n",
       "       9.25063977e-01, 3.31545682e-02, 1.57729137e-02, 9.87430567e-01,\n",
       "       3.24887738e-01, 9.99383896e-01, 9.88462369e-01, 9.99719493e-01,\n",
       "       8.75978662e-01, 1.22933974e-05])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted probabilities for training data.\n",
    "p_pred_train = clf.predict(X_train)\n",
    "p_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted labels for training data.\n",
    "y_pred_train = (p_pred_train > 0.5) * 1\n",
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted label correctness for training data.\n",
    "y_pred_train == y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted label correctness for test data.\n",
    "p_pred_test = clf.predict(X_test)\n",
    "y_pred_test = (p_pred_test > 0.5) * 1\n",
    "y_pred_test == y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9790209790209791"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction accuracy for test data.\n",
    "accuracy(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fitting Sklearn's Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=500,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit sklearn's Logistic Regression.\n",
    "clf2 = LogisticRegressionSklearn(C=1e4, solver='lbfgs', max_iter=500)\n",
    "\n",
    "clf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([56.05003313]),\n",
       " array([[  53.58643008,  -27.26324946,   48.30813261,   10.51429012,\n",
       "          -14.72140714,   98.48590155,  -52.54093157,  -52.1529564 ,\n",
       "           -5.09714283,  -53.94912949,  -33.90682397,   -5.47905937,\n",
       "          -19.38082761,  -44.01004736,   38.82361457,  -51.40837833,\n",
       "           83.34837589,  -21.99821834,   14.90317319,   80.03225554,\n",
       "          -59.03462026,   -3.91874979,  -63.60095116, -103.93267556,\n",
       "           -8.01066718,   20.06509642,  -22.04450625,  -21.2526363 ,\n",
       "          -21.51142977,  -11.72401291]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get coefficients.\n",
    "clf2.intercept_, clf2.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted labels for training data.\n",
    "y_pred_train = clf2.predict(X_train)\n",
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted label correctness for training data.\n",
    "y_pred_train == y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True, False,  True,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted label correctness for test data.\n",
    "y_pred_test = clf2.predict(X_test) \n",
    "y_pred_test == y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.965034965034965"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Prediction accuracy for test data.\n",
    "accuracy(y_test, y_pred_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
