{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic Regression with Implementation (WIP)\n",
    "\n",
    "- Bowen Li\n",
    "- 2018/08/20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Logistic regression is one of the most fundamental machine learning models for binary classification. I will summarize its methodology and implement it from scratch using NumPy.\n",
    "\n",
    "### Binary classification\n",
    "\n",
    "For example, the doctor would like to base on patients's features, including mean radius, mean texture, etc, to classify  breat cancer into one of the following two case: \n",
    "- \"malignant\": $y = 1$\n",
    "- \"benign\": $y = 0$\n",
    "\n",
    "which correspond to serious and gentle case respectively. \n",
    "\n",
    "We would like to load the breast cancer data from scikit-learn as a toy dataset, and split the data into the training and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Summarize methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "bc_data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 71\n",
    "TRAIN_PERCENT = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_names: \n",
      "['mean_radius', 'mean_texture', 'mean_perimeter', 'mean_area', 'mean_smoothness', 'mean_compactness', 'mean_concavity', 'mean_concave_points', 'mean_symmetry', 'mean_fractal_dimension', 'radius_error', 'texture_error', 'perimeter_error', 'area_error', 'smoothness_error', 'compactness_error', 'concavity_error', 'concave_points_error', 'symmetry_error', 'fractal_dimension_error', 'worst_radius', 'worst_texture', 'worst_perimeter', 'worst_area', 'worst_smoothness', 'worst_compactness', 'worst_concavity', 'worst_concave_points', 'worst_symmetry', 'worst_fractal_dimension']\n",
      "X: \n",
      "[[  1.79900000e+01   1.03800000e+01   1.22800000e+02 ...,   2.65400000e-01\n",
      "    4.60100000e-01   1.18900000e-01]\n",
      " [  2.05700000e+01   1.77700000e+01   1.32900000e+02 ...,   1.86000000e-01\n",
      "    2.75000000e-01   8.90200000e-02]\n",
      " [  1.96900000e+01   2.12500000e+01   1.30000000e+02 ...,   2.43000000e-01\n",
      "    3.61300000e-01   8.75800000e-02]\n",
      " ..., \n",
      " [  1.66000000e+01   2.80800000e+01   1.08300000e+02 ...,   1.41800000e-01\n",
      "    2.21800000e-01   7.82000000e-02]\n",
      " [  2.06000000e+01   2.93300000e+01   1.40100000e+02 ...,   2.65000000e-01\n",
      "    4.08700000e-01   1.24000000e-01]\n",
      " [  7.76000000e+00   2.45400000e+01   4.79200000e+01 ...,   0.00000000e+00\n",
      "    2.87100000e-01   7.03900000e-02]]\n",
      "X.shape: (569, 30)\n"
     ]
    }
   ],
   "source": [
    "features = bc_data.get('feature_names')\n",
    "features = ['_'.join(x.split()) for x in features]\n",
    "X = bc_data.get('data')\n",
    "# X = X.reshape((X.shape[1], X.shape[0]))\n",
    "\n",
    "print('feature_names: \\n{}'.format(features))\n",
    "print('X: \\n{}'.format(X))\n",
    "\n",
    "print('X.shape: {}'.format(X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_names: ['malignant' 'benign']\n",
      "target: \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n",
      "y: (569,)\n"
     ]
    }
   ],
   "source": [
    "target = bc_data.get('target_names')\n",
    "y = bc_data.get('target')\n",
    "\n",
    "print('target_names: {}'.format(target))\n",
    "print('target: \\n{}'.format(y))\n",
    "\n",
    "print('y: {}'.format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform basic EDA for the breast cancer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of malignant: 212\n",
      "Number of benign: 357\n"
     ]
    }
   ],
   "source": [
    "# EDA for numbers of malignant and benign.\n",
    "print('Number of malignant: {}'.format((y == 0).sum()))\n",
    "print('Number of benign: {}'.format((y == 1).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_radius</th>\n",
       "      <th>mean_texture</th>\n",
       "      <th>mean_perimeter</th>\n",
       "      <th>mean_area</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>mean_compactness</th>\n",
       "      <th>mean_concavity</th>\n",
       "      <th>mean_concave_points</th>\n",
       "      <th>mean_symmetry</th>\n",
       "      <th>mean_fractal_dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst_radius</th>\n",
       "      <th>worst_texture</th>\n",
       "      <th>worst_perimeter</th>\n",
       "      <th>worst_area</th>\n",
       "      <th>worst_smoothness</th>\n",
       "      <th>worst_compactness</th>\n",
       "      <th>worst_concavity</th>\n",
       "      <th>worst_concave_points</th>\n",
       "      <th>worst_symmetry</th>\n",
       "      <th>worst_fractal_dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>...</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>...</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>...</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>...</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>...</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>...</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>...</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean_radius  mean_texture  mean_perimeter    mean_area  \\\n",
       "count   569.000000    569.000000      569.000000   569.000000   \n",
       "mean     14.127292     19.289649       91.969033   654.889104   \n",
       "std       3.524049      4.301036       24.298981   351.914129   \n",
       "min       6.981000      9.710000       43.790000   143.500000   \n",
       "25%      11.700000     16.170000       75.170000   420.300000   \n",
       "50%      13.370000     18.840000       86.240000   551.100000   \n",
       "75%      15.780000     21.800000      104.100000   782.700000   \n",
       "max      28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       mean_smoothness  mean_compactness  mean_concavity  mean_concave_points  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       mean_symmetry  mean_fractal_dimension           ...             \\\n",
       "count     569.000000              569.000000           ...              \n",
       "mean        0.181162                0.062798           ...              \n",
       "std         0.027414                0.007060           ...              \n",
       "min         0.106000                0.049960           ...              \n",
       "25%         0.161900                0.057700           ...              \n",
       "50%         0.179200                0.061540           ...              \n",
       "75%         0.195700                0.066120           ...              \n",
       "max         0.304000                0.097440           ...              \n",
       "\n",
       "       worst_radius  worst_texture  worst_perimeter   worst_area  \\\n",
       "count    569.000000     569.000000       569.000000   569.000000   \n",
       "mean      16.269190      25.677223       107.261213   880.583128   \n",
       "std        4.833242       6.146258        33.602542   569.356993   \n",
       "min        7.930000      12.020000        50.410000   185.200000   \n",
       "25%       13.010000      21.080000        84.110000   515.300000   \n",
       "50%       14.970000      25.410000        97.660000   686.500000   \n",
       "75%       18.790000      29.720000       125.400000  1084.000000   \n",
       "max       36.040000      49.540000       251.200000  4254.000000   \n",
       "\n",
       "       worst_smoothness  worst_compactness  worst_concavity  \\\n",
       "count        569.000000         569.000000       569.000000   \n",
       "mean           0.132369           0.254265         0.272188   \n",
       "std            0.022832           0.157336         0.208624   \n",
       "min            0.071170           0.027290         0.000000   \n",
       "25%            0.116600           0.147200         0.114500   \n",
       "50%            0.131300           0.211900         0.226700   \n",
       "75%            0.146000           0.339100         0.382900   \n",
       "max            0.222600           1.058000         1.252000   \n",
       "\n",
       "       worst_concave_points  worst_symmetry  worst_fractal_dimension  \n",
       "count            569.000000      569.000000               569.000000  \n",
       "mean               0.114606        0.290076                 0.083946  \n",
       "std                0.065732        0.061867                 0.018061  \n",
       "min                0.000000        0.156500                 0.055040  \n",
       "25%                0.064930        0.250400                 0.071460  \n",
       "50%                0.099930        0.282200                 0.080040  \n",
       "75%                0.161400        0.317900                 0.092080  \n",
       "max                0.291000        0.663800                 0.207500  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDA for feature matrix.\n",
    "pd.DataFrame(X, columns=features).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_feature(x, axis=0):\n",
    "    \"\"\"Implement a function that normalizes each col or row of the matrix x \n",
    "    to have unit length.\n",
    "    \n",
    "    Args:\n",
    "      x: A numpy matrix of shape (n, m).\n",
    "      axis: A integer in {0, 1}, \n",
    "        - 0: normalize for each feature col.\n",
    "        - 1: normalize for each feature row. \n",
    "    \n",
    "    Returns:\n",
    "      x_normalized: The normalized (by row) numpy matrix.\n",
    "    \"\"\"\n",
    "    # Compute x_norm as the norm 2 of x.\n",
    "    x_norm = np.linalg.norm(x, axis=axis, ord=2, keepdims=True)\n",
    "    # Divide x by its norm.\n",
    "    x_normalized = x / x_norm\n",
    "    return x_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized X: [[ 0.05180005  0.02201907  0.0541219  ...,  0.08423164  0.06503422\n",
      "   0.05805201]\n",
      " [ 0.05922885  0.03769547  0.05857329 ...,  0.05903197  0.0388707\n",
      "   0.04346333]\n",
      " [ 0.056695    0.04507758  0.05729517 ...,  0.07712242  0.05106903\n",
      "   0.04276026]\n",
      " ..., \n",
      " [ 0.04779771  0.05956605  0.04773128 ...,  0.04500395  0.03135099\n",
      "   0.03818055]\n",
      " [ 0.05931523  0.06221767  0.06174656 ...,  0.08410469  0.05776893\n",
      "   0.06054205]\n",
      " [ 0.02234399  0.05205665  0.02111988 ...,  0.          0.04058101\n",
      "   0.03436738]]\n",
      "Normalized X.shape: (569, 30)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_radius</th>\n",
       "      <th>mean_texture</th>\n",
       "      <th>mean_perimeter</th>\n",
       "      <th>mean_area</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>mean_compactness</th>\n",
       "      <th>mean_concavity</th>\n",
       "      <th>mean_concave_points</th>\n",
       "      <th>mean_symmetry</th>\n",
       "      <th>mean_fractal_dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst_radius</th>\n",
       "      <th>worst_texture</th>\n",
       "      <th>worst_perimeter</th>\n",
       "      <th>worst_area</th>\n",
       "      <th>worst_smoothness</th>\n",
       "      <th>worst_compactness</th>\n",
       "      <th>worst_concavity</th>\n",
       "      <th>worst_concave_points</th>\n",
       "      <th>worst_symmetry</th>\n",
       "      <th>worst_fractal_dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.040678</td>\n",
       "      <td>0.040919</td>\n",
       "      <td>0.040534</td>\n",
       "      <td>0.036935</td>\n",
       "      <td>0.041483</td>\n",
       "      <td>0.037410</td>\n",
       "      <td>0.031208</td>\n",
       "      <td>0.032855</td>\n",
       "      <td>0.041451</td>\n",
       "      <td>0.041660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040189</td>\n",
       "      <td>0.040772</td>\n",
       "      <td>0.040008</td>\n",
       "      <td>0.035214</td>\n",
       "      <td>0.041313</td>\n",
       "      <td>0.035658</td>\n",
       "      <td>0.033284</td>\n",
       "      <td>0.036373</td>\n",
       "      <td>0.041002</td>\n",
       "      <td>0.040986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.010147</td>\n",
       "      <td>0.009124</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.019848</td>\n",
       "      <td>0.006055</td>\n",
       "      <td>0.018936</td>\n",
       "      <td>0.028017</td>\n",
       "      <td>0.026061</td>\n",
       "      <td>0.006273</td>\n",
       "      <td>0.004684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011939</td>\n",
       "      <td>0.009760</td>\n",
       "      <td>0.012534</td>\n",
       "      <td>0.022768</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>0.022065</td>\n",
       "      <td>0.025511</td>\n",
       "      <td>0.020862</td>\n",
       "      <td>0.008745</td>\n",
       "      <td>0.008818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.020101</td>\n",
       "      <td>0.020598</td>\n",
       "      <td>0.019300</td>\n",
       "      <td>0.008093</td>\n",
       "      <td>0.022657</td>\n",
       "      <td>0.006949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024254</td>\n",
       "      <td>0.033144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019589</td>\n",
       "      <td>0.019086</td>\n",
       "      <td>0.018803</td>\n",
       "      <td>0.007406</td>\n",
       "      <td>0.022213</td>\n",
       "      <td>0.003827</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022121</td>\n",
       "      <td>0.026873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>0.034301</td>\n",
       "      <td>0.033130</td>\n",
       "      <td>0.023705</td>\n",
       "      <td>0.037183</td>\n",
       "      <td>0.023276</td>\n",
       "      <td>0.010389</td>\n",
       "      <td>0.013641</td>\n",
       "      <td>0.037044</td>\n",
       "      <td>0.038278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032138</td>\n",
       "      <td>0.033473</td>\n",
       "      <td>0.031373</td>\n",
       "      <td>0.020606</td>\n",
       "      <td>0.036392</td>\n",
       "      <td>0.020643</td>\n",
       "      <td>0.014001</td>\n",
       "      <td>0.020607</td>\n",
       "      <td>0.035394</td>\n",
       "      <td>0.034890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.038497</td>\n",
       "      <td>0.039965</td>\n",
       "      <td>0.038009</td>\n",
       "      <td>0.031082</td>\n",
       "      <td>0.041272</td>\n",
       "      <td>0.033212</td>\n",
       "      <td>0.021628</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>0.041002</td>\n",
       "      <td>0.040826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036980</td>\n",
       "      <td>0.040348</td>\n",
       "      <td>0.036427</td>\n",
       "      <td>0.027452</td>\n",
       "      <td>0.040980</td>\n",
       "      <td>0.029717</td>\n",
       "      <td>0.027721</td>\n",
       "      <td>0.031715</td>\n",
       "      <td>0.039888</td>\n",
       "      <td>0.039079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.045437</td>\n",
       "      <td>0.046244</td>\n",
       "      <td>0.045880</td>\n",
       "      <td>0.044144</td>\n",
       "      <td>0.045332</td>\n",
       "      <td>0.046754</td>\n",
       "      <td>0.045933</td>\n",
       "      <td>0.049700</td>\n",
       "      <td>0.044778</td>\n",
       "      <td>0.043864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046416</td>\n",
       "      <td>0.047192</td>\n",
       "      <td>0.046774</td>\n",
       "      <td>0.043348</td>\n",
       "      <td>0.045568</td>\n",
       "      <td>0.047555</td>\n",
       "      <td>0.046822</td>\n",
       "      <td>0.051225</td>\n",
       "      <td>0.044935</td>\n",
       "      <td>0.044957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.080939</td>\n",
       "      <td>0.083325</td>\n",
       "      <td>0.083078</td>\n",
       "      <td>0.141055</td>\n",
       "      <td>0.070344</td>\n",
       "      <td>0.123840</td>\n",
       "      <td>0.149994</td>\n",
       "      <td>0.135132</td>\n",
       "      <td>0.069557</td>\n",
       "      <td>0.064642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089028</td>\n",
       "      <td>0.078664</td>\n",
       "      <td>0.093697</td>\n",
       "      <td>0.170113</td>\n",
       "      <td>0.069475</td>\n",
       "      <td>0.148372</td>\n",
       "      <td>0.153097</td>\n",
       "      <td>0.092356</td>\n",
       "      <td>0.093827</td>\n",
       "      <td>0.101310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean_radius  mean_texture  mean_perimeter   mean_area  mean_smoothness  \\\n",
       "count   569.000000    569.000000      569.000000  569.000000       569.000000   \n",
       "mean      0.040678      0.040919        0.040534    0.036935         0.041483   \n",
       "std       0.010147      0.009124        0.010709    0.019848         0.006055   \n",
       "min       0.020101      0.020598        0.019300    0.008093         0.022657   \n",
       "25%       0.033689      0.034301        0.033130    0.023705         0.037183   \n",
       "50%       0.038497      0.039965        0.038009    0.031082         0.041272   \n",
       "75%       0.045437      0.046244        0.045880    0.044144         0.045332   \n",
       "max       0.080939      0.083325        0.083078    0.141055         0.070344   \n",
       "\n",
       "       mean_compactness  mean_concavity  mean_concave_points  mean_symmetry  \\\n",
       "count        569.000000      569.000000           569.000000     569.000000   \n",
       "mean           0.037410        0.031208             0.032855       0.041451   \n",
       "std            0.018936        0.028017             0.026061       0.006273   \n",
       "min            0.006949        0.000000             0.000000       0.024254   \n",
       "25%            0.023276        0.010389             0.013641       0.037044   \n",
       "50%            0.033212        0.021628             0.022500       0.041002   \n",
       "75%            0.046754        0.045933             0.049700       0.044778   \n",
       "max            0.123840        0.149994             0.135132       0.069557   \n",
       "\n",
       "       mean_fractal_dimension           ...             worst_radius  \\\n",
       "count              569.000000           ...               569.000000   \n",
       "mean                 0.041660           ...                 0.040189   \n",
       "std                  0.004684           ...                 0.011939   \n",
       "min                  0.033144           ...                 0.019589   \n",
       "25%                  0.038278           ...                 0.032138   \n",
       "50%                  0.040826           ...                 0.036980   \n",
       "75%                  0.043864           ...                 0.046416   \n",
       "max                  0.064642           ...                 0.089028   \n",
       "\n",
       "       worst_texture  worst_perimeter  worst_area  worst_smoothness  \\\n",
       "count     569.000000       569.000000  569.000000        569.000000   \n",
       "mean        0.040772         0.040008    0.035214          0.041313   \n",
       "std         0.009760         0.012534    0.022768          0.007126   \n",
       "min         0.019086         0.018803    0.007406          0.022213   \n",
       "25%         0.033473         0.031373    0.020606          0.036392   \n",
       "50%         0.040348         0.036427    0.027452          0.040980   \n",
       "75%         0.047192         0.046774    0.043348          0.045568   \n",
       "max         0.078664         0.093697    0.170113          0.069475   \n",
       "\n",
       "       worst_compactness  worst_concavity  worst_concave_points  \\\n",
       "count         569.000000       569.000000            569.000000   \n",
       "mean            0.035658         0.033284              0.036373   \n",
       "std             0.022065         0.025511              0.020862   \n",
       "min             0.003827         0.000000              0.000000   \n",
       "25%             0.020643         0.014001              0.020607   \n",
       "50%             0.029717         0.027721              0.031715   \n",
       "75%             0.047555         0.046822              0.051225   \n",
       "max             0.148372         0.153097              0.092356   \n",
       "\n",
       "       worst_symmetry  worst_fractal_dimension  \n",
       "count      569.000000               569.000000  \n",
       "mean         0.041002                 0.040986  \n",
       "std          0.008745                 0.008818  \n",
       "min          0.022121                 0.026873  \n",
       "25%          0.035394                 0.034890  \n",
       "50%          0.039888                 0.039079  \n",
       "75%          0.044935                 0.044957  \n",
       "max          0.093827                 0.101310  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = normalize_feature(X)\n",
    "\n",
    "print('Normalized X: {}'.format(X))\n",
    "print('Normalized X.shape: {}'.format(X.shape))\n",
    "\n",
    "# EDA for normalized feature matrix.\n",
    "pd.DataFrame(X, columns=features).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (392, 30)\n",
      "y_train.shape: (392,)\n",
      "X_test.shape: (177, 30)\n",
      "y_test.shape: (177,)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "train_flag = np.random.rand(X.shape[0]) < TRAIN_PERCENT\n",
    "\n",
    "X_train = X[train_flag]\n",
    "y_train = y[train_flag]\n",
    "X_test = X[~train_flag]\n",
    "y_test = y[~train_flag]\n",
    "\n",
    "print('X_train.shape: {}'.format(X_train.shape))\n",
    "print('y_train.shape: {}'.format(y_train.shape))\n",
    "print('X_test.shape: {}'.format(X_test.shape))\n",
    "print('y_test.shape: {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Logistic regression using function framework.\"\"\"\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Compute the sigmoid of x.\n",
    "\n",
    "    Args:\n",
    "      x: A scalar or numpy array of any size.\n",
    "\n",
    "    Returns:\n",
    "      s: sigmoid(x).\n",
    "    \"\"\"\n",
    "    s = 1 / (1 + np.exp(-x))    \n",
    "    return s\n",
    "\n",
    "\n",
    "def initialize_weights(dim):\n",
    "    \"\"\"Initialize weights.\n",
    "\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and b to 0.\n",
    "    \n",
    "    Args:\n",
    "      dim: A integer. Size of the w vector (or number of parameters.)\n",
    "    \n",
    "    Returns:\n",
    "      w: A Numpy array. Initialized vector of shape (dim, 1)\n",
    "      b: A integer. Initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    w = np.zeros(dim).reshape(dim, 1)\n",
    "    b = 0\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def activation(w, b, X):\n",
    "    \"\"\"Activation function using sigmoid function.\"\"\"\n",
    "    A = sigmoid(np.dot(X, w) + b)\n",
    "    return A\n",
    "\n",
    "def cross_entropy(y, A, m):\n",
    "    \"\"\"Cross entropy.\"\"\"\n",
    "    cross_entropy = - 1 / m * np.sum(y * np.log(A) + (1 - y) * np.log(1 - A))\n",
    "    return cross_entropy\n",
    "\n",
    "def gradient(X, y, A, m):\n",
    "    \"\"\"Gradient for weight vector and bias.\"\"\"\n",
    "    dw = 1 / m * np.dot(X.T, (A - y))\n",
    "    db = 1 / m * np.sum(A - y)\n",
    "    return dw, db\n",
    "\n",
    "def propagate(w, b, X, y):\n",
    "    \"\"\"Forward & backward propagation.\n",
    "\n",
    "    Implement the cost function and its gradient for the propagation.\n",
    "\n",
    "    Args:\n",
    "      w: A Numpy array. Weights of size (num_px * num_px * 3, 1)\n",
    "      b: A float. Bias.\n",
    "      X: A Numpy array. Data of size (number of examples, num_px * num_px * 3).\n",
    "      y: A Numpy array. True \"label\" vector (containing 0 or 1) \n",
    "         of size (number of examplesm, 1).\n",
    "\n",
    "    Returns:\n",
    "      cost: A float. Negative log-likelihood cost for logistic regression.\n",
    "      dw: A Numpy array. Gradient of the loss w.r.t. w, thus same shape as w.\n",
    "      db: A float. Gradient of the loss w.r.t b, thus same shape as b.\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    y = y.reshape((m, 1))\n",
    "\n",
    "    # Forward propagation from X to cost.\n",
    "    # Compute activation.\n",
    "    A = activation(w, b, X)\n",
    "    # Compute cost.\n",
    "    cost = cross_entropy(y, A, m)\n",
    "    \n",
    "    # Backward propagation to find gradient.\n",
    "    dw, db = gradient(X, y, A, m)\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db} \n",
    "\n",
    "    return grads, cost\n",
    "\n",
    "\n",
    "def gradient_descent(w, b, X, y, num_iterations, learning_rate, print_cost=True):\n",
    "    \"\"\"Optimize using gradient descent.\n",
    "\n",
    "    This function optimizes w and b by running a gradient descent algorithm.\n",
    "    That is, write down two steps and iterate through them:\n",
    "      1. Calculate the cost and the gradient for the current parameters. \n",
    "        Use propagate().\n",
    "      2. Update the parameters using gradient descent rule for w and b.\n",
    "    \n",
    "    Args:\n",
    "      w: A Numpy array. Weights of size (num_px * num_px * 3, 1).\n",
    "      b: A scalar. Bias.\n",
    "      X: A Numpy array. Data of shape (number of examples, num_px * num_px * 3).\n",
    "      y: A Numpy array. True \"label\" vector (containing 0 if non-cat, 1 if cat), \n",
    "        of shape (number of examples, 1)\n",
    "      num_iterations: A integer. Number of iterations of the optimization loop.\n",
    "      learning_rate: A scalr. Learning rate of the gradient descent update rule.\n",
    "      print_cost: A Boolean. Print the loss every 100 steps. Default: True.\n",
    "    \n",
    "    Returns:\n",
    "      params: A dictionary containing the weights w and bias b.\n",
    "      grads: A dictionary containing the gradients of the weights and bias \n",
    "        with respect to the cost function\n",
    "      costs: A list of all the costs computed during the optimization, \n",
    "        this will be used to plot the learning curve.\n",
    "    \"\"\"   \n",
    "    costs = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        grads, cost = propagate(w, b, X, y)\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads.get('dw')\n",
    "        db = grads.get('db')\n",
    "        \n",
    "        # Update rule.\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "        \n",
    "        # Record the costs every 200 training examples and print.\n",
    "        if i % 200 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost and i % 200 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs\n",
    "\n",
    "\n",
    "def predict(w, b, X):\n",
    "    \"\"\"Prediction.\n",
    "\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression \n",
    "    parameters (w, b)\n",
    "    \n",
    "    Args:\n",
    "      w: A Numpy array. Learned weights of size (num_px * num_px * 3, 1).\n",
    "      b: A scalar. Learned bias.\n",
    "      X: A Numpy array. New data of size (num_px * num_px * 3, number of examples).\n",
    "    \n",
    "    Returns:\n",
    "      y_pred: A Numpy array containing all predictions (0/1) \n",
    "        for the examples in X.\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    y_pred = np.zeros((m, 1))\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a label 1.\n",
    "    A = activation(w, b, X)\n",
    "    \n",
    "    for i in range(A.shape[0]):\n",
    "        # Convert probabilities a[i] to actual predictions y_pred[i].\n",
    "        if A[i] > 0.5:\n",
    "            y_pred[i] = 1\n",
    "        else:\n",
    "            y_pred[i] = 0\n",
    "    \n",
    "    assert(y_pred.shape == (m, 1))\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def accuracy(y_pred, y):\n",
    "    acc = 1 - np.mean(np.abs(y_pred - y))\n",
    "    return acc\n",
    "\n",
    "\n",
    "def logistic_regression(X_train, y_train, X_test, y_test, \n",
    "                        num_iterations=2000, learning_rate=0.5, print_cost=True):\n",
    "    '''Wrap-up function for logistic regression.\n",
    "\n",
    "    Builds the logistic regression model by calling the function \n",
    "    you've implemented previously.\n",
    "    \n",
    "    Args:\n",
    "      X_train: A Numpy. Training set of shape (m_train, num_px * num_px * 3).\n",
    "      y_train: A Numpy array. Training labels of shape (m_train, 1).\n",
    "      X_test: A Numpy array. Test set of shape (m_test, num_px * num_px * 3).\n",
    "      y_test: A Numpy array. Test labels of shape (m_test, 1).\n",
    "      num_iterations: An integer. Hyperparameter for the number of iterations \n",
    "        to optimize the parameters. Default: 2000.\n",
    "      learning_rate: A scalar. Hyperparameter for the learning rate used \n",
    "        in the update rule of optimize(). Default: 0.005.\n",
    "      print_cost: A Boolean. Print the cost every 100 iterations. Default: True.\n",
    "    \n",
    "    Returns:\n",
    "      d: A dictionary containing information about the model.\n",
    "    '''    \n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    w, b = initialize_weights(X_train.shape[1])\n",
    "\n",
    "    # Gradient descent.\n",
    "    parameters, grads, costs = gradient_descent(\n",
    "        w, b, X_train, y_train, \n",
    "        num_iterations=num_iterations, learning_rate=learning_rate, \n",
    "        print_cost=print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary 'parameters'\n",
    "    w = parameters.get('w')\n",
    "    b = parameters.get('b')\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    y_pred_train = predict(w, b, X_train)\n",
    "    y_pred_test = predict(w, b, X_test)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print('Train accuracy: {} %'\n",
    "          .format(accuracy(y_pred_train.ravel(), y_train) * 100))\n",
    "    print('Test accuracy: {} %'\n",
    "          .format(accuracy(y_pred_test.ravel(), y_test) * 100))\n",
    "    \n",
    "    d = {'costs': costs,\n",
    "         'y_pred_train': y_pred_train, \n",
    "         'y_pred_test': y_pred_test, \n",
    "         'w': w, \n",
    "         'b': b,\n",
    "         'learning_rate' : learning_rate,\n",
    "         'num_iterations': num_iterations}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 200: 0.564298\n",
      "Cost after iteration 400: 0.499417\n",
      "Cost after iteration 600: 0.452810\n",
      "Cost after iteration 800: 0.417814\n",
      "Cost after iteration 1000: 0.390538\n",
      "Cost after iteration 1200: 0.368625\n",
      "Cost after iteration 1400: 0.350578\n",
      "Cost after iteration 1600: 0.335411\n",
      "Cost after iteration 1800: 0.322446\n",
      "Cost after iteration 2000: 0.311204\n",
      "Cost after iteration 2200: 0.301337\n",
      "Cost after iteration 2400: 0.292586\n",
      "Cost after iteration 2600: 0.284755\n",
      "Cost after iteration 2800: 0.277690\n",
      "Cost after iteration 3000: 0.271273\n",
      "Cost after iteration 3200: 0.265409\n",
      "Cost after iteration 3400: 0.260020\n",
      "Cost after iteration 3600: 0.255044\n",
      "Cost after iteration 3800: 0.250430\n",
      "Cost after iteration 4000: 0.246133\n",
      "Cost after iteration 4200: 0.242119\n",
      "Cost after iteration 4400: 0.238355\n",
      "Cost after iteration 4600: 0.234817\n",
      "Cost after iteration 4800: 0.231482\n",
      "Cost after iteration 5000: 0.228329\n",
      "Cost after iteration 5200: 0.225344\n",
      "Cost after iteration 5400: 0.222510\n",
      "Cost after iteration 5600: 0.219815\n",
      "Cost after iteration 5800: 0.217248\n",
      "Cost after iteration 6000: 0.214798\n",
      "Cost after iteration 6200: 0.212457\n",
      "Cost after iteration 6400: 0.210216\n",
      "Cost after iteration 6600: 0.208068\n",
      "Cost after iteration 6800: 0.206008\n",
      "Cost after iteration 7000: 0.204028\n",
      "Cost after iteration 7200: 0.202124\n",
      "Cost after iteration 7400: 0.200290\n",
      "Cost after iteration 7600: 0.198523\n",
      "Cost after iteration 7800: 0.196819\n",
      "Cost after iteration 8000: 0.195174\n",
      "Cost after iteration 8200: 0.193584\n",
      "Cost after iteration 8400: 0.192046\n",
      "Cost after iteration 8600: 0.190558\n",
      "Cost after iteration 8800: 0.189117\n",
      "Cost after iteration 9000: 0.187721\n",
      "Cost after iteration 9200: 0.186366\n",
      "Cost after iteration 9400: 0.185052\n",
      "Cost after iteration 9600: 0.183776\n",
      "Cost after iteration 9800: 0.182537\n",
      "Train accuracy: 94.387755102 %\n",
      "Test accuracy: 93.7853107345 %\n"
     ]
    }
   ],
   "source": [
    "d = logistic_regression(X_train, y_train, X_test, y_test, \n",
    "                        num_iterations=10000, learning_rate=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Benchmark with sklearn's LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Train accuracy: 100.0 %\n",
      "Test accuracy: 93.7853107345 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Default parameter setting.\n",
    "print(LogisticRegression())\n",
    "\n",
    "logist_reg = LogisticRegression(C=1e10, max_iter=100)\n",
    "logist_reg.fit(X_train, y_train)\n",
    "y_pred_train_skl = logist_reg.predict(X_train)\n",
    "y_pred_test_skl = logist_reg.predict(X_test)\n",
    "\n",
    "print('Train accuracy: {} %'\n",
    "      .format(accuracy(y_pred_train_skl, y_train) * 100))\n",
    "print('Test accuracy: {} %'\n",
    "      .format(accuracy(y_pred_test_skl, y_test) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn result generaly quite similar with our implementation from scratch: both produce test error which are about 93 %. Nevertheless, the former is somewhat different from ours, due to the facts that it uses regularizations to reduce model complexity (see later) in default and does not apply stochastic gradient descent to solve unknown weights and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Add references."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
