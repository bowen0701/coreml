{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Modeling\n",
    "\n",
    "- Bowen Li\n",
    "- 2018/01/17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Bayesian modeling is one of the most important machine learning techniques. This notebook is to summarize its methodology and implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Probability Models\n",
    "\n",
    "- **Likelihood:** $p(x | \\theta)$\n",
    "- **Prior distribution:** $p(\\theta)$\n",
    "- **Marginal likelihood:** \n",
    "  $p(x) = \\int p(x | \\theta) p(\\theta) d(\\theta)$\n",
    "- **Posterior distribution:** \n",
    "  $p(\\theta | x) = p(x | \\theta) p(\\theta) / p(x)$\n",
    "- **Predictive distribution:** \n",
    "  $p(x_{new} | x) = \\int p(x_{new} | \\theta) p(\\theta | x) d(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De Finetti's Thereom\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point Estimation\n",
    "\n",
    "### MLE vs. MAP\n",
    "\n",
    "**Maximum Likelihood Estimation (MLE)** is to estimate $\\theta$ as the **mode of the likelihood function** $p(x | \\theta)$:\n",
    "\n",
    "$$\n",
    "\\theta_{MLE} = argmax_{\\theta} p(x | \\theta)\n",
    "$$\n",
    "\n",
    "**Maximum a Posteriori (MAP) Estimation** is to estimate $\\theta$ as the **mode of the posterior distributin** $p(\\theta | x)$:\n",
    "\n",
    "$$\n",
    "\\theta_{MAP} = argmax_{\\theta} p(\\theta | x) = argmax_{\\theta} p(x | \\theta) p(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Estimator\n",
    "\n",
    "TODO\n",
    "\n",
    "Nevertheless, the above MLE, MAP and Bayes estimator just provide point estimators for parameter, $\\theta$, of interest. In the usual Bayesian modeling, we would like to obtain the distribution of $\\theta$ given the observed data $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate Prior\n",
    "\n",
    "\n",
    "Posterior and prior distributions follow the same distribution family.\n",
    "\n",
    "\n",
    "### Binomial-Beta Conjugacy\n",
    "\n",
    "- Likelihood: $p(x | \\theta) \\sim Binomial(\\theta)$\n",
    "- Prior: $p(\\theta) \\sim Beta(\\alpha, \\beta)$\n",
    "- Posterior: $ p(\\theta | x) \\sim Beta(\\alpha^*, \\beta^*)$\n",
    "\n",
    "\n",
    "### Multinomial-Dirichlet Conjugacy\n",
    "\n",
    "- Likelihood: p(x | \\theta) ~ Multinomial(\\theta_1,...,\\theta_k)\n",
    "- Prior: p(\\theta) ~ Dirichlet(\\alpha_1,...,\\alpha_k)\n",
    "- Posterior: p(\\theta | x) ~ Dirichlet(.)\n",
    "\n",
    "\n",
    "### Poisson-Gamma Conjugacy\n",
    "\n",
    "- p(x | \\theta) ~ Poisson(\\theta)\n",
    "- p(\\theta) ~ Gamma(\\theta, \\theta)\n",
    "- p(\\theta | x) ~ Gamma(.)\n",
    "\n",
    "\n",
    "### Gaussian-Gaussian Conjugacy\n",
    "\n",
    "- p(x | \\mu, \\sigma^2) ~ Gaussian(\\mu, \\sigma^2)\n",
    "- If \\sigma^2 fixed,\n",
    "  * p(\\mu) ~ Gaussian(\\mu_0, \\sigma_0^2)\n",
    "  * p(\\mu | x) ~ Gaussian(.)\n",
    "  * Notes: Var(X) = E[Var(X | \\mu)] + Var[E(X | \\mu)]\n",
    "- If \\mu^2 fixed,\n",
    "  * p(\\sigma^2) ~ InverseGamma(\\theta_1, \\theta_2)\n",
    "  * p(\\sigma^2 | x) ~ InverseGamma(.)\n",
    "  * Note: X ~ InverseGamma() => X^(-1) ~ Gamma(.)\n",
    "- If both (\\mu, \\sigma^2) are unknown, fit by stagewise modeling:\n",
    "  * p(\\mu | \\sigma^2) ~ N(\\mu_0, n0 \\tau)\n",
    "  * p(\\tau) ~ Gamma(\\alpha, \\beta)\n",
    "\n",
    "\n",
    "### General Exponential Family Conjugacy\n",
    "\n",
    "- Exponential Family: \n",
    "  p(x | \\eta) = h(x) exp{\\eta^T(x) - A(\\eta)}\n",
    "- Conjugate prior:\n",
    "  p(\\eta) = H(\\tau, n0) exp{\\tau^\\eta - n0 A(\\eta)}\n",
    "- Posterior probability:\n",
    "  p(\\eta | x) ~ (\\tau, n0) -> (\\tau + \\sum T(x_j), n + n0)\n",
    "\n",
    "\n",
    "### Mutivariate Gaussian Conjugacy\n",
    "\n",
    "- p(x | \\mu, \\Sigma) ~ MutivariateGaussian(\\mu, \\Sigma)\n",
    "  * Similar with Gaussian(.)\n",
    "- If \\Sigma fixed,\n",
    "  * p(\\mu) ~ MutivariateGaussian(\\mu_0, \\Sigma_0)\n",
    "  * p(\\mu | x) ~ MutivariateGaussian(.)\n",
    "- If \\mu fixed,\n",
    "  * p(\\Sigma) ~ InverseWishart(\\theta_1, \\theta_2)\n",
    "  * InverseWishart = InverseGamma(.)\n",
    "  * p(Precision Matrix) = p(\\Sigma^{-1}) \n",
    "    ~ Wishart(.) = GeneralizedGamma(.)\n",
    "  * p(\\Sigma | x) ~ InverseWishart(.)\n",
    "- If both (\\mu, \\Sigma) are unknown:\n",
    "  * p(\\mu | \\Sigma) ~ N(\\mu_0, n0 \\tau)\n",
    "  * p(\\tau) ~ InverseWishart(\\alpha, \\beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jeffreys Priors\n",
    "\n",
    "- Invariance Principle: Jeffreys Priors is invariant to change of variable \\theta.\n",
    "- Jeffreys Priors: Square root of Fisher Information.\n",
    "  \\pi_J = I(\\theta)^{1/2}.\n",
    "- Fisher Information:\n",
    "  I(\\theta) = E [- d^2 log p(x | \\theta) / d\\theta^2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
