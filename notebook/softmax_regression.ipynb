{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression with Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Logistic regression is one of the most fundamental machine learning models for binary classification. I will summarize its methodology and implement it in NumPy and PyTorch.\n",
    "\n",
    "The problem we solve is **multiclass classification,** for example, we would like to enable computer vision to classify grayscale images into one of the 10 classes. \n",
    "\n",
    "We will load the MNIST data from scikit-learn as a toy dataset, and split the data into the training and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression Model\n",
    "\n",
    "[To be continued.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "sys.path.append('../numpy/')\n",
    "from metrics import accuracy\n",
    "\n",
    "np.random.seed(71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read breast cancer data.\n",
    "mnist_data = load_digits()\n",
    "X, y = mnist_data.data, mnist_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64), (1797,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pixel_0_0', 'pixel_0_1', 'pixel_0_2', 'pixel_0_3', 'pixel_0_4', 'pixel_0_5', 'pixel_0_6', 'pixel_0_7', 'pixel_1_0', 'pixel_1_1', 'pixel_1_2', 'pixel_1_3', 'pixel_1_4', 'pixel_1_5', 'pixel_1_6', 'pixel_1_7', 'pixel_2_0', 'pixel_2_1', 'pixel_2_2', 'pixel_2_3', 'pixel_2_4', 'pixel_2_5', 'pixel_2_6', 'pixel_2_7', 'pixel_3_0', 'pixel_3_1', 'pixel_3_2', 'pixel_3_3', 'pixel_3_4', 'pixel_3_5', 'pixel_3_6', 'pixel_3_7', 'pixel_4_0', 'pixel_4_1', 'pixel_4_2', 'pixel_4_3', 'pixel_4_4', 'pixel_4_5', 'pixel_4_6', 'pixel_4_7', 'pixel_5_0', 'pixel_5_1', 'pixel_5_2', 'pixel_5_3', 'pixel_5_4', 'pixel_5_5', 'pixel_5_6', 'pixel_5_7', 'pixel_6_0', 'pixel_6_1', 'pixel_6_2', 'pixel_6_3', 'pixel_6_4', 'pixel_6_5', 'pixel_6_6', 'pixel_6_7', 'pixel_7_0', 'pixel_7_1', 'pixel_7_2', 'pixel_7_3', 'pixel_7_4', 'pixel_7_5', 'pixel_7_6', 'pixel_7_7']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "        15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "        12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "         0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "        10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., 12., 13.,  5.,  0.,  0.,  0.,  0.,  0., 11., 16.,\n",
       "         9.,  0.,  0.,  0.,  0.,  3., 15., 16.,  6.,  0.,  0.,  0.,  7.,\n",
       "        15., 16., 16.,  2.,  0.,  0.,  0.,  0.,  1., 16., 16.,  3.,  0.,\n",
       "         0.,  0.,  0.,  1., 16., 16.,  6.,  0.,  0.,  0.,  0.,  1., 16.,\n",
       "        16.,  6.,  0.,  0.,  0.,  0.,  0., 11., 16., 10.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  4., 15., 12.,  0.,  0.,  0.,  0.,  3., 16., 15.,\n",
       "        14.,  0.,  0.,  0.,  0.,  8., 13.,  8., 16.,  0.,  0.,  0.,  0.,\n",
       "         1.,  6., 15., 11.,  0.,  0.,  0.,  1.,  8., 13., 15.,  1.,  0.,\n",
       "         0.,  0.,  9., 16., 16.,  5.,  0.,  0.,  0.,  0.,  3., 13., 16.,\n",
       "        16., 11.,  5.,  0.,  0.,  0.,  0.,  3., 11., 16.,  9.,  0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(mnist_data.feature_names)\n",
    "X[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(mnist_data.target_names)\n",
    "y[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test datasets.\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=71, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1347, 64) (1347,)\n",
      "(450, 64) (450,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_raw.shape, y_train.shape)\n",
    "print(X_test_raw.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering for standardizing features by min-max scaler.\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "X_train = min_max_scaler.fit_transform(X_train_raw)\n",
    "X_test = min_max_scaler.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert arrays to float32.\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    np.float32(X_train), np.float32(X_test), np.float32(y_train), np.float32(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float32'), dtype('float32'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype, y_train.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy Implementation of Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression(object):\n",
    "    \"\"\"Numpy implementation of Softmax Regression.\"\"\"\n",
    "    # TODO\n",
    "\n",
    "    def __init__(self, batch_size=64, lr=0.01, n_epochs=1000):\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "    def get_data(self, X_train, y_train, shuffle=True):\n",
    "        \"\"\"Get dataset and information.\"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        # Get the numbers of examples and inputs.\n",
    "        self.n_examples, self.n_inputs = self.X_train.shape\n",
    "\n",
    "        if shuffle:\n",
    "            idx = list(range(self.n_examples))\n",
    "            random.shuffle(idx)\n",
    "            self.X_train = self.X_train[idx]\n",
    "            self.y_train = self.y_train[idx]\n",
    "\n",
    "    def _create_weights(self):\n",
    "        \"\"\"Create model weights and bias.\"\"\"\n",
    "        self.w = np.zeros(self.n_inputs).reshape(self.n_inputs, 1)\n",
    "        self.b = np.zeros(1).reshape(1, 1)\n",
    "\n",
    "    def _logit(self, X):\n",
    "        \"\"\"Logit: unnormalized log probability.\"\"\"\n",
    "        return np.matmul(X, self.w) + self.b\n",
    "\n",
    "    def _sigmoid(self, logit):\n",
    "        \"\"\"Sigmoid function by stabilization trick.\n",
    "\n",
    "        sigmoid(z) = 1 / (1 + exp(-z)) \n",
    "                   = exp(z) / (1 + exp(z)) * exp(z_max) / exp(z_max)\n",
    "                   = exp(z - z_max) / (exp(-z_max) + exp(z - z_max)),\n",
    "        where z is the logit, and z_max = z - max(0, z).\n",
    "        \"\"\"\n",
    "        logit_max = np.maximum(0, logit)\n",
    "        logit_stable = logit - logit_max\n",
    "        return np.exp(logit_stable) / (np.exp(-logit_max) + np.exp(logit_stable))\n",
    "    \n",
    "    def _model(self, X):\n",
    "        \"\"\"Logistic regression model.\"\"\"\n",
    "        logit = self._logit(X)\n",
    "        return self._sigmoid(logit)\n",
    "\n",
    "    def _loss(self, y, logit):\n",
    "        \"\"\"Cross entropy loss by stabilizaiton trick.\n",
    "\n",
    "        cross_entropy_loss(y, z) \n",
    "          = - 1/n * \\sum_{i=1}^n y_i * log p(y_i = 1|x_i) + (1 - y_i) * log p(y_i = 0|x_i)\n",
    "          = - 1/n * \\sum_{i=1}^n y_i * (z_i - log(1 + exp(z_i))) + (1 - y_i) * (-log(1 + exp(z_i))),\n",
    "        where z is the logit, z_max = z - max(0, z),\n",
    "          log p(y = 1|x)\n",
    "            = log (1 / (1 + exp(-z))) \n",
    "            = log (exp(z) / (1 + exp(z)))\n",
    "            = z - log(1 + exp(z))\n",
    "        and \n",
    "          log(1 + exp(z)) := logsumexp(z)\n",
    "            = log(exp(0) + exp(z))\n",
    "            = log(exp(0) + exp(z) * exp(z_max) / exp(z_max))\n",
    "            = z_max + log(exp(-z_max) + exp(z - z_max)).\n",
    "        \"\"\"\n",
    "        logit_max = np.maximum(0, logit)\n",
    "        logit_stable = logit - logit_max\n",
    "        logsumexp_stable = logit_max + np.log(np.exp(-logit_max) + np.exp(logit_stable))\n",
    "        self.cross_entropy = -(y * (logit - logsumexp_stable) + (1 - y) * (-logsumexp_stable))\n",
    "        return np.mean(self.cross_entropy)\n",
    "\n",
    "    def _optimize(self, X, y):\n",
    "        \"\"\"Optimize by stochastic gradient descent.\"\"\"\n",
    "        m = X.shape[0]\n",
    "\n",
    "        y_ = self._model(X) \n",
    "        dw = 1 / m * np.matmul(X.T, y_ - y)\n",
    "        db = np.mean(y_ - y)\n",
    "\n",
    "        for (param, grad) in zip([self.w, self.b], [dw, db]):\n",
    "            param[:] = param - self.lr * grad\n",
    "\n",
    "    def _fetch_batch(self):\n",
    "        \"\"\"Fetch batch dataset.\"\"\"\n",
    "        idx = list(range(self.n_examples))\n",
    "        for i in range(0, self.n_examples, self.batch_size):\n",
    "            idx_batch = idx[i:min(i + self.batch_size, self.n_examples)]\n",
    "            yield (self.X_train.take(idx_batch, axis=0), self.y_train.take(idx_batch, axis=0))\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"Fit model.\"\"\"\n",
    "        self._create_weights()\n",
    "\n",
    "        for epoch in range(1, self.n_epochs + 1):\n",
    "            total_loss = 0\n",
    "            for X_train_b, y_train_b in self._fetch_batch():\n",
    "                y_train_b = y_train_b.reshape((y_train_b.shape[0], -1))\n",
    "                self._optimize(X_train_b, y_train_b)\n",
    "                train_loss = self._loss(y_train_b, self._logit(X_train_b))\n",
    "                total_loss += train_loss * X_train_b.shape[0]\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print('epoch {0}: training loss {1}'.format(epoch, total_loss / self.n_examples))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_coeff(self):\n",
    "        return self.b, self.w.reshape((-1,))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self._model(X).reshape((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Softmax Regression in NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our Softmax Regression.\n",
    "softmax = SoftmaxRegression(batch_size=64, lr=1, n_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get datasets and build graph.\n",
    "softmax.get_data(X_train, y_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coefficient.\n",
    "softmax.get_coeff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probabilities for training data.\n",
    "p_train_ = softmax.predict(X_train)\n",
    "p_train_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted labels for training data.\n",
    "y_train_ = (p_train_ > 0.5) * 1\n",
    "y_train_[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction accuracy for training data.\n",
    "accuracy(y_train, y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted label correctness for test data.\n",
    "p_test_ = softmax.predict(X_test)\n",
    "print(p_test_[:10])\n",
    "y_test_ = (p_test_ > 0.5) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction accuracy for test data.\n",
    "accuracy(y_test, y_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Implementation of Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegressionTorch(nn.Module):\n",
    "    \"\"\"PyTorch implementation of Softmax Regression.\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size=64, lr=0.01, n_epochs=1000):\n",
    "        super(SoftmaxRegressionTorch, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "    def get_data(self, X_train, y_train, shuffle=True):\n",
    "        \"\"\"Get dataset and information.\"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        # Get the numbers of examples and inputs.\n",
    "        self.n_examples, self.n_inputs = self.X_train.shape\n",
    "\n",
    "        if shuffle:\n",
    "            idx = list(range(self.n_examples))\n",
    "            random.shuffle(idx)\n",
    "            self.X_train = self.X_train[idx]\n",
    "            self.y_train = self.y_train[idx]\n",
    "\n",
    "    def _create_model(self):\n",
    "        \"\"\"Create logistic regression model.\"\"\"\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.n_inputs, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.model(x)\n",
    "        return y\n",
    "\n",
    "    def _create_loss(self):\n",
    "        \"\"\"Create (binary) cross entropy loss.\"\"\"\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def _create_optimizer(self):\n",
    "        \"\"\"Create optimizer by stochastic gradient descent.\"\"\"\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"Build model, loss function and optimizer.\"\"\"\n",
    "        self._create_model()\n",
    "        self._create_loss()\n",
    "        self._create_optimizer()\n",
    "\n",
    "    def _fetch_batch(self):\n",
    "        \"\"\"Fetch batch dataset.\"\"\"\n",
    "        idx = list(range(self.n_examples))\n",
    "        for i in range(0, self.n_examples, self.batch_size):\n",
    "            idx_batch = idx[i:min(i + self.batch_size, self.n_examples)]\n",
    "            yield (self.X_train.take(idx_batch, axis=0), \n",
    "                   self.y_train.take(idx_batch, axis=0))\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"Fit model.\"\"\"\n",
    "        for epoch in range(1, self.n_epochs + 1):\n",
    "            total_loss = 0\n",
    "            for X_train_b, y_train_b in self._fetch_batch():\n",
    "                # Convert to Tensor from NumPy array and reshape ys.\n",
    "                X_train_b, y_train_b = (\n",
    "                    torch.from_numpy(X_train_b), \n",
    "                    torch.from_numpy(y_train_b).view(-1, 1))\n",
    "\n",
    "                y_pred_b = self.model(X_train_b)\n",
    "                batch_loss = self.criterion(y_pred_b, y_train_b)\n",
    "                total_loss += batch_loss * X_train_b.shape[0]\n",
    "\n",
    "                # Zero grads, performs backward pass, and update weights.\n",
    "                self.optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print('Epoch {0}: training loss: {1}'\n",
    "                      .format(epoch, total_loss / self.n_examples))\n",
    "\n",
    "    def get_coeff(self):\n",
    "        \"\"\"Get model coefficients.\"\"\"\n",
    "        # Detach var which require grad.\n",
    "        return (self.model[0].bias.detach().numpy(),\n",
    "                self.model[0].weight.detach().numpy())\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict for new data.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            X_ = torch.from_numpy(X)\n",
    "            return self.model(X_).numpy().reshape((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Softmax Regression in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PyTorch Logistic Regression.\n",
    "softmax_torch = SoftmaxRegressionTorch(batch_size=64, lr=0.5, n_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_torch.get_data(X_train, y_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_torch.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_torch.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_torch.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coefficient.\n",
    "softmax_torch.get_coeff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probabilities for training data.\n",
    "p_train_ = softmax_torch.predict(X_train)\n",
    "p_train_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted labels for training data.\n",
    "y_train_ = (p_train_ > 0.5) * 1\n",
    "y_train_[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction accuracy for training data.\n",
    "accuracy(y_train, y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted label correctness for test data.\n",
    "p_test_ = softmax_torch.predict(X_test)\n",
    "print(p_test_[:10])\n",
    "y_test_ = (p_test_ > 0.5) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction accuracy for test data.\n",
    "accuracy(y_test, y_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark with Sklearn's Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit sklearn's Softmax Regression.\n",
    "from sklearn.linear_model import LogisticRegression as LogisticRegressionSklearn\n",
    "\n",
    "softmax_sk = LogisticRegressionSklearn(C=1e4, multi_class='multinomial', max_iter=500)\n",
    "\n",
    "softmax_sk.fit(X_train, y_train.reshape(y_train.shape[0], ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coefficients.\n",
    "softmax_sk.intercept_, softmax_sk.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted labels for training data.\n",
    "p_train_ = softmax_sk.predict(X_train)\n",
    "p_train_[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ = (p_train_ > 0.5) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction accuracy for training data.\n",
    "accuracy(y_train, y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted label correctness for test data.\n",
    "p_test_ = softmax_sk.predict(X_test)\n",
    "y_test_ = (p_test_ > 0.5) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prediction accuracy for test data.\n",
    "accuracy(y_test, y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
