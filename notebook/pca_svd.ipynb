{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA and SVD with Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bowen Li\n",
    "- 2016/10/05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "- What is Principal Component Analysis (PCA)?\n",
    "- What is Singular Value Decomposition (SVD)?\n",
    "- PCA and SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is PCA?\n",
    "\n",
    "**Motivation:** Identify the most meaningful basis to re-express data which can maximize the **Signal-to-Noise Ratio (SNR):** $\\sigma^2_{signal} / \\sigma^2_{noise}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/snr.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Minimize redundancy (noise), measured by the magnitude of the covariance.\n",
    "- Maximize the signal, measured by the variance; large variance corresponds to interesting structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the observed data be\n",
    "\n",
    "$$\n",
    "\\underset{(n \\times p)}X = \\{x_{ij}\\} =\n",
    "\\begin{bmatrix}\n",
    "x_{11} & \\cdots & x_{1p} \\\\\n",
    "\\vdots &        & \\vdots \\\\ \n",
    "x_{n1} & \\cdots & x_{np}\n",
    "\\end{bmatrix}\n",
    "= [x_1, \\cdots, x_p]\n",
    "$$\n",
    "\n",
    "- each row represents an **example** measured on $p$-dimensional features,\n",
    "- each column, $x_j$, represents **normalized feature** with **zero means,** $E(x_j) = 0$, and **variance one,** $Var(x_j) = 1$.\n",
    "\n",
    "$$\n",
    "x_j = \n",
    "\\begin{bmatrix}\n",
    "x_{1j} \\\\\n",
    "\\vdots \\\\ \n",
    "x_{nj}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Covariance Matrix:**\n",
    "\n",
    "$$\n",
    "\\underset{(p \\times p)}{C_X} = \\frac{1}{n}X^{T}X \n",
    "= \\frac{1}{n}\n",
    "\\begin{bmatrix}\n",
    "x^T_{1} \\\\\n",
    "\\vdots \\\\ \n",
    "x^T_{p}\n",
    "\\end{bmatrix}\n",
    "[x_1,...,x_p]\n",
    "= \\frac{1}{n}\n",
    "\\begin{bmatrix}\n",
    "x^T_{1}x_1 & \\cdots & x^T_{1}x_p \\\\\n",
    "\\vdots     &        & \\vdots \\\\ \n",
    "x^T_{p}x_1 & \\cdots & x^T_{p}x_p\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Goal:** Find some transformation, $Y = f(X)$, of X such that covariance of $Y$ is a *diagonal* matrix,\n",
    "\n",
    "$$\n",
    "\\underset{(p \\times p)}{C_Y} = \\frac{1}{n}Y^{T}Y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways for diagonalizing $C_X$, PCA arguably select the easiest method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assumptions for PCA:**\n",
    "- Linear transformation: Let $\\underset{p \\times p}E = [e_1,...,e_p]$,\n",
    "\n",
    "$$\n",
    "Y = XE\n",
    "$$\n",
    "\n",
    "- Large variance have important structure.\n",
    "- The principal components are orthogonal. \n",
    "\n",
    "The last assumption provides an intuitive simplification that makes PCA soluble with linear algebra decomposition techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computation for PCA:** Based on the above assumptions, define\n",
    "- First principal component = linear combination $e^T_1X$ that maximizes $Var(e^T_1X)$, subject to $e^T_1e_1 = 1$.\n",
    "- Second principal component = linear combination $e^T_2X$ that maximizes $Var(e^T_2X)$, subject to $e^T_2e_2 = 1$ and $Cov(e^T_1X, e^T_2X) = 0$.\n",
    "- At the ith step, ith principal component = linear combination $e^T_jX$ that maximizes $Var(e^T_jX)$, subject to $e^T_j e_j = 1$ and $Cov(e^T_kX, e^T_jX) = 0$ for $k < j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eigenvector Decomposition (also called Spectral Decomposition):**\n",
    "- Suppose $Y = XE$,\n",
    "\n",
    "$$\n",
    "C_Y = \\frac{1}{n}Y^TY = \\frac{1}{n}(XE)^T(XE) = E^T \\left(\\frac{1}{n}X^TX \\right)E\n",
    "$$\n",
    "\n",
    "- Since $\\frac{1}{n}X^TX$ is a *symmetric* matrix, it can be **diagonalized by a matrix of orthonormal eigenvectors** using **Spectral Decomposistion.** That is,\n",
    "\n",
    "$$\n",
    "\\frac{1}{n}X^TX = VDV^T\n",
    "$$\n",
    "\n",
    "where $D = diag(d_1,...,d_p)$ is a **diagonal** matrix with rank-ordered set of **eigenvalues**, $d_1 \\ge d_2 \\ge ... \\ge d_p$, and $V = [v_1,...,v_p]$ is the matrix of the corresponding **eigenvectors** with $V^TV=VV^T=1$.\n",
    "\n",
    "- *Sketch of proof:* From the Spectral Decomposition, for all $j$, we have \n",
    "\n",
    "$$\n",
    "\\frac{1}{n}X^T X v_j = d_j v_j\n",
    "$$\n",
    "\n",
    "Then, $\\frac{1}{n}X^T X V = V D \\Rightarrow \\frac{1}{n}X^T X = V D V^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Alternative proof:* We can interpret PCA as finding a **unit vector** $u$ which **maximizes variance of the projection of all points $x_i$'s onto $u$:**\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^n (x_i^T u)^2 \n",
    "= \\frac{1}{n} \\sum_{i=1}^n (x_i^T u)^T (x_i^T u)\n",
    "= u^T \\big(\\frac{1}{n} \\sum_{i=1}^n x_i x_i^T \\big) u\n",
    "$$\n",
    "\n",
    "Thus, we would like to solve\n",
    "\n",
    "$$\n",
    "max_u u^T \\big(\\frac{1}{n} \\sum_{i=1}^n x_i x_i^T \\big) u\n",
    "$$\n",
    "\n",
    "such that $u^T u = 1$. Using the **Lagrange Multiplier Method,** the objective becomes\n",
    "\n",
    "$$\n",
    "max_u L(u, \\lambda) = u^T \\big(\\frac{1}{n} \\sum_{i=1}^n x_i x_i^T \\big) u - \\lambda (u^T u - 1)\n",
    "$$\n",
    "\n",
    "Take partial derivative with respect to $u$ we have\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial u} L(u, \\lambda) = \\frac{1}{n} X^T X u - \\lambda u := 0\n",
    "$$\n",
    "\n",
    "Hence, we are solving $\\frac{1}{n} X^T X u = \\lambda u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trick:** Select $E = V$. Then since $V^TV=VV^T = I$, $C_Y$ now is a diagonal matrix\n",
    "\n",
    "$$\n",
    "C_Y = E^T \\left( \\frac{1}{n}X^T X \\right) E\n",
    "    = V^T \\left( V D V^T \\right) V = D\n",
    "$$\n",
    "\n",
    "From the above results we can observe that the **eigenvalue $\\lambda_i$ is the variance of transformed feature $y_j$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of PCA:**\n",
    "- Substract off the mean of each feature: $x_j \\leftarrow x_j - \\overline x_j$, and divide the standard deviation: $x_j \\leftarrow x_j/\\sigma_j$\n",
    "- Compute the eigenvectors of covariance matrix, $\\frac{1}{n}X^TX$\n",
    "- Principal components of $X$: eigenvectors, $v_i$, of covariance matrix\n",
    "- The ith diagonal value of $C_Y$: variance of $X$ along $v_i$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is SVD?\n",
    "First recall the eigenvector decomposition: $\\frac{1}{n}X^T X v_j = d_j v_j$. Define\n",
    "- **Singular values:** $\\lambda_j = \\sqrt{d_j}$, **square root of eigenvalue (variance)** for $j = 1,...,k$.\n",
    "- Matrix $U = [ u_1,...,u_2 ]$, with $u_j$ defined by\n",
    "\n",
    "$$\n",
    "u_j = \\frac{1}{\\lambda_j}\\left( \\frac{1}{\\sqrt{n}}X \\right) v_j\n",
    "$$\n",
    "\n",
    "where $v_j$ is the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then (1) $U$ is *orthonormal* matrix:\n",
    "$u_j^T u_k = \n",
    "\\begin{cases} \n",
    "1, \\text{if } j = k \\\\\n",
    "0, \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "and (2) $\\lVert X v_j\\rVert = \\lambda_j$\n",
    "\n",
    "- *Sketch of proof:* $u_j^T u_k = \\frac{1}{n} \\left( \\frac{1}{\\lambda_j} X v_j \\right)^T \\left( \\frac{1}{\\lambda_k} X v_k \\right) = \\frac{1}{n} \\frac{1}{\\lambda_j \\lambda_k} v_j^T X^T X v_k = \\frac{1}{\\lambda_j \\lambda_k} v_j^T d_j v_k = \\frac{\\lambda_j}{\\lambda_k} v_j^T v_k$.\n",
    "\n",
    "The first result follows. Further, we can obtain the second result similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By rewriting we have\n",
    "\n",
    "$$\n",
    "\\left( \\frac{1}{\\sqrt{n}}X \\right) v_j = \\lambda_j u_j\n",
    "$$\n",
    "\n",
    "That is, **normalized $X$ multiplied by an eigenvector of covariance of $X$, $\\frac{1}{n} X^T X$, is equal to a scalar times another vector.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By constructing a new *diagonal* matrix of singular values, $\\Sigma = diag(\\lambda_1,...,\\lambda_p)$, where $\\lambda_1 \\ge \\lambda_2 \\ge ... \\ge \\lambda_p$.\n",
    "- Define $V = [v_1,...,v_p]$ and $U = [u_1,...,u_p]$.\n",
    "\n",
    "**Matrix Version of SVD:**\n",
    "\n",
    "$$\n",
    "\\left( \\frac{1}{\\sqrt{n}}X \\right) V = U \\Sigma,\n",
    "$$\n",
    "$$\n",
    "\\frac{1}{\\sqrt{n}}X = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "\n",
    "Hence, **any arbitrary matrix $\\frac{1}{\\sqrt{n}}X$ can be converted to**\n",
    "- **an orthogonal matrix (rotation): $U$**\n",
    "- **a diagonal matrix (stretch): $\\Sigma$**\n",
    "- **another orthogonal matrix (second rotation): $V$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCA and SVD\n",
    "From SVD, we have\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} X^T X = \\left( \\frac{1}{\\sqrt{n}}X \\right)^T \\left( \\frac{1}{\\sqrt{n}}X \\right)\n",
    "                  = \\left( U \\Sigma V^T \\right)^T \\left( U \\Sigma V^T \\right) \\\\\n",
    "                  = \\left( V \\Sigma U^T U \\Sigma V^T \\right)\n",
    "                  = V \\Sigma^2 V^T \\equiv V D V^T\n",
    "$$\n",
    "\n",
    "That is, **squared singular value is equal to variance of $X$ along $v_j$, $\\lambda_j^2 = d_j$.**\n",
    "\n",
    "**How many pricipal components we shall use?**\n",
    "- **Total variances:** $\\textstyle \\sum_{j=1}^p \\lambda_j^2$.\n",
    "- **Scree plot:** Choose the number of principal component by **Elbow Method.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/scree_plot_pca.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More on SVD:**\n",
    "- Recall from SVD, $\\frac{1}{\\sqrt{n}}X = U \\Sigma V^T$\n",
    "- Let $s < p = rank(X)$. Then the **reduced rank-$s$ least squares approximation to $\\frac{1}{\\sqrt{n}} X$ is**\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sqrt{n}}\\widehat{X} = \\sum_{j=1}^s \\lambda_j u_j v_j^T\n",
    "$$\n",
    "\n",
    "which **minimizes**\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^p \\left( x_{ij} - \\widehat{x}_{ij} \\right)^2\n",
    "= tr \\left[ \\left(\\frac{1}{\\sqrt{n}} (X - \\widehat{X}) \\right) \\left(\\frac{1}{\\sqrt{n}} (X - \\widehat{X}) \\right)^T \\right]\n",
    "$$\n",
    "\n",
    "over all matrices $\\widehat{X}$ having rank no greater than $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: implement PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Johnson & Wichern (2002). Applied Multivariate Statistical Analysis.\n",
    "- Shlens (arXiv, 2014). A Tutorial on Principal Component Analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
