{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Numpy Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Logistic regression is one of the most fundamental machine learning models for binary classification. I will summarize its methodology and implement it from scratch using NumPy.\n",
    "\n",
    "The problem we solve is **binary classification,** for example, the doctor would like to base on patients's features, including mean radius, mean texture, etc, to classify breat cancer into one of the following two case:\n",
    "\n",
    "- \"malignant\":  ùë¶=1 \n",
    "- \"benign\":  ùë¶=0 \n",
    "\n",
    "which correspond to serious and gentle case respectively.\n",
    "\n",
    "We will load the breast cancer data from scikit-learn as a toy dataset, and split the data into the training and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression Model\n",
    "\n",
    "[To be continued.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Numpy Implementation of Logistic Regressio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/bowen0701/machine-learning/blob/master/logistic_regression.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(71)\n",
    "\n",
    "\n",
    "class LogisticRegression(object):\n",
    "    \"\"\"Numpy implementation of Logistic Regression.\"\"\"\n",
    "    def __init__(self, batch_size=10, lr=0.01, n_epochs=5):\n",
    "        self._batch_size = batch_size\n",
    "        self._lr = lr\n",
    "        self._n_epochs = n_epochs\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        def f(x):\n",
    "            if x < 0:\n",
    "                return np.exp(x) / (1 + np.exp(x))\n",
    "            else:\n",
    "                return 1 / (1 + np.exp(-x))\n",
    "        return np.array(list(map(f, z)))\n",
    "\n",
    "    def _logreg(self, X, w, b):\n",
    "        return self._sigmoid(np.dot(X, w) + b)\n",
    "\n",
    "    def _cross_entropy(self, y_hat, y, eps=1e-7):\n",
    "        # To avoid overflow in log, add epsilon = 1E-7.\n",
    "        return - np.mean(y * np.log(y_hat + eps) + (1 - y) * np.log(1 - y_hat + eps))\n",
    "\n",
    "    def _weights_init(self):\n",
    "        w = np.zeros(self._n_inputs).reshape(self._n_inputs, 1)\n",
    "        b = np.zeros(1).reshape(1, 1)\n",
    "        return w, b\n",
    "\n",
    "    def _sgd(self, X, y, w, b):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        y_hat = self._logreg(X, w, b) \n",
    "        dw = - 1 / m * np.matmul(X.T, y - y_hat)\n",
    "        db = - np.mean(y - y_hat)\n",
    "        \n",
    "        for (param, grad) in zip([w, b], [dw, db]):\n",
    "            param[:] = param - self._lr * grad\n",
    "\n",
    "    def _data_iter(self):\n",
    "        idx = list(range(self._n_examples))\n",
    "        random.shuffle(idx)\n",
    "        for i in range(0, self._n_examples, self._batch_size):\n",
    "            idx_batch = np.array(\n",
    "                idx[i:min(i + self._batch_size, self._n_examples)])\n",
    "            yield (self._X_train.take(idx_batch, axis=0), \n",
    "                   self._y_train.take(idx_batch, axis=0))\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self._X_train = X_train\n",
    "        self._y_train = y_train\n",
    "        self._n_examples, self._n_inputs = X_train.shape\n",
    "\n",
    "        logreg = self._logreg\n",
    "        loss = self._cross_entropy\n",
    "        w, b = self._weights_init()\n",
    "\n",
    "        for epoch in range(self._n_epochs):\n",
    "            for step, (X, y) in enumerate(self._data_iter()):\n",
    "                y = y.reshape((y.shape[0], -1))\n",
    "                self._sgd(X, y, w, b)\n",
    "            train_loss = loss(logreg(X, w, b), y)\n",
    "            if epoch % 10 == 0:\n",
    "                print('epoch {0}: loss {1}'.format(epoch + 1, train_loss))\n",
    "\n",
    "        self._logreg = logreg\n",
    "        self._w, self._b = w, b\n",
    "        return self\n",
    "\n",
    "    def get_coeff(self):\n",
    "        return self._b, self._w.reshape((-1,))\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self._logreg(X_test, self._w, self._b).reshape((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression as LogisticRegressionSklearn\n",
    "\n",
    "# https://github.com/bowen0701/machine-learning/blob/master/numpy_metrics.py\n",
    "from numpy_metrics import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read breast cancer data.\n",
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.799e+01, 1.038e+01, 1.228e+02, 1.001e+03, 1.184e-01, 2.776e-01,\n",
       "        3.001e-01, 1.471e-01, 2.419e-01, 7.871e-02, 1.095e+00, 9.053e-01,\n",
       "        8.589e+00, 1.534e+02, 6.399e-03, 4.904e-02, 5.373e-02, 1.587e-02,\n",
       "        3.003e-02, 6.193e-03, 2.538e+01, 1.733e+01, 1.846e+02, 2.019e+03,\n",
       "        1.622e-01, 6.656e-01, 7.119e-01, 2.654e-01, 4.601e-01, 1.189e-01],\n",
       "       [2.057e+01, 1.777e+01, 1.329e+02, 1.326e+03, 8.474e-02, 7.864e-02,\n",
       "        8.690e-02, 7.017e-02, 1.812e-01, 5.667e-02, 5.435e-01, 7.339e-01,\n",
       "        3.398e+00, 7.408e+01, 5.225e-03, 1.308e-02, 1.860e-02, 1.340e-02,\n",
       "        1.389e-02, 3.532e-03, 2.499e+01, 2.341e+01, 1.588e+02, 1.956e+03,\n",
       "        1.238e-01, 1.866e-01, 2.416e-01, 1.860e-01, 2.750e-01, 8.902e-02],\n",
       "       [1.969e+01, 2.125e+01, 1.300e+02, 1.203e+03, 1.096e-01, 1.599e-01,\n",
       "        1.974e-01, 1.279e-01, 2.069e-01, 5.999e-02, 7.456e-01, 7.869e-01,\n",
       "        4.585e+00, 9.403e+01, 6.150e-03, 4.006e-02, 3.832e-02, 2.058e-02,\n",
       "        2.250e-02, 4.571e-03, 2.357e+01, 2.553e+01, 1.525e+02, 1.709e+03,\n",
       "        1.444e-01, 4.245e-01, 4.504e-01, 2.430e-01, 3.613e-01, 8.758e-02]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test datasets.\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=71, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30) (426,)\n",
      "(143, 30) (143,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_raw.shape, y_train.shape)\n",
    "print(X_test_raw.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering for standardizing features by min-max scaler.\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "X_train = min_max_scaler.fit_transform(X_train_raw)\n",
    "X_test = min_max_scaler.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fitting Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss 2.432988683901155\n",
      "epoch 11: loss 0.08269506633173168\n",
      "epoch 21: loss 0.02205420752252182\n",
      "epoch 31: loss 0.043176617239112924\n",
      "epoch 41: loss 0.3093572198198679\n",
      "epoch 51: loss 0.10770636028790158\n",
      "epoch 61: loss 0.029891748379843333\n",
      "epoch 71: loss 0.02073525084060601\n",
      "epoch 81: loss 0.05118910235934664\n",
      "epoch 91: loss 0.01949032998807695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.LogisticRegression at 0x7fc0fa8d4048>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit our Logistic Regression.\n",
    "clf = LogisticRegression(batch_size=100, lr=10, n_epochs=100)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[16.37297703]]),\n",
       " array([-1.22119274, -3.19738366, -1.32460176, -2.82844307, -1.02977342,\n",
       "         1.05390748, -4.41262344, -6.73851879, -1.45374884,  3.14150419,\n",
       "        -7.11112612, -0.52413638, -5.56553335, -4.46050678,  1.55378861,\n",
       "         3.95417232,  1.93726362,  1.38320829,  2.75462555,  2.93060893,\n",
       "        -5.2695596 , -4.95495259, -4.70659005, -5.21991334, -3.38171848,\n",
       "        -0.77473968, -3.70639665, -5.12344415, -2.59490509, -0.86470708]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get coefficient.\n",
    "clf.get_coeff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.97464581e-01, 4.58526243e-12, 2.56514901e-04])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted probabilities for training data.\n",
    "p_pred_train = clf.predict(X_train)\n",
    "p_pred_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted labels for training data.\n",
    "y_pred_train = (p_pred_train > 0.5) * 1\n",
    "y_pred_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted label correctness for training data.\n",
    "# y_pred_train == y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9765258215962441"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction accuracy for training data.\n",
    "accuracy(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted label correctness for test data.\n",
    "p_pred_test = clf.predict(X_test)\n",
    "y_pred_test = (p_pred_test > 0.5) * 1\n",
    "\n",
    "# y_pred_test == y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.958041958041958"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction accuracy for test data.\n",
    "accuracy(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fitting Sklearn's Logistic Regression as Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10000.0, max_iter=500)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit sklearn's Logistic Regression.\n",
    "clf2 = LogisticRegressionSklearn(C=1e4, solver='lbfgs', max_iter=500)\n",
    "\n",
    "clf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([56.06250509]),\n",
       " array([[  53.5460616 ,  -27.2575739 ,   48.30697654,   10.5636878 ,\n",
       "          -14.75837806,   98.5009966 ,  -52.51936527,  -52.16906591,\n",
       "           -5.08742246,  -53.96348797,  -33.97198842,   -5.48905184,\n",
       "          -19.38885928,  -43.89981909,   38.75665922,  -51.43678914,\n",
       "           83.21007672,  -21.89925037,   14.96797392,   79.99757062,\n",
       "          -59.04206865,   -3.91791317,  -63.58395555, -103.96747709,\n",
       "           -7.9699581 ,   20.04904076,  -21.96650031,  -21.30939901,\n",
       "          -21.55187209,  -11.69936363]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get coefficients.\n",
    "clf2.intercept_, clf2.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted labels for training data.\n",
    "y_pred_train = clf2.predict(X_train)\n",
    "y_pred_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted label correctness for training data.\n",
    "# y_pred_train == y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction accuracy for training data.\n",
    "accuracy(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted label correctness for test data.\n",
    "y_pred_test = clf2.predict(X_test) \n",
    "# y_pred_test == y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.965034965034965"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Prediction accuracy for test data.\n",
    "accuracy(y_test, y_pred_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
