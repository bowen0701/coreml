{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic Regression Implementation from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Logistic regression is one of the most fundamental machine learning models for binary classification. I will summarize its methodology and implement it from scratch using NumPy.\n",
    "\n",
    "### Binary classification\n",
    "\n",
    "For example, the doctor would like to base on patients's features, including mean radius, mean texture, etc, to classify  breat cancer into one of the following two case: \n",
    "- \"malignant\": $y = 1$\n",
    "- \"benign\": $y = 0$\n",
    "\n",
    "which correspond to serious and gentle case respectively. \n",
    "\n",
    "We would like to load the breast cancer data from scikit-learn as a toy dataset, and split the data into the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "bc_data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 71\n",
    "TRAIN_PERCENT = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_names: \n",
      "['mean_radius', 'mean_texture', 'mean_perimeter', 'mean_area', 'mean_smoothness', 'mean_compactness', 'mean_concavity', 'mean_concave_points', 'mean_symmetry', 'mean_fractal_dimension', 'radius_error', 'texture_error', 'perimeter_error', 'area_error', 'smoothness_error', 'compactness_error', 'concavity_error', 'concave_points_error', 'symmetry_error', 'fractal_dimension_error', 'worst_radius', 'worst_texture', 'worst_perimeter', 'worst_area', 'worst_smoothness', 'worst_compactness', 'worst_concavity', 'worst_concave_points', 'worst_symmetry', 'worst_fractal_dimension']\n",
      "X: \n",
      "[[  1.79900000e+01   1.03800000e+01   1.22800000e+02 ...,   5.37200000e-01\n",
      "    2.38800000e-01   2.76800000e-01]\n",
      " [  7.61500000e-02   1.35400000e+01   1.43600000e+01 ...,   4.61900000e-02\n",
      "    4.83300000e-02   5.01300000e-02]\n",
      " [  1.98700000e-01   6.16900000e-02   1.49900000e+01 ...,   1.62400000e-01\n",
      "    3.51100000e-01   3.87900000e-01]\n",
      " ..., \n",
      " [  5.56700000e+02   1.10600000e-01   1.46900000e-01 ...,   7.75700000e-02\n",
      "    1.16700000e+01   2.00200000e+01]\n",
      " [  7.52100000e+01   4.16200000e+02   1.01600000e-01 ...,   3.05900000e-01\n",
      "    7.62600000e-02   1.08600000e+01]\n",
      " [  2.14800000e+01   6.85100000e+01   3.60500000e+02 ...,   0.00000000e+00\n",
      "    2.87100000e-01   7.03900000e-02]]\n",
      "X.shape: (30, 569)\n"
     ]
    }
   ],
   "source": [
    "features = bc_data.get('feature_names')\n",
    "features = ['_'.join(x.split()) for x in features]\n",
    "X = bc_data.get('data')\n",
    "X = X.reshape((X.shape[1], X.shape[0]))\n",
    "\n",
    "print('feature_names: \\n{}'.format(features))\n",
    "print('X: \\n{}'.format(X))\n",
    "\n",
    "print('X.shape: {}'.format(X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_names: ['malignant' 'benign']\n",
      "target: \n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      "  1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      "  1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      "  1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      "  1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      "  1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      "  1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      "  0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      "  1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      "  0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      "  1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      "  1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 0 0 0 0 0 0 1]]\n",
      "Y: (1, 569)\n"
     ]
    }
   ],
   "source": [
    "target = bc_data.get('target_names')\n",
    "Y = bc_data.get('target')\n",
    "Y = Y.reshape((1, Y.shape[0]))\n",
    "\n",
    "print('target_names: {}'.format(target))\n",
    "print('target: \\n{}'.format(Y))\n",
    "\n",
    "print('Y: {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform basic EDA for the breast cancer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of malignant: 212\n",
      "Number of benign: 357\n"
     ]
    }
   ],
   "source": [
    "# EDA for numbers of malignant and benign.\n",
    "print('Number of malignant: {}'.format((Y == 0).sum()))\n",
    "print('Number of benign: {}'.format((Y == 1).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_radius</th>\n",
       "      <th>mean_texture</th>\n",
       "      <th>mean_perimeter</th>\n",
       "      <th>mean_area</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>mean_compactness</th>\n",
       "      <th>mean_concavity</th>\n",
       "      <th>mean_concave_points</th>\n",
       "      <th>mean_symmetry</th>\n",
       "      <th>mean_fractal_dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst_radius</th>\n",
       "      <th>worst_texture</th>\n",
       "      <th>worst_perimeter</th>\n",
       "      <th>worst_area</th>\n",
       "      <th>worst_smoothness</th>\n",
       "      <th>worst_compactness</th>\n",
       "      <th>worst_concavity</th>\n",
       "      <th>worst_concave_points</th>\n",
       "      <th>worst_symmetry</th>\n",
       "      <th>worst_fractal_dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>82.418324</td>\n",
       "      <td>80.149257</td>\n",
       "      <td>60.928757</td>\n",
       "      <td>50.710758</td>\n",
       "      <td>72.551373</td>\n",
       "      <td>50.093678</td>\n",
       "      <td>66.625860</td>\n",
       "      <td>51.650359</td>\n",
       "      <td>65.675619</td>\n",
       "      <td>62.371637</td>\n",
       "      <td>...</td>\n",
       "      <td>52.327407</td>\n",
       "      <td>50.158185</td>\n",
       "      <td>52.939044</td>\n",
       "      <td>62.088177</td>\n",
       "      <td>61.756132</td>\n",
       "      <td>59.385983</td>\n",
       "      <td>59.984329</td>\n",
       "      <td>57.359308</td>\n",
       "      <td>49.329894</td>\n",
       "      <td>59.330395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>284.315843</td>\n",
       "      <td>281.218327</td>\n",
       "      <td>213.040592</td>\n",
       "      <td>182.652881</td>\n",
       "      <td>256.071418</td>\n",
       "      <td>188.256875</td>\n",
       "      <td>237.277474</td>\n",
       "      <td>173.442381</td>\n",
       "      <td>243.700592</td>\n",
       "      <td>249.772254</td>\n",
       "      <td>...</td>\n",
       "      <td>184.718843</td>\n",
       "      <td>165.364693</td>\n",
       "      <td>184.089029</td>\n",
       "      <td>214.003553</td>\n",
       "      <td>262.632038</td>\n",
       "      <td>198.181448</td>\n",
       "      <td>226.077133</td>\n",
       "      <td>216.440465</td>\n",
       "      <td>176.069135</td>\n",
       "      <td>222.413865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001997</td>\n",
       "      <td>0.001777</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.001435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>0.001638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001219</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.001217</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.078640</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>0.056160</td>\n",
       "      <td>0.065560</td>\n",
       "      <td>0.056770</td>\n",
       "      <td>0.061940</td>\n",
       "      <td>0.056280</td>\n",
       "      <td>0.053520</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055500</td>\n",
       "      <td>0.046050</td>\n",
       "      <td>0.052440</td>\n",
       "      <td>0.055810</td>\n",
       "      <td>0.052050</td>\n",
       "      <td>0.053550</td>\n",
       "      <td>0.059280</td>\n",
       "      <td>0.053850</td>\n",
       "      <td>0.045310</td>\n",
       "      <td>0.048240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.241600</td>\n",
       "      <td>0.225200</td>\n",
       "      <td>0.189500</td>\n",
       "      <td>0.181900</td>\n",
       "      <td>0.202700</td>\n",
       "      <td>0.192200</td>\n",
       "      <td>0.200100</td>\n",
       "      <td>0.169200</td>\n",
       "      <td>0.182200</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182500</td>\n",
       "      <td>0.162200</td>\n",
       "      <td>0.183600</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.168700</td>\n",
       "      <td>0.177500</td>\n",
       "      <td>0.187000</td>\n",
       "      <td>0.170900</td>\n",
       "      <td>0.162800</td>\n",
       "      <td>0.171000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18.250000</td>\n",
       "      <td>17.020000</td>\n",
       "      <td>15.530000</td>\n",
       "      <td>14.250000</td>\n",
       "      <td>16.150000</td>\n",
       "      <td>13.290000</td>\n",
       "      <td>15.760000</td>\n",
       "      <td>13.930000</td>\n",
       "      <td>14.770000</td>\n",
       "      <td>14.470000</td>\n",
       "      <td>...</td>\n",
       "      <td>13.720000</td>\n",
       "      <td>13.800000</td>\n",
       "      <td>13.980000</td>\n",
       "      <td>15.660000</td>\n",
       "      <td>14.920000</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>14.690000</td>\n",
       "      <td>13.660000</td>\n",
       "      <td>13.870000</td>\n",
       "      <td>16.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2398.000000</td>\n",
       "      <td>2615.000000</td>\n",
       "      <td>2145.000000</td>\n",
       "      <td>1866.000000</td>\n",
       "      <td>2562.000000</td>\n",
       "      <td>2360.000000</td>\n",
       "      <td>2073.000000</td>\n",
       "      <td>1590.000000</td>\n",
       "      <td>2403.000000</td>\n",
       "      <td>3216.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2081.000000</td>\n",
       "      <td>1349.000000</td>\n",
       "      <td>1748.000000</td>\n",
       "      <td>2022.000000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>1740.000000</td>\n",
       "      <td>2782.000000</td>\n",
       "      <td>2642.000000</td>\n",
       "      <td>1809.000000</td>\n",
       "      <td>2027.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean_radius  mean_texture  mean_perimeter    mean_area  \\\n",
       "count   569.000000    569.000000      569.000000   569.000000   \n",
       "mean     82.418324     80.149257       60.928757    50.710758   \n",
       "std     284.315843    281.218327      213.040592   182.652881   \n",
       "min       0.001997      0.001777        0.001286     0.000692   \n",
       "25%       0.078640      0.065400        0.055040     0.056160   \n",
       "50%       0.241600      0.225200        0.189500     0.181900   \n",
       "75%      18.250000     17.020000       15.530000    14.250000   \n",
       "max    2398.000000   2615.000000     2145.000000  1866.000000   \n",
       "\n",
       "       mean_smoothness  mean_compactness  mean_concavity  mean_concave_points  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean         72.551373         50.093678       66.625860            51.650359   \n",
       "std         256.071418        188.256875      237.277474           173.442381   \n",
       "min           0.001435          0.000000        0.001465             0.000000   \n",
       "25%           0.065560          0.056770        0.061940             0.056280   \n",
       "50%           0.202700          0.192200        0.200100             0.169200   \n",
       "75%          16.150000         13.290000       15.760000            13.930000   \n",
       "max        2562.000000       2360.000000     2073.000000          1590.000000   \n",
       "\n",
       "       mean_symmetry  mean_fractal_dimension           ...             \\\n",
       "count     569.000000              569.000000           ...              \n",
       "mean       65.675619               62.371637           ...              \n",
       "std       243.700592              249.772254           ...              \n",
       "min         0.001519                0.000000           ...              \n",
       "25%         0.053520                0.039900           ...              \n",
       "50%         0.182200                0.166700           ...              \n",
       "75%        14.770000               14.470000           ...              \n",
       "max      2403.000000             3216.000000           ...              \n",
       "\n",
       "       worst_radius  worst_texture  worst_perimeter   worst_area  \\\n",
       "count    569.000000     569.000000       569.000000   569.000000   \n",
       "mean      52.327407      50.158185        52.939044    62.088177   \n",
       "std      184.718843     165.364693       184.089029   214.003553   \n",
       "min        0.000000       0.001432         0.001180     0.001638   \n",
       "25%        0.055500       0.046050         0.052440     0.055810   \n",
       "50%        0.182500       0.162200         0.183600     0.165000   \n",
       "75%       13.720000      13.800000        13.980000    15.660000   \n",
       "max     2081.000000    1349.000000      1748.000000  2022.000000   \n",
       "\n",
       "       worst_smoothness  worst_compactness  worst_concavity  \\\n",
       "count        569.000000         569.000000       569.000000   \n",
       "mean          61.756132          59.385983        59.984329   \n",
       "std          262.632038         198.181448       226.077133   \n",
       "min            0.000000           0.001219         0.001541   \n",
       "25%            0.052050           0.053550         0.059280   \n",
       "50%            0.168700           0.177500         0.187000   \n",
       "75%           14.920000          14.200000        14.690000   \n",
       "max         4254.000000        1740.000000      2782.000000   \n",
       "\n",
       "       worst_concave_points  worst_symmetry  worst_fractal_dimension  \n",
       "count            569.000000      569.000000               569.000000  \n",
       "mean              57.359308       49.329894                59.330395  \n",
       "std              216.440465      176.069135               222.413865  \n",
       "min                0.001217        0.000000                 0.000000  \n",
       "25%                0.053850        0.045310                 0.048240  \n",
       "50%                0.170900        0.162800                 0.171000  \n",
       "75%               13.660000       13.870000                16.600000  \n",
       "max             2642.000000     1809.000000              2027.000000  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDA for feature matrix.\n",
    "pd.DataFrame(X.T, columns=features).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_feature(x, axis=1):\n",
    "    \"\"\"Implement a function that normalizes each col or row of the matrix x \n",
    "    to have unit length.\n",
    "    \n",
    "    Args:\n",
    "      x: A numpy matrix of shape (n, m).\n",
    "      axis: A integer in {0, 1}, \n",
    "        - 0: normalize for each feature col.\n",
    "        - 1: normalize for each feature row. \n",
    "    \n",
    "    Returns:\n",
    "      x_normalized: The normalized (by row) numpy matrix.\n",
    "    \"\"\"\n",
    "    # Compute x_norm as the norm 2 of x.\n",
    "    x_norm = np.linalg.norm(x, axis=axis, ord=2, keepdims=True)\n",
    "    # Divide x by its norm.\n",
    "    x_normalized = x / x_norm\n",
    "    return x_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized X: [[  2.54979475e-03   1.47119897e-03   1.74049358e-02 ...,   7.61395074e-05\n",
      "    3.38460804e-05   3.92319725e-05]\n",
      " [  1.09260839e-05   1.94273377e-03   2.06038825e-03 ...,   6.62739090e-06\n",
      "    6.93444040e-06   7.19270634e-06]\n",
      " [  3.76235879e-05   1.16809217e-05   2.83833711e-03 ...,   3.07502299e-05\n",
      "    6.64803308e-05   7.34483632e-05]\n",
      " ..., \n",
      " [  1.04314500e-01   2.07242386e-05   2.75261361e-05 ...,   1.45350741e-05\n",
      "    2.18672572e-03   3.75134953e-03]\n",
      " [  1.72576186e-02   9.55008756e-02   2.33130441e-05 ...,   7.01915373e-05\n",
      "    1.74985506e-05   2.49192578e-03]\n",
      " [  3.91512700e-03   1.24872137e-02   6.57077880e-02 ...,   0.00000000e+00\n",
      "    5.23292813e-05   1.28298785e-05]]\n",
      "Normalized X.shape: (30, 569)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_radius</th>\n",
       "      <th>mean_texture</th>\n",
       "      <th>mean_perimeter</th>\n",
       "      <th>mean_area</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>mean_compactness</th>\n",
       "      <th>mean_concavity</th>\n",
       "      <th>mean_concave_points</th>\n",
       "      <th>mean_symmetry</th>\n",
       "      <th>mean_fractal_dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst_radius</th>\n",
       "      <th>worst_texture</th>\n",
       "      <th>worst_perimeter</th>\n",
       "      <th>worst_area</th>\n",
       "      <th>worst_smoothness</th>\n",
       "      <th>worst_compactness</th>\n",
       "      <th>worst_concavity</th>\n",
       "      <th>worst_concave_points</th>\n",
       "      <th>worst_symmetry</th>\n",
       "      <th>worst_fractal_dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.168148e-02</td>\n",
       "      <td>1.149990e-02</td>\n",
       "      <td>1.153678e-02</td>\n",
       "      <td>1.122401e-02</td>\n",
       "      <td>1.143708e-02</td>\n",
       "      <td>0.010789</td>\n",
       "      <td>1.134239e-02</td>\n",
       "      <td>0.011975</td>\n",
       "      <td>1.091750e-02</td>\n",
       "      <td>0.010165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011435</td>\n",
       "      <td>1.217814e-02</td>\n",
       "      <td>1.159554e-02</td>\n",
       "      <td>1.169054e-02</td>\n",
       "      <td>0.009604</td>\n",
       "      <td>1.204324e-02</td>\n",
       "      <td>1.075992e-02</td>\n",
       "      <td>1.074799e-02</td>\n",
       "      <td>0.011319</td>\n",
       "      <td>0.010814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.029722e-02</td>\n",
       "      <td>4.034951e-02</td>\n",
       "      <td>4.033896e-02</td>\n",
       "      <td>4.042726e-02</td>\n",
       "      <td>4.036739e-02</td>\n",
       "      <td>0.040546</td>\n",
       "      <td>4.039415e-02</td>\n",
       "      <td>0.040211</td>\n",
       "      <td>4.051125e-02</td>\n",
       "      <td>0.040707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040368</td>\n",
       "      <td>4.014966e-02</td>\n",
       "      <td>4.032208e-02</td>\n",
       "      <td>4.029459e-02</td>\n",
       "      <td>0.040843</td>\n",
       "      <td>4.019040e-02</td>\n",
       "      <td>4.055347e-02</td>\n",
       "      <td>4.055664e-02</td>\n",
       "      <td>0.040401</td>\n",
       "      <td>0.040539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.830428e-07</td>\n",
       "      <td>2.549659e-07</td>\n",
       "      <td>2.435024e-07</td>\n",
       "      <td>1.531630e-07</td>\n",
       "      <td>2.262150e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.494018e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.525090e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.476819e-07</td>\n",
       "      <td>2.584622e-07</td>\n",
       "      <td>3.084180e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.472083e-07</td>\n",
       "      <td>2.764229e-07</td>\n",
       "      <td>2.280416e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.114596e-05</td>\n",
       "      <td>9.383662e-06</td>\n",
       "      <td>1.042175e-05</td>\n",
       "      <td>1.243011e-05</td>\n",
       "      <td>1.033495e-05</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>1.054467e-05</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>8.896828e-06</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>1.118069e-05</td>\n",
       "      <td>1.148623e-05</td>\n",
       "      <td>1.050843e-05</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1.085973e-05</td>\n",
       "      <td>1.063358e-05</td>\n",
       "      <td>1.009042e-05</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.424294e-05</td>\n",
       "      <td>3.231194e-05</td>\n",
       "      <td>3.588158e-05</td>\n",
       "      <td>4.026062e-05</td>\n",
       "      <td>3.195386e-05</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>3.406505e-05</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>3.028778e-05</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>3.938129e-05</td>\n",
       "      <td>4.021496e-05</td>\n",
       "      <td>3.106774e-05</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>3.599629e-05</td>\n",
       "      <td>3.354385e-05</td>\n",
       "      <td>3.202326e-05</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.586646e-03</td>\n",
       "      <td>2.442048e-03</td>\n",
       "      <td>2.940585e-03</td>\n",
       "      <td>3.154007e-03</td>\n",
       "      <td>2.545904e-03</td>\n",
       "      <td>0.002862</td>\n",
       "      <td>2.682984e-03</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>2.455272e-03</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>3.350566e-03</td>\n",
       "      <td>3.062120e-03</td>\n",
       "      <td>2.948611e-03</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>2.879703e-03</td>\n",
       "      <td>2.635076e-03</td>\n",
       "      <td>2.559612e-03</td>\n",
       "      <td>0.003183</td>\n",
       "      <td>0.003026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.398781e-01</td>\n",
       "      <td>3.752030e-01</td>\n",
       "      <td>4.061530e-01</td>\n",
       "      <td>4.130089e-01</td>\n",
       "      <td>4.038766e-01</td>\n",
       "      <td>0.508284</td>\n",
       "      <td>3.529078e-01</td>\n",
       "      <td>0.368626</td>\n",
       "      <td>3.994596e-01</td>\n",
       "      <td>0.524131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454775</td>\n",
       "      <td>3.275300e-01</td>\n",
       "      <td>3.828745e-01</td>\n",
       "      <td>3.807211e-01</td>\n",
       "      <td>0.661560</td>\n",
       "      <td>3.528650e-01</td>\n",
       "      <td>4.990321e-01</td>\n",
       "      <td>4.950582e-01</td>\n",
       "      <td>0.415092</td>\n",
       "      <td>0.369458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        mean_radius  mean_texture  mean_perimeter     mean_area  \\\n",
       "count  5.690000e+02  5.690000e+02    5.690000e+02  5.690000e+02   \n",
       "mean   1.168148e-02  1.149990e-02    1.153678e-02  1.122401e-02   \n",
       "std    4.029722e-02  4.034951e-02    4.033896e-02  4.042726e-02   \n",
       "min    2.830428e-07  2.549659e-07    2.435024e-07  1.531630e-07   \n",
       "25%    1.114596e-05  9.383662e-06    1.042175e-05  1.243011e-05   \n",
       "50%    3.424294e-05  3.231194e-05    3.588158e-05  4.026062e-05   \n",
       "75%    2.586646e-03  2.442048e-03    2.940585e-03  3.154007e-03   \n",
       "max    3.398781e-01  3.752030e-01    4.061530e-01  4.130089e-01   \n",
       "\n",
       "       mean_smoothness  mean_compactness  mean_concavity  mean_concave_points  \\\n",
       "count     5.690000e+02        569.000000    5.690000e+02           569.000000   \n",
       "mean      1.143708e-02          0.010789    1.134239e-02             0.011975   \n",
       "std       4.036739e-02          0.040546    4.039415e-02             0.040211   \n",
       "min       2.262150e-07          0.000000    2.494018e-07             0.000000   \n",
       "25%       1.033495e-05          0.000012    1.054467e-05             0.000013   \n",
       "50%       3.195386e-05          0.000041    3.406505e-05             0.000039   \n",
       "75%       2.545904e-03          0.002862    2.682984e-03             0.003230   \n",
       "max       4.038766e-01          0.508284    3.529078e-01             0.368626   \n",
       "\n",
       "       mean_symmetry  mean_fractal_dimension           ...             \\\n",
       "count   5.690000e+02              569.000000           ...              \n",
       "mean    1.091750e-02                0.010165           ...              \n",
       "std     4.051125e-02                0.040707           ...              \n",
       "min     2.525090e-07                0.000000           ...              \n",
       "25%     8.896828e-06                0.000007           ...              \n",
       "50%     3.028778e-05                0.000027           ...              \n",
       "75%     2.455272e-03                0.002358           ...              \n",
       "max     3.994596e-01                0.524131           ...              \n",
       "\n",
       "       worst_radius  worst_texture  worst_perimeter    worst_area  \\\n",
       "count    569.000000   5.690000e+02     5.690000e+02  5.690000e+02   \n",
       "mean       0.011435   1.217814e-02     1.159554e-02  1.169054e-02   \n",
       "std        0.040368   4.014966e-02     4.032208e-02  4.029459e-02   \n",
       "min        0.000000   3.476819e-07     2.584622e-07  3.084180e-07   \n",
       "25%        0.000012   1.118069e-05     1.148623e-05  1.050843e-05   \n",
       "50%        0.000040   3.938129e-05     4.021496e-05  3.106774e-05   \n",
       "75%        0.002998   3.350566e-03     3.062120e-03  2.948611e-03   \n",
       "max        0.454775   3.275300e-01     3.828745e-01  3.807211e-01   \n",
       "\n",
       "       worst_smoothness  worst_compactness  worst_concavity  \\\n",
       "count        569.000000       5.690000e+02     5.690000e+02   \n",
       "mean           0.009604       1.204324e-02     1.075992e-02   \n",
       "std            0.040843       4.019040e-02     4.055347e-02   \n",
       "min            0.000000       2.472083e-07     2.764229e-07   \n",
       "25%            0.000008       1.085973e-05     1.063358e-05   \n",
       "50%            0.000026       3.599629e-05     3.354385e-05   \n",
       "75%            0.002320       2.879703e-03     2.635076e-03   \n",
       "max            0.661560       3.528650e-01     4.990321e-01   \n",
       "\n",
       "       worst_concave_points  worst_symmetry  worst_fractal_dimension  \n",
       "count          5.690000e+02      569.000000               569.000000  \n",
       "mean           1.074799e-02        0.011319                 0.010814  \n",
       "std            4.055664e-02        0.040401                 0.040539  \n",
       "min            2.280416e-07        0.000000                 0.000000  \n",
       "25%            1.009042e-05        0.000010                 0.000009  \n",
       "50%            3.202326e-05        0.000037                 0.000031  \n",
       "75%            2.559612e-03        0.003183                 0.003026  \n",
       "max            4.950582e-01        0.415092                 0.369458  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = normalize_feature(X)\n",
    "\n",
    "print('Normalized X: {}'.format(X))\n",
    "print('Normalized X.shape: {}'.format(X.shape))\n",
    "\n",
    "# EDA for normalized feature matrix.\n",
    "pd.DataFrame(X.T, columns=features).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (30, 455)\n",
      "Y_train.shape: (1, 455)\n",
      "X_test.shape: (30, 114)\n",
      "Y_test.shape: (1, 114)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "train_flag = np.random.rand(X.shape[1]) < TRAIN_PERCENT\n",
    "\n",
    "X_train = X[:, train_flag]\n",
    "Y_train = Y[:, train_flag]\n",
    "X_test = X[:, ~train_flag]\n",
    "Y_test = Y[:, ~train_flag]\n",
    "\n",
    "print('X_train.shape: {}'.format(X_train.shape))\n",
    "print('Y_train.shape: {}'.format(Y_train.shape))\n",
    "print('X_test.shape: {}'.format(X_test.shape))\n",
    "print('Y_test.shape: {}'.format(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sigmoid function.\n",
    "def sigmoid(x):\n",
    "    \"\"\"Compute the sigmoid of x.\n",
    "\n",
    "    Args:\n",
    "      x: A scalar or numpy array of any size.\n",
    "\n",
    "    Returns:\n",
    "      s: sigmoid(x).\n",
    "    \"\"\"\n",
    "    s = 1 / (1 + np.exp(-x))    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(s):\n",
    "    \"\"\"Compute the gradient of the sigmoid function.\n",
    "    \n",
    "    Args:\n",
    "      s: A scalar or numpy array. Sigmoid function.\n",
    "\n",
    "    Returns:\n",
    "      ds: Computed gradient.\n",
    "    \"\"\"\n",
    "    ds = s * (1 - s)    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_weights(dim):\n",
    "    \"\"\"Initialize weights.\n",
    "\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and b to 0.\n",
    "    \n",
    "    Args:\n",
    "      dim: A integer. Size of the w vector (or number of parameters.)\n",
    "    \n",
    "    Returns:\n",
    "      w: A Numpy array. Initialized vector of shape (dim, 1)\n",
    "      b: A integer. Initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    w = np.zeros(dim).reshape(dim, 1)\n",
    "    b = 0\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"Forward & backward propagation.\n",
    "\n",
    "    Implement the cost function and its gradient for the propagation.\n",
    "\n",
    "    Args:\n",
    "      w: A Numpy array. Weights of size (num_px * num_px * 3, 1)\n",
    "      b: A float. Bias.\n",
    "      X: A Numpy array. Data of size (num_px * num_px * 3, number of examples).\n",
    "      Y: A Numpy array. True \"label\" vector (containing 0 or 1) \n",
    "         of size (1, number of examples).\n",
    "\n",
    "    Returns:\n",
    "      cost: A float. Negative log-likelihood cost for logistic regression.\n",
    "      dw: A Numpy array. Gradient of the loss w.r.t. w, thus same shape as w.\n",
    "      db: A float. Gradient of the loss w.r.t b, thus same shape as b.\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Forward propagation from X to cost.\n",
    "    # Compute activation.\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    # Compute cost.\n",
    "    cost = - 1 / m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n",
    "    \n",
    "    # Backward propagation to find gradient.\n",
    "    dw = 1 / m * np.dot(X, (A - Y).T)\n",
    "    db = 1 / m * np.sum(A - Y)\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db} \n",
    "\n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=True):\n",
    "    \"\"\"Optimization function.\n",
    "\n",
    "    This function optimizes w and b by running a gradient descent algorithm.\n",
    "    That is, write down two steps and iterate through them:\n",
    "      1. Calculate the cost and the gradient for the current parameters. \n",
    "        Use propagate().\n",
    "      2. Update the parameters using gradient descent rule for w and b.\n",
    "    \n",
    "    Args:\n",
    "      w: A Numpy array. Weights of size (num_px * num_px * 3, 1).\n",
    "      b: A scalar. Bias.\n",
    "      X: A Numpy array. Data of shape (num_px * num_px * 3, number of examples).\n",
    "      Y: A Numpy array. True \"label\" vector (containing 0 if non-cat, 1 if cat), \n",
    "        of shape (1, number of examples)\n",
    "      num_iterations: A integer. Number of iterations of the optimization loop.\n",
    "      learning_rate: A scalr. Learning rate of the gradient descent update rule.\n",
    "      print_cost: A Boolean. Print the loss every 100 steps. Default: True.\n",
    "    \n",
    "    Returns:\n",
    "      params: A dictionary containing the weights w and bias b.\n",
    "      grads: A dictionary containing the gradients of the weights and bias \n",
    "        with respect to the cost function\n",
    "      costs: A list of all the costs computed during the optimization, \n",
    "        this will be used to plot the learning curve.\n",
    "    \"\"\"   \n",
    "    costs = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads.get('dw')\n",
    "        db = grads.get('db')\n",
    "        \n",
    "        # Update rule.\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    \"\"\"Prediction.\n",
    "\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression \n",
    "    parameters (w, b)\n",
    "    \n",
    "    Args:\n",
    "      w: A Numpy array. Learned weights of size (num_px * num_px * 3, 1).\n",
    "      b: A scalar. Learned bias.\n",
    "      X: A Numpy array. New data of size (num_px * num_px * 3, number of examples).\n",
    "    \n",
    "    Returns:\n",
    "      Y_prediction: A Numpy array containing all predictions (0/1) \n",
    "        for the examples in X.\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a label 1 \n",
    "    # being present in the picture.\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        # Convert probabilities a[0,i] to actual predictions p[0,i]\n",
    "        if A[0, i] > 0.5:\n",
    "            Y_prediction[0, i] = 1\n",
    "        else:\n",
    "            Y_prediction[0, i] = 0\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(Y_prediction, Y):\n",
    "    acc = 1 - np.mean(np.abs(Y_prediction - Y))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(X_train, Y_train, X_test, Y_test, \n",
    "                        num_iterations=2000, learning_rate=0.001, print_cost=True):\n",
    "    '''Wrap-up function for logistic regression.\n",
    "\n",
    "    Builds the logistic regression model by calling the function \n",
    "    you've implemented previously.\n",
    "    \n",
    "    Args:\n",
    "      X_train: A Numpy. Training set of shape (num_px * num_px * 3, m_train).\n",
    "      Y_train: A Numpy array. Training labels of shape (1, m_train).\n",
    "      X_test: A Numpy array. Test set of shape (num_px * num_px * 3, m_test).\n",
    "      Y_test: A Numpy array. Test labels of shape (1, m_test).\n",
    "      num_iterations: An integer. Hyperparameter for the number of iterations \n",
    "        to optimize the parameters. Default: 2000.\n",
    "      learning_rate: A scalar. Hyperparameter for the learning rate used \n",
    "        in the update rule of optimize(). Default: 0.005.\n",
    "      print_cost: A Boolean. Print the cost every 100 iterations. Default: True.\n",
    "    \n",
    "    Returns:\n",
    "      d: A dictionary containing information about the model.\n",
    "    '''    \n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    w, b = initialize_weights(X_train.shape[0])\n",
    "\n",
    "    # Gradient descent.\n",
    "    parameters, grads, costs = optimize(\n",
    "        w, b, X_train, Y_train, \n",
    "        num_iterations=num_iterations, learning_rate=learning_rate, \n",
    "        print_cost=print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary 'parameters'\n",
    "    w = parameters.get('w')\n",
    "    b = parameters.get('b')\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_pred_train = predict(w, b, X_train)\n",
    "    Y_pred_test = predict(w, b, X_test)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print('Train accuracy: {} %'\n",
    "          .format(accuracy(Y_pred_train, Y_train) * 100))\n",
    "    print('Test accuracy: {} %'\n",
    "          .format(accuracy(Y_pred_test, Y_test) * 100))\n",
    "    \n",
    "    d = {'costs': costs,\n",
    "         'Y_pred_train': Y_pred_train, \n",
    "         'Y_pred_test': Y_pred_test, \n",
    "         'w': w, \n",
    "         'b': b,\n",
    "         'learning_rate' : learning_rate,\n",
    "         'num_iterations': num_iterations}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.691354\n",
      "Cost after iteration 200: 0.689648\n",
      "Cost after iteration 300: 0.688026\n",
      "Cost after iteration 400: 0.686482\n",
      "Cost after iteration 500: 0.685015\n",
      "Cost after iteration 600: 0.683618\n",
      "Cost after iteration 700: 0.682290\n",
      "Cost after iteration 800: 0.681027\n",
      "Cost after iteration 900: 0.679825\n",
      "Cost after iteration 1000: 0.678681\n",
      "Cost after iteration 1100: 0.677594\n",
      "Cost after iteration 1200: 0.676559\n",
      "Cost after iteration 1300: 0.675574\n",
      "Cost after iteration 1400: 0.674637\n",
      "Cost after iteration 1500: 0.673746\n",
      "Cost after iteration 1600: 0.672897\n",
      "Cost after iteration 1700: 0.672090\n",
      "Cost after iteration 1800: 0.671322\n",
      "Cost after iteration 1900: 0.670590\n",
      "Train accuracy: 63.5164835165 %\n",
      "Test accuracy: 59.649122807 %\n"
     ]
    }
   ],
   "source": [
    "d = logistic_regression(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 53.5087719298 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logist_reg = LogisticRegression(C=10**6, max_iter=2000)\n",
    "logist_reg.fit(X_train.T, Y_train.flatten())\n",
    "Y_pred_test = logist_reg.predict(X_test.T)\n",
    "\n",
    "print('Test accuracy: {} %'\n",
    "      .format(accuracy(Y_pred_test, Y_test.flatten()) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
