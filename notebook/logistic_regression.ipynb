{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Logistic regression is one of the most fundamental machine learning models for binary classification. I will summarize its methodology and implement it from scratch using NumPy.\n",
    "\n",
    "Binary classification\n",
    "For example, the doctor would like to base on patients's features, including mean radius, mean texture, etc, to classify breat cancer into one of the following two case:\n",
    "\n",
    "\"malignant\":  ùë¶=1 \n",
    "\"benign\":  ùë¶=0 \n",
    "which correspond to serious and gentle case respectively.\n",
    "\n",
    "We would like to load the breast cancer data from scikit-learn as a toy dataset, and split the data into the training and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression Model\n",
    "\n",
    "[To be continued.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression as LogisticRegressionSklearn\n",
    "\n",
    "# https://github.com/bowen0701/machine-learning/blob/master/np_logistic_regression.py\n",
    "from np_logistic_regression import LogisticRegression\n",
    "\n",
    "# https://github.com/bowen0701/machine-learning/blob/master/np_metrics.py\n",
    "from np_metrics import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read breast cancer data.\n",
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.799e+01, 1.038e+01, 1.228e+02, 1.001e+03, 1.184e-01, 2.776e-01,\n",
       "        3.001e-01, 1.471e-01, 2.419e-01, 7.871e-02, 1.095e+00, 9.053e-01,\n",
       "        8.589e+00, 1.534e+02, 6.399e-03, 4.904e-02, 5.373e-02, 1.587e-02,\n",
       "        3.003e-02, 6.193e-03, 2.538e+01, 1.733e+01, 1.846e+02, 2.019e+03,\n",
       "        1.622e-01, 6.656e-01, 7.119e-01, 2.654e-01, 4.601e-01, 1.189e-01],\n",
       "       [2.057e+01, 1.777e+01, 1.329e+02, 1.326e+03, 8.474e-02, 7.864e-02,\n",
       "        8.690e-02, 7.017e-02, 1.812e-01, 5.667e-02, 5.435e-01, 7.339e-01,\n",
       "        3.398e+00, 7.408e+01, 5.225e-03, 1.308e-02, 1.860e-02, 1.340e-02,\n",
       "        1.389e-02, 3.532e-03, 2.499e+01, 2.341e+01, 1.588e+02, 1.956e+03,\n",
       "        1.238e-01, 1.866e-01, 2.416e-01, 1.860e-01, 2.750e-01, 8.902e-02],\n",
       "       [1.969e+01, 2.125e+01, 1.300e+02, 1.203e+03, 1.096e-01, 1.599e-01,\n",
       "        1.974e-01, 1.279e-01, 2.069e-01, 5.999e-02, 7.456e-01, 7.869e-01,\n",
       "        4.585e+00, 9.403e+01, 6.150e-03, 4.006e-02, 3.832e-02, 2.058e-02,\n",
       "        2.250e-02, 4.571e-03, 2.357e+01, 2.553e+01, 1.525e+02, 1.709e+03,\n",
       "        1.444e-01, 4.245e-01, 4.504e-01, 2.430e-01, 3.613e-01, 8.758e-02]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test datasets.\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=71, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30) (426,)\n",
      "(143, 30) (143,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_raw.shape, y_train.shape)\n",
    "print(X_test_raw.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering for standardizing features by min-max scaler.\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "X_train = min_max_scaler.fit_transform(X_train_raw)\n",
    "X_test = min_max_scaler.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fitting Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss 0.9186545912614569\n",
      "epoch 11: loss 0.11606218740367903\n",
      "epoch 21: loss 0.12713299304428574\n",
      "epoch 31: loss 0.18602308107303367\n",
      "epoch 41: loss 0.09073612017471461\n",
      "epoch 51: loss 0.13361715850108571\n",
      "epoch 61: loss 0.10838730427263459\n",
      "epoch 71: loss 0.2306510974754015\n",
      "epoch 81: loss 0.08566830244549475\n",
      "epoch 91: loss 0.04738681575338377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<np_logistic_regression.LogisticRegression at 0x7f817323a898>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit our Logistic Regression.\n",
    "clf = LogisticRegression(batch_size=100, lr=5, n_epochs=100)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[12.78952101]]),\n",
       " array([-1.39329455, -2.41286307, -1.48371562, -2.34195007, -0.50589684,\n",
       "         0.10987669, -3.52945501, -5.47550081, -0.70913116,  2.84175594,\n",
       "        -4.58475538, -0.27308136, -3.57920473, -2.92072581,  1.01224453,\n",
       "         2.89362755,  1.55390066,  0.93025122,  2.10446407,  2.23325215,\n",
       "        -4.1280024 , -4.07195503, -3.75255751, -3.84522616, -2.66030014,\n",
       "        -0.9274295 , -2.70900639, -4.7528603 , -2.13695895, -0.49447423]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get coefficient.\n",
    "clf.get_coeff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.87026842e-01, 3.20543684e-10, 5.31875738e-04, 9.92209483e-01,\n",
       "       9.96132879e-01, 9.97605773e-01, 9.99710734e-03, 4.02808538e-03,\n",
       "       9.99699841e-01, 2.49314285e-02, 9.96762190e-01, 9.80962691e-01,\n",
       "       8.07980349e-01, 4.44774372e-05, 8.54952727e-07, 1.26325545e-02,\n",
       "       1.89339170e-02, 1.34269194e-06, 9.99742894e-01, 1.18365700e-09,\n",
       "       9.73781856e-01, 9.98859770e-01, 1.85504505e-08, 8.59019168e-01,\n",
       "       9.99221509e-01, 2.33407907e-01, 4.12994029e-01, 9.99993731e-01,\n",
       "       5.74946999e-02, 7.95092341e-01, 1.81821803e-04, 4.35830753e-02,\n",
       "       6.89092339e-06, 9.99880080e-01, 1.31645931e-01, 4.04612490e-03,\n",
       "       1.45122054e-01, 9.80581670e-01, 1.87511307e-02, 4.15246223e-02,\n",
       "       9.77290842e-01, 1.86445456e-02, 9.99159013e-01, 7.24793604e-01,\n",
       "       1.82841483e-03, 6.61184078e-01, 9.97849760e-01, 1.97692193e-02,\n",
       "       2.72319403e-05, 1.51266750e-07, 9.83365050e-01, 9.56258883e-01,\n",
       "       5.95040680e-03, 9.99305548e-01, 4.27208873e-06, 9.96180152e-01,\n",
       "       9.95831806e-01, 4.83810320e-01, 9.98368062e-01, 9.94730148e-01,\n",
       "       9.90597907e-01, 6.38308595e-06, 5.28274276e-02, 5.71777560e-01,\n",
       "       9.82858235e-01, 9.68556115e-01, 9.12503911e-01, 9.06496143e-01,\n",
       "       9.97617840e-01, 9.99770222e-01, 8.07818062e-01, 9.99051281e-01,\n",
       "       9.56222276e-01, 9.99821953e-01, 9.99978581e-01, 6.28520462e-05,\n",
       "       9.98189196e-01, 9.87534738e-01, 9.73831599e-01, 9.98577802e-01,\n",
       "       8.85331746e-01, 9.93782338e-01, 7.83826011e-01, 9.95366349e-01,\n",
       "       9.92670474e-01, 9.95232662e-01, 5.30137613e-10, 9.78368684e-01,\n",
       "       8.80289970e-01, 1.19739309e-03, 5.64642065e-04, 7.65756120e-01,\n",
       "       1.02725268e-03, 5.34457457e-02, 2.54862005e-05, 5.17565649e-01,\n",
       "       5.30329880e-02, 9.99650935e-01, 9.82909117e-01, 9.99697306e-01,\n",
       "       9.66475050e-05, 8.22701043e-04, 9.49949526e-01, 3.07589304e-04,\n",
       "       9.99696771e-01, 9.98162229e-01, 5.67291540e-03, 9.97826704e-01,\n",
       "       9.94628054e-01, 9.03936085e-01, 9.93081528e-01, 9.79307320e-01,\n",
       "       4.74898192e-03, 2.79442912e-03, 9.99431732e-01, 3.81198377e-03,\n",
       "       2.82467123e-01, 7.96355539e-06, 9.98945243e-01, 9.17340519e-05,\n",
       "       9.71951580e-01, 8.12206932e-03, 2.25753919e-07, 9.98082021e-01,\n",
       "       9.84635121e-01, 1.99655802e-02, 8.75179817e-01, 9.59635647e-01,\n",
       "       9.91005323e-01, 7.11822903e-01, 4.29034035e-02, 9.94069097e-01,\n",
       "       9.98196455e-01, 2.41437901e-08, 9.99471230e-01, 9.97513107e-01,\n",
       "       9.84374821e-01, 9.98113915e-01, 1.10859368e-02, 1.23344182e-03,\n",
       "       9.91330968e-01, 9.91883680e-01, 9.99837184e-01, 9.99193956e-01,\n",
       "       9.88325261e-01, 9.88539458e-01, 9.87344213e-01, 9.92151954e-01,\n",
       "       9.96636350e-01, 9.96707025e-01, 8.23934007e-01, 9.97127247e-01,\n",
       "       9.95195276e-01, 9.98337306e-01, 7.95752505e-01, 9.92633919e-01,\n",
       "       9.65917800e-01, 9.86480766e-01, 1.62747307e-03, 9.60517568e-01,\n",
       "       9.77500222e-01, 9.90866166e-01, 1.23050310e-03, 9.99937783e-01,\n",
       "       9.99815100e-01, 7.60844094e-01, 9.99596568e-01, 1.87888429e-03,\n",
       "       9.86765454e-01, 9.96688835e-01, 3.12073030e-03, 5.69297543e-02,\n",
       "       6.78058781e-03, 9.99781702e-01, 9.99711111e-01, 9.99997829e-01,\n",
       "       1.15019306e-04, 9.80010940e-01, 4.47996091e-05, 7.50948484e-05,\n",
       "       2.72107988e-01, 9.82010011e-01, 8.03087854e-01, 9.97277745e-01,\n",
       "       5.74273264e-01, 9.69512361e-01, 7.74434369e-02, 9.97604410e-01,\n",
       "       9.99656634e-01, 2.70723059e-03, 1.71875745e-01, 9.92302393e-01,\n",
       "       9.98421188e-01, 9.90636236e-01, 9.95359297e-01, 2.85720405e-02,\n",
       "       7.95011458e-01, 9.97547208e-01, 9.55647924e-04, 9.31732294e-01,\n",
       "       9.30739101e-01, 9.96103892e-01, 8.98834273e-01, 9.95690770e-01,\n",
       "       1.14522172e-01, 9.98018426e-01, 1.62073311e-04, 9.75973901e-01,\n",
       "       7.29024777e-01, 9.75328454e-01, 1.80402585e-02, 9.78848557e-01,\n",
       "       3.76724931e-02, 9.50588023e-01, 6.08917116e-04, 7.70400711e-01,\n",
       "       5.96295579e-14, 3.10350161e-01, 1.01241018e-04, 9.99775337e-01,\n",
       "       9.81138638e-01, 9.80885125e-01, 9.98165120e-01, 9.49653226e-01,\n",
       "       9.97921909e-01, 8.08519485e-06, 9.96778051e-01, 4.75436224e-04,\n",
       "       3.14203233e-03, 9.72712555e-01, 9.99508736e-01, 2.07258363e-09,\n",
       "       9.26292606e-01, 9.87799885e-01, 9.99292044e-01, 7.04529500e-05,\n",
       "       9.99643867e-01, 9.80145051e-01, 9.79005717e-01, 1.65429738e-04,\n",
       "       1.47456036e-05, 9.99911997e-01, 9.99989993e-01, 9.98175944e-01,\n",
       "       9.75390964e-01, 9.98349418e-01, 9.24195592e-01, 7.19772183e-01,\n",
       "       9.99354728e-01, 9.99838694e-01, 9.99161620e-01, 9.90282345e-02,\n",
       "       9.88765798e-01, 9.98107964e-01, 8.78699358e-01, 9.90146477e-01,\n",
       "       2.38743520e-05, 9.94053172e-01, 9.54985215e-01, 1.51659405e-06,\n",
       "       6.79209343e-08, 7.96925560e-02, 9.95859487e-01, 9.99555783e-01,\n",
       "       3.60620053e-04, 4.44935109e-03, 3.35194941e-07, 9.76910182e-01,\n",
       "       9.99948508e-01, 1.06241104e-01, 3.42881903e-04, 7.19869105e-04,\n",
       "       8.33620698e-01, 9.79169009e-01, 1.16683382e-04, 6.96850286e-06,\n",
       "       9.94589365e-01, 3.50699995e-01, 8.62909942e-03, 9.99164490e-01,\n",
       "       1.86631806e-09, 3.20099695e-03, 9.95974567e-01, 1.72473154e-02,\n",
       "       1.12026306e-03, 9.86516298e-01, 9.94821532e-01, 9.92597928e-01,\n",
       "       3.69334785e-02, 9.76138518e-01, 4.24606588e-05, 9.99749137e-01,\n",
       "       2.78247257e-05, 4.35633303e-01, 7.89237506e-01, 4.01302164e-01,\n",
       "       8.75088702e-01, 9.98980320e-01, 1.52127200e-06, 9.99682319e-01,\n",
       "       9.65380841e-01, 1.38208948e-02, 9.99943756e-01, 9.82747934e-03,\n",
       "       9.95996969e-01, 2.45793966e-03, 8.24391036e-01, 3.70999222e-01,\n",
       "       1.55841768e-01, 2.39544585e-01, 9.54724145e-01, 4.02650815e-03,\n",
       "       9.98269590e-01, 8.24390029e-02, 1.31540884e-08, 9.99267378e-01,\n",
       "       9.99858324e-01, 8.91781569e-01, 9.91237696e-01, 9.68366648e-01,\n",
       "       8.95963210e-01, 9.87873983e-01, 9.99977420e-01, 9.31080955e-01,\n",
       "       9.93829607e-01, 9.73936615e-01, 4.90180378e-01, 9.96414965e-01,\n",
       "       9.99281388e-01, 5.95633526e-01, 2.24112601e-05, 9.90715403e-01,\n",
       "       9.99451725e-01, 9.84267416e-01, 9.51032565e-01, 9.99502368e-01,\n",
       "       6.45405655e-01, 8.91595026e-01, 9.89678802e-01, 1.85690824e-02,\n",
       "       9.94960337e-01, 2.68496037e-06, 8.21188279e-04, 9.85855209e-01,\n",
       "       9.00018264e-01, 9.96650153e-01, 2.46814149e-03, 9.99213954e-01,\n",
       "       9.75586912e-01, 9.99759290e-01, 5.80162893e-05, 9.85680505e-01,\n",
       "       9.27422976e-01, 9.99188207e-01, 1.59118750e-01, 9.72772059e-01,\n",
       "       9.99989223e-01, 9.99078678e-01, 9.99880765e-01, 9.99699760e-01,\n",
       "       9.99051234e-01, 8.78418063e-03, 9.89810583e-01, 3.67836739e-01,\n",
       "       3.27670234e-03, 1.74723972e-03, 1.16534435e-05, 9.06732423e-01,\n",
       "       9.73648503e-01, 4.27190952e-01, 9.88745649e-01, 9.78271576e-01,\n",
       "       4.29122008e-05, 2.08100002e-03, 9.93476106e-01, 9.96250985e-01,\n",
       "       9.53676934e-01, 2.22492648e-05, 8.35194372e-01, 9.74797712e-08,\n",
       "       9.50115993e-01, 9.53175425e-01, 1.65105408e-05, 8.99563589e-02,\n",
       "       9.89997515e-01, 1.58277912e-02, 8.06749137e-01, 9.93806276e-01,\n",
       "       9.99872560e-01, 9.73007176e-01, 9.18596294e-01, 9.96839863e-01,\n",
       "       1.85125566e-02, 9.83991044e-01, 8.90303810e-01, 9.92642389e-01,\n",
       "       2.52968871e-06, 1.65544941e-04, 9.98642176e-01, 4.25629273e-07,\n",
       "       9.29829651e-02, 9.99669119e-01, 9.92977366e-01, 9.99969891e-01,\n",
       "       8.47555094e-05, 4.03405169e-02, 3.62013619e-04, 9.98404466e-01,\n",
       "       9.99529208e-01, 4.38103503e-04, 9.99839475e-01, 9.85669766e-08,\n",
       "       9.99639280e-01, 9.71732017e-01, 9.98579850e-01, 9.95957723e-01,\n",
       "       9.42912788e-01, 4.82972367e-02, 2.26089028e-02, 9.90921035e-01,\n",
       "       4.07063964e-01, 9.99547677e-01, 9.91728971e-01, 9.99803297e-01,\n",
       "       9.09896639e-01, 2.00123729e-05])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted probabilities for training data.\n",
    "p_pred_train = clf.predict(X_train)\n",
    "p_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted labels for training data.\n",
    "y_pred_train = (p_pred_train > 0.5) * 1\n",
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted label correctness for training data.\n",
    "y_pred_train == y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted label correctness for test data.\n",
    "p_pred_test = clf.predict(X_test)\n",
    "y_pred_test = (p_pred_test > 0.5) * 1\n",
    "y_pred_test == y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.965034965034965"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction accuracy for test data.\n",
    "accuracy(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fitting Sklearn's Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=500,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit sklearn's Logistic Regression.\n",
    "clf2 = LogisticRegressionSklearn(C=1e4, solver='lbfgs', max_iter=500)\n",
    "\n",
    "clf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([56.05003313]),\n",
       " array([[  53.58643008,  -27.26324946,   48.30813261,   10.51429012,\n",
       "          -14.72140714,   98.48590155,  -52.54093157,  -52.1529564 ,\n",
       "           -5.09714283,  -53.94912949,  -33.90682397,   -5.47905937,\n",
       "          -19.38082761,  -44.01004736,   38.82361457,  -51.40837833,\n",
       "           83.34837589,  -21.99821834,   14.90317319,   80.03225554,\n",
       "          -59.03462026,   -3.91874979,  -63.60095116, -103.93267556,\n",
       "           -8.01066718,   20.06509642,  -22.04450625,  -21.2526363 ,\n",
       "          -21.51142977,  -11.72401291]]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get coefficients.\n",
    "clf2.intercept_, clf2.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted labels for training data.\n",
    "y_pred_train = clf2.predict(X_train)\n",
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted label correctness for training data.\n",
    "y_pred_train == y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True, False,  True,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted label correctness for test data.\n",
    "y_pred_test = clf2.predict(X_test) \n",
    "y_pred_test == y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.965034965034965"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Prediction accuracy for test data.\n",
    "accuracy(y_test, y_pred_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
